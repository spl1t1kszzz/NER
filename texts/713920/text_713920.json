{
  "entities": [
    [
      [
        2,
        21
      ],
      [
        93,
        112
      ],
      [
        1610,
        1629
      ],
      [
        2376,
        2395
      ],
      [
        2518,
        2537
      ],
      [
        9833,
        9852
      ],
      [
        16560,
        16579
      ],
      [
        22794,
        22813
      ],
      [
        33709,
        33728
      ],
      [
        47902,
        47921
      ]
    ],
    [
      [
        24,
        41
      ],
      [
        115,
        132
      ],
      [
        247,
        263
      ],
      [
        383,
        401
      ],
      [
        1226,
        1244
      ],
      [
        1411,
        1428
      ],
      [
        1473,
        1490
      ],
      [
        1577,
        1594
      ],
      [
        1690,
        1708
      ],
      [
        2310,
        2327
      ],
      [
        2476,
        2494
      ],
      [
        2691,
        2708
      ],
      [
        2764,
        2781
      ],
      [
        3506,
        3524
      ],
      [
        4191,
        4208
      ],
      [
        4791,
        4808
      ],
      [
        4957,
        4974
      ],
      [
        5082,
        5100
      ],
      [
        5225,
        5242
      ],
      [
        5315,
        5332
      ],
      [
        5901,
        5919
      ],
      [
        6147,
        6164
      ],
      [
        6293,
        6310
      ],
      [
        15442,
        15460
      ],
      [
        25549,
        25567
      ],
      [
        25621,
        25639
      ],
      [
        28240,
        28258
      ],
      [
        29609,
        29611
      ],
      [
        35124,
        35141
      ],
      [
        35278,
        35295
      ],
      [
        35303,
        35321
      ]
    ],
    [
      [
        52,
        68
      ],
      [
        143,
        159
      ],
      [
        881,
        897
      ],
      [
        3614,
        3630
      ],
      [
        3633,
        3635
      ],
      [
        4015,
        4031
      ],
      [
        4248,
        4264
      ],
      [
        4379,
        4395
      ],
      [
        4882,
        4898
      ],
      [
        6561,
        6577
      ],
      [
        10525,
        10541
      ],
      [
        15059,
        15075
      ],
      [
        23074,
        23090
      ],
      [
        38511,
        38527
      ],
      [
        38569,
        38585
      ],
      [
        38800,
        38816
      ],
      [
        39097,
        39103
      ],
      [
        39155,
        39161
      ],
      [
        40187,
        40193
      ],
      [
        40212,
        40218
      ],
      [
        40388,
        40394
      ]
    ],
    [
      [
        328,
        330
      ],
      [
        3160,
        3162
      ],
      [
        3480,
        3482
      ],
      [
        5962,
        5964
      ],
      [
        9113,
        9115
      ],
      [
        10100,
        10102
      ],
      [
        10173,
        10175
      ],
      [
        10274,
        10276
      ],
      [
        10347,
        10349
      ],
      [
        10468,
        10470
      ],
      [
        10547,
        10549
      ],
      [
        11129,
        11131
      ],
      [
        11438,
        11440
      ],
      [
        11609,
        11611
      ],
      [
        12084,
        12087
      ],
      [
        12146,
        12148
      ],
      [
        12447,
        12449
      ],
      [
        12684,
        12686
      ],
      [
        13109,
        13111
      ],
      [
        13291,
        13293
      ],
      [
        13549,
        13552
      ],
      [
        13967,
        13969
      ],
      [
        14004,
        14006
      ],
      [
        14386,
        14389
      ],
      [
        14475,
        14477
      ],
      [
        14897,
        14899
      ],
      [
        14943,
        14946
      ],
      [
        15076,
        15078
      ],
      [
        15359,
        15361
      ],
      [
        15972,
        15974
      ],
      [
        16271,
        16273
      ],
      [
        16496,
        16497
      ],
      [
        16663,
        16665
      ],
      [
        16782,
        16784
      ],
      [
        16845,
        16847
      ],
      [
        16881,
        16883
      ],
      [
        17387,
        17389
      ],
      [
        17651,
        17653
      ],
      [
        17790,
        17792
      ],
      [
        17858,
        17860
      ],
      [
        17940,
        17942
      ],
      [
        18346,
        18348
      ],
      [
        18407,
        18409
      ],
      [
        18671,
        18673
      ],
      [
        18796,
        18798
      ],
      [
        20039,
        20041
      ],
      [
        20903,
        20905
      ],
      [
        21738,
        21740
      ],
      [
        21975,
        21977
      ],
      [
        22095,
        22097
      ],
      [
        22223,
        22225
      ],
      [
        22281,
        22287
      ],
      [
        22469,
        22471
      ],
      [
        23298,
        23300
      ],
      [
        24381,
        24383
      ],
      [
        24412,
        24414
      ],
      [
        24756,
        24758
      ],
      [
        24882,
        24884
      ],
      [
        24998,
        25002
      ],
      [
        26624,
        26626
      ],
      [
        27250,
        27254
      ],
      [
        29832,
        29834
      ],
      [
        29923,
        29925
      ],
      [
        30002,
        30004
      ],
      [
        30404,
        30407
      ],
      [
        30473,
        30475
      ],
      [
        30602,
        30605
      ],
      [
        30625,
        30627
      ],
      [
        30785,
        30789
      ],
      [
        30977,
        30980
      ],
      [
        31144,
        31146
      ],
      [
        31221,
        31223
      ],
      [
        31581,
        31583
      ],
      [
        31625,
        31629
      ],
      [
        32189,
        32191
      ],
      [
        32249,
        32251
      ],
      [
        32571,
        32575
      ],
      [
        32702,
        32704
      ],
      [
        32746,
        32750
      ],
      [
        33468,
        33472
      ],
      [
        33783,
        33785
      ],
      [
        34020,
        34022
      ],
      [
        34098,
        34100
      ],
      [
        34239,
        34241
      ],
      [
        34289,
        34291
      ],
      [
        35162,
        35164
      ],
      [
        35456,
        35458
      ],
      [
        35511,
        35513
      ],
      [
        35688,
        35692
      ],
      [
        35743,
        35746
      ],
      [
        35766,
        35768
      ],
      [
        35861,
        35863
      ],
      [
        35912,
        35915
      ],
      [
        36023,
        36025
      ],
      [
        36253,
        36255
      ],
      [
        36732,
        36734
      ],
      [
        36783,
        36785
      ],
      [
        37047,
        37050
      ],
      [
        37150,
        37152
      ],
      [
        37290,
        37292
      ],
      [
        37744,
        37746
      ],
      [
        38100,
        38102
      ],
      [
        38298,
        38301
      ],
      [
        38546,
        38548
      ],
      [
        38729,
        38732
      ],
      [
        38877,
        38879
      ],
      [
        39405,
        39407
      ],
      [
        39856,
        39858
      ],
      [
        43329,
        43331
      ],
      [
        43540,
        43544
      ],
      [
        45411,
        45413
      ],
      [
        45961,
        45963
      ],
      [
        46242,
        46244
      ],
      [
        46356,
        46358
      ],
      [
        46700,
        46702
      ],
      [
        47318,
        47321
      ]
    ],
    [
      [
        413,
        433
      ],
      [
        3752,
        3772
      ],
      [
        4471,
        4491
      ],
      [
        4512,
        4532
      ],
      [
        5526,
        5539
      ],
      [
        21479,
        21499
      ],
      [
        21512,
        21532
      ],
      [
        21655,
        21676
      ],
      [
        21681,
        21706
      ],
      [
        21866,
        21897
      ],
      [
        23727,
        23740
      ],
      [
        25693,
        25713
      ],
      [
        26034,
        26054
      ],
      [
        27504,
        27524
      ],
      [
        28938,
        28960
      ],
      [
        34467,
        34487
      ]
    ],
    [
      [
        427,
        433
      ],
      [
        2854,
        2860
      ],
      [
        3710,
        3716
      ],
      [
        5403,
        5409
      ],
      [
        13797,
        13803
      ],
      [
        25845,
        25851
      ],
      [
        29007,
        29013
      ],
      [
        30339,
        30345
      ],
      [
        30347,
        30354
      ],
      [
        30459,
        30462
      ],
      [
        30770,
        30776
      ],
      [
        34279,
        34285
      ],
      [
        34377,
        34383
      ],
      [
        34815,
        34821
      ],
      [
        38233,
        38239
      ]
    ],
    [
      [
        435,
        458
      ],
      [
        4545,
        4568
      ],
      [
        22740,
        22763
      ],
      [
        22899,
        22925
      ],
      [
        23039,
        23055
      ],
      [
        23205,
        23228
      ],
      [
        23617,
        23633
      ],
      [
        24199,
        24222
      ]
    ],
    [
      [
        460,
        475
      ],
      [
        3821,
        3836
      ],
      [
        4614,
        4629
      ],
      [
        4672,
        4687
      ],
      [
        25092,
        25107
      ],
      [
        25500,
        25515
      ],
      [
        25758,
        25773
      ],
      [
        26899,
        26914
      ],
      [
        26970,
        26985
      ],
      [
        27041,
        27044
      ],
      [
        29261,
        29276
      ],
      [
        29553,
        29568
      ],
      [
        33733,
        33748
      ]
    ],
    [
      [
        478,
        510
      ],
      [
        690,
        722
      ],
      [
        994,
        1027
      ],
      [
        3645,
        3679
      ],
      [
        3874,
        3907
      ],
      [
        13829,
        13900
      ],
      [
        29847,
        29879
      ],
      [
        30020,
        30032
      ],
      [
        31503,
        31535
      ],
      [
        31715,
        31747
      ],
      [
        32014,
        32048
      ],
      [
        32282,
        32314
      ],
      [
        33907,
        33939
      ],
      [
        33970,
        33972
      ],
      [
        34152,
        34162
      ],
      [
        34540,
        34573
      ],
      [
        34690,
        34719
      ],
      [
        47826,
        47859
      ]
    ],
    [
      [
        522,
        540
      ],
      [
        17802,
        17820
      ],
      [
        37916,
        37934
      ]
    ],
    [
      [
        543,
        574
      ],
      [
        4022,
        4053
      ],
      [
        4811,
        4842
      ],
      [
        18613,
        18649
      ],
      [
        18685,
        18688
      ],
      [
        38448,
        38479
      ]
    ],
    [
      [
        582,
        606
      ],
      [
        12382,
        12407
      ],
      [
        12578,
        12603
      ],
      [
        14489,
        14514
      ],
      [
        14544,
        14569
      ],
      [
        15897,
        15922
      ],
      [
        16814,
        16838
      ],
      [
        16876,
        16880
      ],
      [
        18516,
        18541
      ],
      [
        18810,
        18834
      ],
      [
        19143,
        19168
      ],
      [
        19798,
        19808
      ],
      [
        19874,
        19884
      ],
      [
        20013,
        20037
      ],
      [
        20107,
        20132
      ],
      [
        20686,
        20710
      ],
      [
        20950,
        20974
      ],
      [
        30677,
        30702
      ],
      [
        30704,
        30707
      ],
      [
        30845,
        30869
      ],
      [
        31366,
        31391
      ],
      [
        31684,
        31708
      ],
      [
        32412,
        32422
      ],
      [
        32503,
        32513
      ],
      [
        32664,
        32674
      ],
      [
        35565,
        35590
      ],
      [
        37198,
        37223
      ],
      [
        38611,
        38636
      ],
      [
        38642,
        38650
      ],
      [
        47868,
        47892
      ]
    ],
    [
      [
        617,
        629
      ],
      [
        919,
        930
      ]
    ],
    [
      [
        630,
        660
      ],
      [
        11770,
        11799
      ],
      [
        11868,
        11871
      ],
      [
        14973,
        15002
      ],
      [
        16165,
        16195
      ],
      [
        26710,
        26739
      ],
      [
        39871,
        39900
      ],
      [
        39914,
        39918
      ],
      [
        43377,
        43380
      ],
      [
        43461,
        43464
      ],
      [
        43618,
        43621
      ],
      [
        47921,
        47950
      ]
    ],
    [
      [
        725,
        738
      ],
      [
        4288,
        4301
      ],
      [
        6593,
        6606
      ],
      [
        6638,
        6651
      ],
      [
        6779,
        6792
      ],
      [
        7651,
        7664
      ],
      [
        7817,
        7830
      ],
      [
        8274,
        8287
      ],
      [
        8594,
        8607
      ],
      [
        8642,
        8655
      ],
      [
        9188,
        9201
      ],
      [
        23057,
        23070
      ],
      [
        38149,
        38162
      ]
    ],
    [
      [
        751,
        764
      ],
      [
        799,
        812
      ],
      [
        931,
        944
      ]
    ],
    [
      [
        774,
        782
      ]
    ],
    [
      [
        785,
        792
      ]
    ],
    [
      [
        843,
        871
      ],
      [
        9766,
        9792
      ],
      [
        10193,
        10220
      ],
      [
        12458,
        12484
      ],
      [
        13048,
        13061
      ],
      [
        13072,
        13085
      ],
      [
        13188,
        13201
      ],
      [
        13606,
        13619
      ],
      [
        17084,
        17097
      ],
      [
        17548,
        17561
      ],
      [
        17892,
        17918
      ],
      [
        17954,
        17967
      ],
      [
        19455,
        19468
      ],
      [
        19470,
        19477
      ],
      [
        19772,
        19785
      ],
      [
        19847,
        19861
      ],
      [
        20539,
        20552
      ],
      [
        20597,
        20610
      ],
      [
        26542,
        26568
      ],
      [
        26592,
        26618
      ]
    ],
    [
      [
        1030,
        1049
      ],
      [
        1052,
        1055
      ],
      [
        2288,
        2307
      ],
      [
        2540,
        2557
      ],
      [
        2574,
        2587
      ],
      [
        15331,
        15349
      ],
      [
        15351,
        15358
      ],
      [
        25586,
        25589
      ],
      [
        32146,
        32163
      ],
      [
        32166,
        32169
      ],
      [
        34034,
        34051
      ],
      [
        34053,
        34060
      ],
      [
        34757,
        34776
      ],
      [
        38379,
        38396
      ],
      [
        38398,
        38405
      ]
    ],
    [
      [
        1090,
        1120
      ]
    ],
    [
      [
        1090,
        1161
      ],
      [
        1190,
        1202
      ]
    ],
    [
      [
        1122,
        1140
      ]
    ],
    [
      [
        1143,
        1161
      ],
      [
        35060,
        35078
      ]
    ],
    [
      [
        1264,
        1296
      ]
    ],
    [
      [
        1298,
        1311
      ]
    ],
    [
      [
        1313,
        1328
      ],
      [
        35043,
        35058
      ]
    ],
    [
      [
        1337,
        1348
      ]
    ],
    [
      [
        1351,
        1366
      ],
      [
        20338,
        20351
      ],
      [
        39252,
        39267
      ]
    ],
    [
      [
        1391,
        1408
      ]
    ],
    [
      [
        1429,
        1448
      ],
      [
        1556,
        1562
      ]
    ],
    [
      [
        1517,
        1525
      ],
      [
        2007,
        2010
      ],
      [
        2057,
        2060
      ],
      [
        2141,
        2143
      ],
      [
        18709,
        18711
      ],
      [
        18731,
        18735
      ],
      [
        23538,
        23541
      ]
    ],
    [
      [
        1712,
        1723
      ],
      [
        2952,
        2962
      ],
      [
        6027,
        6037
      ],
      [
        6385,
        6395
      ],
      [
        23674,
        23684
      ],
      [
        47892,
        47902
      ]
    ],
    [
      [
        1836,
        1864
      ]
    ],
    [
      [
        1893,
        1916
      ]
    ],
    [
      [
        1956,
        1971
      ]
    ],
    [
      [
        2338,
        2355
      ],
      [
        16541,
        16548
      ],
      [
        16970,
        16977
      ],
      [
        19973,
        19980
      ],
      [
        20159,
        20166
      ],
      [
        20806,
        20813
      ],
      [
        21003,
        21010
      ]
    ],
    [
      [
        2375,
        2442
      ],
      [
        16534,
        16548
      ]
    ],
    [
      [
        2798,
        2808
      ]
    ],
    [
      [
        2930,
        2938
      ],
      [
        3074,
        3085
      ]
    ],
    [
      [
        2995,
        3008
      ]
    ],
    [
      [
        3022,
        3038
      ]
    ],
    [
      [
        3527,
        3549
      ],
      [
        4211,
        4231
      ],
      [
        4977,
        4997
      ],
      [
        4997,
        5017
      ],
      [
        5113,
        5115
      ],
      [
        5245,
        5265
      ],
      [
        5473,
        5493
      ],
      [
        5934,
        5956
      ],
      [
        6520,
        6527
      ],
      [
        30220,
        30264
      ]
    ],
    [
      [
        3580,
        3587
      ],
      [
        6352,
        6358
      ],
      [
        6396,
        6402
      ],
      [
        6434,
        6437
      ],
      [
        9038,
        9044
      ],
      [
        9337,
        9343
      ],
      [
        9406,
        9412
      ],
      [
        9738,
        9744
      ],
      [
        9935,
        9941
      ],
      [
        10112,
        10118
      ],
      [
        10417,
        10423
      ],
      [
        15435,
        15441
      ],
      [
        15705,
        15717
      ],
      [
        15776,
        15782
      ],
      [
        16222,
        16228
      ],
      [
        16288,
        16294
      ],
      [
        16359,
        16361
      ],
      [
        22814,
        22820
      ],
      [
        22822,
        22829
      ],
      [
        22988,
        22994
      ],
      [
        23700,
        23706
      ],
      [
        23708,
        23715
      ],
      [
        23858,
        23864
      ],
      [
        24341,
        24347
      ],
      [
        24554,
        24560
      ],
      [
        24626,
        24632
      ],
      [
        25003,
        25009
      ],
      [
        26578,
        26584
      ],
      [
        27110,
        27116
      ],
      [
        27526,
        27532
      ],
      [
        27710,
        27716
      ],
      [
        27976,
        27982
      ],
      [
        32576,
        32582
      ],
      [
        32751,
        32757
      ],
      [
        32945,
        32951
      ],
      [
        32988,
        32991
      ],
      [
        33078,
        33085
      ],
      [
        33473,
        33479
      ],
      [
        33493,
        33496
      ],
      [
        34799,
        34805
      ],
      [
        35296,
        35302
      ],
      [
        43343,
        43350
      ],
      [
        43390,
        43397
      ],
      [
        43487,
        43494
      ],
      [
        43545,
        43551
      ],
      [
        43582,
        43588
      ],
      [
        43678,
        43684
      ],
      [
        43921,
        43928
      ],
      [
        44113,
        44119
      ],
      [
        46949,
        46955
      ],
      [
        47066,
        47072
      ],
      [
        47575,
        47581
      ]
    ],
    [
      [
        3682,
        3697
      ],
      [
        3931,
        3947
      ]
    ],
    [
      [
        3774,
        3818
      ]
    ],
    [
      [
        3966,
        3987
      ],
      [
        4691,
        4712
      ],
      [
        4733,
        4754
      ],
      [
        4767,
        4788
      ],
      [
        29796,
        29817
      ],
      [
        30199,
        30220
      ],
      [
        30536,
        30558
      ],
      [
        30790,
        30811
      ],
      [
        31630,
        31651
      ],
      [
        31834,
        31855
      ],
      [
        35100,
        35121
      ],
      [
        35177,
        35198
      ],
      [
        35693,
        35714
      ],
      [
        36698,
        36722
      ],
      [
        36724,
        36731
      ],
      [
        47805,
        47826
      ]
    ],
    [
      [
        4109,
        4131
      ],
      [
        4133,
        4140
      ]
    ],
    [
      [
        4277,
        4286
      ],
      [
        6582,
        6591
      ],
      [
        6654,
        6663
      ],
      [
        6702,
        6711
      ],
      [
        7693,
        7702
      ],
      [
        7781,
        7790
      ],
      [
        8660,
        8669
      ],
      [
        9204,
        9213
      ],
      [
        10653,
        10662
      ],
      [
        16653,
        16662
      ],
      [
        26471,
        26480
      ],
      [
        38165,
        38174
      ],
      [
        38339,
        38348
      ],
      [
        47859,
        47868
      ]
    ],
    [
      [
        4304,
        4326
      ],
      [
        6609,
        6631
      ],
      [
        8701,
        8724
      ],
      [
        8742,
        8764
      ]
    ],
    [
      [
        4353,
        4366
      ],
      [
        4856,
        4869
      ],
      [
        9165,
        9178
      ],
      [
        9681,
        9694
      ],
      [
        38485,
        38498
      ]
    ],
    [
      [
        4431,
        4453
      ],
      [
        18767,
        18789
      ],
      [
        20289,
        20311
      ],
      [
        20313,
        20331
      ],
      [
        20354,
        20357
      ],
      [
        21021,
        21043
      ],
      [
        21059,
        21081
      ]
    ],
    [
      [
        4581,
        4600
      ],
      [
        23591,
        23610
      ],
      [
        23650,
        23669
      ],
      [
        23818,
        23842
      ],
      [
        24058,
        24077
      ],
      [
        24122,
        24142
      ]
    ],
    [
      [
        4632,
        4650
      ],
      [
        25110,
        25128
      ]
    ],
    [
      [
        4870,
        4879
      ],
      [
        12430,
        12440
      ],
      [
        13092,
        13101
      ],
      [
        15945,
        15955
      ],
      [
        17996,
        18005
      ],
      [
        18139,
        18148
      ],
      [
        18363,
        18372
      ],
      [
        18578,
        18587
      ],
      [
        30708,
        30717
      ],
      [
        30872,
        30882
      ],
      [
        31608,
        31617
      ],
      [
        38499,
        38508
      ],
      [
        38695,
        38704
      ],
      [
        38766,
        38775
      ],
      [
        39058,
        39067
      ],
      [
        39180,
        39189
      ],
      [
        39191,
        39193
      ],
      [
        39329,
        39338
      ],
      [
        47348,
        47357
      ],
      [
        47652,
        47661
      ]
    ],
    [
      [
        4912,
        4953
      ],
      [
        45870,
        45911
      ]
    ],
    [
      [
        5026,
        5036
      ],
      [
        5515,
        5525
      ],
      [
        9350,
        9359
      ],
      [
        9361,
        9368
      ],
      [
        9463,
        9473
      ],
      [
        9486,
        9496
      ],
      [
        9948,
        9957
      ],
      [
        11346,
        11355
      ],
      [
        13732,
        13742
      ],
      [
        13902,
        13911
      ],
      [
        13987,
        13989
      ],
      [
        14097,
        14106
      ],
      [
        15667,
        15677
      ],
      [
        17610,
        17618
      ],
      [
        33842,
        33852
      ],
      [
        34121,
        34130
      ],
      [
        34265,
        34274
      ],
      [
        34332,
        34342
      ],
      [
        34416,
        34426
      ],
      [
        35469,
        35478
      ],
      [
        35676,
        35686
      ],
      [
        36600,
        36610
      ],
      [
        36907,
        36917
      ],
      [
        46628,
        46638
      ],
      [
        46966,
        46974
      ],
      [
        46980,
        46982
      ]
    ],
    [
      [
        5048,
        5055
      ],
      [
        5551,
        5558
      ],
      [
        6094,
        6102
      ]
    ],
    [
      [
        5149,
        5155
      ],
      [
        5157,
        5164
      ]
    ],
    [
      [
        5370,
        5380
      ],
      [
        5382,
        5389
      ]
    ],
    [
      [
        5708,
        5735
      ]
    ],
    [
      [
        5741,
        5782
      ]
    ],
    [
      [
        5808,
        5829
      ],
      [
        5845,
        5849
      ]
    ],
    [
      [
        6060,
        6077
      ]
    ],
    [
      [
        6187,
        6212
      ],
      [
        10135,
        10147
      ],
      [
        11487,
        11499
      ],
      [
        11547,
        11559
      ],
      [
        13464,
        13476
      ],
      [
        15864,
        15877
      ],
      [
        16328,
        16340
      ],
      [
        16362,
        16374
      ],
      [
        24305,
        24317
      ],
      [
        40163,
        40175
      ],
      [
        40311,
        40323
      ],
      [
        43805,
        43817
      ],
      [
        43997,
        44009
      ],
      [
        44194,
        44206
      ],
      [
        45451,
        45463
      ],
      [
        47358,
        47370
      ]
    ],
    [
      [
        6217,
        6226
      ],
      [
        6264,
        6276
      ]
    ],
    [
      [
        6489,
        6507
      ]
    ],
    [
      [
        6681,
        6698
      ],
      [
        6712,
        6727
      ],
      [
        6751,
        6754
      ],
      [
        6880,
        6883
      ],
      [
        6962,
        6965
      ],
      [
        7874,
        7891
      ],
      [
        8100,
        8115
      ],
      [
        8453,
        8470
      ],
      [
        8987,
        9004
      ]
    ],
    [
      [
        6728,
        6742
      ],
      [
        6838,
        6862
      ],
      [
        6977,
        7000
      ]
    ],
    [
      [
        6795,
        6809
      ],
      [
        6819,
        6834
      ],
      [
        7197,
        7220
      ],
      [
        7610,
        7622
      ],
      [
        8142,
        8156
      ]
    ],
    [
      [
        6932,
        6948
      ],
      [
        8499,
        8515
      ],
      [
        8519,
        8521
      ],
      [
        8948,
        8964
      ]
    ],
    [
      [
        7012,
        7027
      ]
    ],
    [
      [
        7034,
        7050
      ],
      [
        7064,
        7067
      ]
    ],
    [
      [
        7158,
        7165
      ],
      [
        7357,
        7364
      ],
      [
        8361,
        8368
      ],
      [
        11930,
        11936
      ],
      [
        11966,
        11972
      ],
      [
        12113,
        12119
      ]
    ],
    [
      [
        7246,
        7254
      ]
    ],
    [
      [
        7417,
        7435
      ],
      [
        7855,
        7873
      ],
      [
        8434,
        8452
      ],
      [
        8968,
        8986
      ]
    ],
    [
      [
        7508,
        7531
      ],
      [
        8891,
        8914
      ]
    ],
    [
      [
        7533,
        7547
      ],
      [
        8917,
        8931
      ],
      [
        10505,
        10519
      ],
      [
        12163,
        12177
      ],
      [
        13533,
        13547
      ],
      [
        14411,
        14425
      ],
      [
        16732,
        16746
      ],
      [
        17734,
        17748
      ],
      [
        17750,
        17757
      ],
      [
        20187,
        20201
      ],
      [
        20465,
        20479
      ],
      [
        20920,
        20934
      ],
      [
        21177,
        21191
      ],
      [
        38193,
        38207
      ],
      [
        39268,
        39282
      ],
      [
        39526,
        39540
      ],
      [
        39546,
        39560
      ],
      [
        39622,
        39624
      ],
      [
        39841,
        39855
      ],
      [
        40048,
        40062
      ],
      [
        43361,
        43376
      ],
      [
        43602,
        43617
      ],
      [
        44332,
        44346
      ],
      [
        46081,
        46095
      ],
      [
        46152,
        46166
      ],
      [
        46583,
        46597
      ]
    ],
    [
      [
        7550,
        7566
      ],
      [
        25667,
        25683
      ],
      [
        25807,
        25822
      ],
      [
        26325,
        26341
      ]
    ],
    [
      [
        7735,
        7747
      ],
      [
        7760,
        7763
      ]
    ],
    [
      [
        7841,
        7854
      ]
    ],
    [
      [
        7893,
        7905
      ],
      [
        7938,
        7940
      ]
    ],
    [
      [
        7953,
        7962
      ],
      [
        43468,
        43477
      ]
    ],
    [
      [
        8046,
        8073
      ]
    ],
    [
      [
        8324,
        8333
      ],
      [
        8402,
        8411
      ]
    ],
    [
      [
        9045,
        9104
      ],
      [
        9108,
        9112
      ]
    ],
    [
      [
        9274,
        9290
      ],
      [
        40521,
        40539
      ]
    ],
    [
      [
        9295,
        9311
      ]
    ],
    [
      [
        9445,
        9459
      ],
      [
        10918,
        10932
      ],
      [
        23170,
        23184
      ],
      [
        23272,
        23286
      ]
    ],
    [
      [
        9541,
        9571
      ]
    ],
    [
      [
        9587,
        9605
      ],
      [
        13407,
        13425
      ]
    ],
    [
      [
        9868,
        9891
      ],
      [
        15986,
        16008
      ],
      [
        35532,
        35554
      ]
    ],
    [
      [
        9962,
        9982
      ],
      [
        16089,
        16109
      ],
      [
        16428,
        16448
      ],
      [
        45921,
        45951
      ],
      [
        45953,
        45960
      ],
      [
        46001,
        46004
      ]
    ],
    [
      [
        10026,
        10037
      ],
      [
        10405,
        10416
      ],
      [
        13311,
        13322
      ],
      [
        13395,
        13406
      ],
      [
        13757,
        13768
      ],
      [
        14144,
        14156
      ],
      [
        22610,
        22621
      ],
      [
        23158,
        23169
      ],
      [
        27273,
        27284
      ],
      [
        29960,
        29972
      ],
      [
        35965,
        35976
      ],
      [
        36133,
        36144
      ],
      [
        36172,
        36184
      ],
      [
        36285,
        36297
      ],
      [
        36629,
        36640
      ],
      [
        37127,
        37138
      ],
      [
        39436,
        39447
      ],
      [
        47563,
        47574
      ]
    ],
    [
      [
        10307,
        10315
      ]
    ],
    [
      [
        10378,
        10391
      ],
      [
        10456,
        10462
      ],
      [
        11480,
        11486
      ],
      [
        11540,
        11546
      ],
      [
        11763,
        11769
      ],
      [
        12248,
        12254
      ],
      [
        13457,
        13463
      ],
      [
        17151,
        17157
      ],
      [
        36938,
        36944
      ],
      [
        36964,
        36970
      ],
      [
        40156,
        40162
      ],
      [
        43798,
        43804
      ],
      [
        43990,
        43996
      ],
      [
        47592,
        47598
      ]
    ],
    [
      [
        10556,
        10570
      ],
      [
        10631,
        10633
      ],
      [
        10691,
        10701
      ],
      [
        11410,
        11414
      ],
      [
        15546,
        15560
      ],
      [
        18421,
        18435
      ],
      [
        18446,
        18449
      ],
      [
        21372,
        21387
      ],
      [
        23315,
        23329
      ],
      [
        23413,
        23417
      ],
      [
        35368,
        35382
      ],
      [
        36475,
        36490
      ],
      [
        38862,
        38876
      ],
      [
        45494,
        45508
      ],
      [
        46685,
        46699
      ],
      [
        47159,
        47174
      ]
    ],
    [
      [
        10579,
        10592
      ]
    ],
    [
      [
        10604,
        10612
      ],
      [
        18467,
        18474
      ],
      [
        18481,
        18487
      ],
      [
        18557,
        18563
      ],
      [
        46732,
        46739
      ],
      [
        46741,
        46748
      ],
      [
        46831,
        46844
      ]
    ],
    [
      [
        10670,
        10689
      ]
    ],
    [
      [
        10710,
        10723
      ],
      [
        10782,
        10789
      ],
      [
        11234,
        11247
      ],
      [
        11303,
        11316
      ]
    ],
    [
      [
        10718,
        10723
      ],
      [
        10808,
        10813
      ],
      [
        10834,
        10839
      ],
      [
        21456,
        21461
      ],
      [
        46717,
        46721
      ]
    ],
    [
      [
        10728,
        10743
      ]
    ],
    [
      [
        10736,
        10743
      ],
      [
        46824,
        46830
      ],
      [
        46846,
        46853
      ]
    ],
    [
      [
        10870,
        10887
      ]
    ],
    [
      [
        10935,
        10943
      ]
    ],
    [
      [
        11036,
        11043
      ],
      [
        11134,
        11143
      ],
      [
        11157,
        11166
      ]
    ],
    [
      [
        11047,
        11054
      ]
    ],
    [
      [
        11190,
        11200
      ]
    ],
    [
      [
        11274,
        11281
      ]
    ],
    [
      [
        11503,
        11520
      ],
      [
        12198,
        12215
      ],
      [
        14044,
        14061
      ],
      [
        21828,
        21837
      ],
      [
        25312,
        25321
      ],
      [
        27067,
        27084
      ],
      [
        27638,
        27647
      ],
      [
        35977,
        35994
      ],
      [
        47007,
        47024
      ]
    ],
    [
      [
        11525,
        11559
      ],
      [
        11621,
        11642
      ]
    ],
    [
      [
        11585,
        11607
      ],
      [
        13263,
        13282
      ],
      [
        15411,
        15434
      ],
      [
        16758,
        16781
      ],
      [
        17161,
        17184
      ],
      [
        17227,
        17242
      ]
    ],
    [
      [
        11805,
        11834
      ],
      [
        11887,
        11890
      ],
      [
        11989,
        11992
      ],
      [
        17023,
        17052
      ],
      [
        26798,
        26827
      ]
    ],
    [
      [
        12306,
        12311
      ],
      [
        14227,
        14232
      ],
      [
        14333,
        14338
      ],
      [
        14434,
        14439
      ],
      [
        14967,
        14972
      ],
      [
        18952,
        18958
      ],
      [
        19048,
        19053
      ],
      [
        19180,
        19185
      ],
      [
        26356,
        26362
      ],
      [
        29982,
        29987
      ],
      [
        36315,
        36320
      ],
      [
        37323,
        37329
      ],
      [
        37517,
        37522
      ]
    ],
    [
      [
        12312,
        12332
      ],
      [
        15754,
        15775
      ],
      [
        16034,
        16055
      ],
      [
        16679,
        16699
      ],
      [
        17101,
        17122
      ],
      [
        17209,
        17222
      ],
      [
        17244,
        17264
      ]
    ],
    [
      [
        12368,
        12381
      ],
      [
        13174,
        13187
      ],
      [
        15882,
        15896
      ],
      [
        18502,
        18515
      ],
      [
        26756,
        26769
      ],
      [
        30729,
        30736
      ],
      [
        30749,
        30752
      ],
      [
        30915,
        30923
      ],
      [
        31596,
        31603
      ],
      [
        35597,
        35611
      ],
      [
        38651,
        38664
      ],
      [
        39290,
        39303
      ],
      [
        43818,
        43831
      ]
    ],
    [
      [
        12548,
        12603
      ],
      [
        12604,
        12615
      ],
      [
        12664,
        12667
      ],
      [
        12709,
        12716
      ]
    ],
    [
      [
        12556,
        12577
      ],
      [
        20236,
        20257
      ],
      [
        20502,
        20523
      ],
      [
        20576,
        20585
      ],
      [
        24593,
        24614
      ],
      [
        24860,
        24881
      ],
      [
        31032,
        31053
      ],
      [
        31087,
        31110
      ]
    ],
    [
      [
        12751,
        12763
      ],
      [
        12990,
        12997
      ]
    ],
    [
      [
        12802,
        12811
      ],
      [
        12859,
        12867
      ]
    ],
    [
      [
        12826,
        12837
      ],
      [
        12945,
        12961
      ]
    ],
    [
      [
        12871,
        12889
      ]
    ],
    [
      [
        12891,
        12902
      ]
    ],
    [
      [
        12927,
        12943
      ],
      [
        20658,
        20674
      ]
    ],
    [
      [
        13016,
        13023
      ],
      [
        14832,
        14839
      ]
    ],
    [
      [
        13138,
        13146
      ],
      [
        37345,
        37352
      ]
    ],
    [
      [
        13211,
        13219
      ],
      [
        13225,
        13232
      ]
    ],
    [
      [
        13364,
        13382
      ],
      [
        13698,
        13716
      ],
      [
        14449,
        14467
      ],
      [
        15012,
        15030
      ],
      [
        47094,
        47111
      ]
    ],
    [
      [
        13577,
        13588
      ],
      [
        14515,
        14518
      ],
      [
        16841,
        16844
      ],
      [
        17677,
        17688
      ],
      [
        18334,
        18345
      ]
    ],
    [
      [
        13723,
        13742
      ],
      [
        13749,
        13756
      ]
    ],
    [
      [
        13770,
        13783
      ],
      [
        13929,
        13942
      ]
    ],
    [
      [
        14131,
        14143
      ],
      [
        14208,
        14220
      ],
      [
        14314,
        14326
      ],
      [
        29947,
        29959
      ],
      [
        31074,
        31086
      ],
      [
        36159,
        36171
      ],
      [
        36272,
        36284
      ],
      [
        37332,
        37344
      ],
      [
        37388,
        37400
      ],
      [
        37653,
        37665
      ]
    ],
    [
      [
        14235,
        14247
      ],
      [
        14276,
        14288
      ],
      [
        25644,
        25656
      ],
      [
        28322,
        28334
      ],
      [
        28892,
        28905
      ],
      [
        29163,
        29175
      ]
    ],
    [
      [
        14248,
        14262
      ],
      [
        14289,
        14291
      ]
    ],
    [
      [
        14292,
        14301
      ],
      [
        14305,
        14313
      ],
      [
        14339,
        14349
      ],
      [
        20227,
        20235
      ],
      [
        20492,
        20501
      ],
      [
        37074,
        37084
      ],
      [
        37231,
        37240
      ],
      [
        39171,
        39179
      ],
      [
        39427,
        39435
      ]
    ],
    [
      [
        14361,
        14372
      ],
      [
        14877,
        14880
      ],
      [
        20442,
        20453
      ],
      [
        36213,
        36224
      ]
    ],
    [
      [
        14582,
        14602
      ],
      [
        14685,
        14687
      ]
    ],
    [
      [
        14605,
        14608
      ],
      [
        14871,
        14874
      ],
      [
        20205,
        20216
      ]
    ],
    [
      [
        14618,
        14629
      ],
      [
        14744,
        14754
      ],
      [
        15088,
        15098
      ],
      [
        15104,
        15111
      ],
      [
        15166,
        15175
      ],
      [
        15261,
        15263
      ],
      [
        38714,
        38724
      ],
      [
        39226,
        39235
      ],
      [
        39238,
        39245
      ],
      [
        39573,
        39582
      ],
      [
        39597,
        39607
      ],
      [
        39749,
        39760
      ],
      [
        46016,
        46026
      ]
    ],
    [
      [
        14663,
        14672
      ],
      [
        14795,
        14804
      ],
      [
        31410,
        31418
      ]
    ],
    [
      [
        14705,
        14722
      ]
    ],
    [
      [
        14723,
        14734
      ],
      [
        14764,
        14766
      ],
      [
        14810,
        14813
      ]
    ],
    [
      [
        15190,
        15211
      ],
      [
        15238,
        15260
      ]
    ],
    [
      [
        15526,
        15544
      ],
      [
        35333,
        35351
      ]
    ],
    [
      [
        15562,
        15588
      ]
    ],
    [
      [
        15593,
        15616
      ],
      [
        15623,
        15643
      ]
    ],
    [
      [
        15809,
        15814
      ],
      [
        15827,
        15836
      ]
    ],
    [
      [
        16155,
        16160
      ],
      [
        16405,
        16410
      ]
    ],
    [
      [
        16206,
        16221
      ],
      [
        16480,
        16495
      ],
      [
        17873,
        17888
      ],
      [
        24430,
        24445
      ],
      [
        26515,
        26530
      ],
      [
        26659,
        26674
      ]
    ],
    [
      [
        16718,
        16746
      ],
      [
        16748,
        16755
      ]
    ],
    [
      [
        16910,
        16932
      ],
      [
        17586,
        17608
      ],
      [
        17619,
        17627
      ],
      [
        17693,
        17715
      ]
    ],
    [
      [
        16924,
        16932
      ],
      [
        17600,
        17608
      ],
      [
        17707,
        17715
      ],
      [
        17812,
        17820
      ]
    ],
    [
      [
        16938,
        16945
      ],
      [
        16956,
        16977
      ],
      [
        19959,
        19980
      ],
      [
        20145,
        20166
      ],
      [
        20792,
        20813
      ],
      [
        20989,
        21010
      ]
    ],
    [
      [
        16978,
        16980
      ]
    ],
    [
      [
        17143,
        17157
      ]
    ],
    [
      [
        17425,
        17432
      ],
      [
        17434,
        17441
      ]
    ],
    [
      [
        17670,
        17688
      ],
      [
        17721,
        17725
      ]
    ],
    [
      [
        18880,
        18910
      ]
    ],
    [
      [
        18923,
        18941
      ],
      [
        19058,
        19066
      ],
      [
        25217,
        25224
      ],
      [
        25350,
        25369
      ]
    ],
    [
      [
        18969,
        18999
      ],
      [
        19010,
        19016
      ],
      [
        19201,
        19220
      ],
      [
        19222,
        19239
      ],
      [
        25178,
        25189
      ]
    ],
    [
      [
        19525,
        19539
      ]
    ],
    [
      [
        19605,
        19609
      ],
      [
        19695,
        19700
      ]
    ],
    [
      [
        19614,
        19617
      ],
      [
        19705,
        19709
      ]
    ],
    [
      [
        19897,
        19914
      ],
      [
        19993,
        20007
      ]
    ],
    [
      [
        20058,
        20079
      ]
    ],
    [
      [
        20221,
        20226
      ],
      [
        20486,
        20491
      ],
      [
        39421,
        39426
      ]
    ],
    [
      [
        20404,
        20429
      ],
      [
        21106,
        21131
      ]
    ],
    [
      [
        20568,
        20575
      ],
      [
        20913,
        20919
      ]
    ],
    [
      [
        20630,
        20654
      ],
      [
        20713,
        20737
      ],
      [
        20816,
        20840
      ]
    ],
    [
      [
        20751,
        20759
      ]
    ],
    [
      [
        21134,
        21151
      ],
      [
        21197,
        21214
      ],
      [
        21323,
        21340
      ],
      [
        21406,
        21408
      ]
    ],
    [
      [
        21167,
        21176
      ],
      [
        21425,
        21433
      ],
      [
        21444,
        21452
      ]
    ],
    [
      [
        21259,
        21265
      ]
    ],
    [
      [
        21273,
        21287
      ],
      [
        21310,
        21322
      ]
    ],
    [
      [
        21293,
        21295
      ]
    ],
    [
      [
        21559,
        21568
      ],
      [
        27324,
        27340
      ]
    ],
    [
      [
        21571,
        21586
      ]
    ],
    [
      [
        21615,
        21640
      ],
      [
        24721,
        24746
      ]
    ],
    [
      [
        21627,
        21640
      ],
      [
        21717,
        21730
      ],
      [
        22506,
        22519
      ],
      [
        22523,
        22530
      ],
      [
        22557,
        22570
      ],
      [
        25396,
        25409
      ],
      [
        25913,
        25926
      ],
      [
        27922,
        27935
      ],
      [
        30307,
        30320
      ],
      [
        30445,
        30458
      ],
      [
        30938,
        30951
      ],
      [
        31193,
        31206
      ],
      [
        31282,
        31295
      ],
      [
        31301,
        31309
      ],
      [
        31951,
        31964
      ],
      [
        32355,
        32368
      ],
      [
        32373,
        32381
      ],
      [
        32455,
        32468
      ],
      [
        32626,
        32639
      ],
      [
        32787,
        32800
      ],
      [
        35231,
        35244
      ],
      [
        35479,
        35492
      ]
    ],
    [
      [
        21779,
        21806
      ],
      [
        21813,
        21821
      ]
    ],
    [
      [
        21803,
        21806
      ],
      [
        22143,
        22146
      ],
      [
        22178,
        22181
      ],
      [
        24587,
        24591
      ],
      [
        24712,
        24716
      ],
      [
        25037,
        25041
      ],
      [
        35758,
        35761
      ],
      [
        35787,
        35790
      ]
    ],
    [
      [
        21840,
        21856
      ],
      [
        25324,
        25340
      ],
      [
        26075,
        26091
      ],
      [
        26187,
        26203
      ],
      [
        27595,
        27611
      ],
      [
        27655,
        27658
      ],
      [
        47392,
        47408
      ]
    ],
    [
      [
        21904,
        21914
      ],
      [
        21925,
        21927
      ]
    ],
    [
      [
        21998,
        22012
      ],
      [
        22494,
        22502
      ],
      [
        25134,
        25148
      ],
      [
        25167,
        25174
      ]
    ],
    [
      [
        22157,
        22161
      ],
      [
        22459,
        22463
      ]
    ],
    [
      [
        22166,
        22181
      ],
      [
        22185,
        22192
      ]
    ],
    [
      [
        22232,
        22239
      ],
      [
        22288,
        22296
      ],
      [
        22486,
        22493
      ]
    ],
    [
      [
        22264,
        22275
      ],
      [
        22335,
        22339
      ],
      [
        22377,
        22381
      ],
      [
        22406,
        22410
      ]
    ],
    [
      [
        22862,
        22873
      ]
    ],
    [
      [
        22996,
        23018
      ],
      [
        23020,
        23027
      ],
      [
        23115,
        23128
      ],
      [
        23140,
        23143
      ]
    ],
    [
      [
        23343,
        23351
      ],
      [
        23502,
        23510
      ],
      [
        23577,
        23585
      ]
    ],
    [
      [
        23355,
        23366
      ],
      [
        23547,
        23557
      ]
    ],
    [
      [
        24160,
        24178
      ],
      [
        24239,
        24245
      ]
    ],
    [
      [
        24777,
        24799
      ],
      [
        24932,
        24954
      ]
    ],
    [
      [
        24804,
        24814
      ],
      [
        24969,
        24979
      ]
    ],
    [
      [
        25178,
        25274
      ],
      [
        29038,
        29085
      ],
      [
        33233,
        33280
      ]
    ],
    [
      [
        25486,
        25491
      ],
      [
        28186,
        28187
      ],
      [
        29493,
        29496
      ]
    ],
    [
      [
        25855,
        25860
      ],
      [
        25934,
        25939
      ],
      [
        36948,
        36953
      ]
    ],
    [
      [
        25863,
        25867
      ],
      [
        25891,
        25895
      ],
      [
        25970,
        25974
      ],
      [
        36974,
        36978
      ]
    ],
    [
      [
        25944,
        25966
      ],
      [
        26093,
        26103
      ],
      [
        26204,
        26219
      ],
      [
        26926,
        26941
      ]
    ],
    [
      [
        25984,
        26003
      ]
    ],
    [
      [
        26018,
        26025
      ]
    ],
    [
      [
        26240,
        26251
      ]
    ],
    [
      [
        26414,
        26440
      ]
    ],
    [
      [
        26445,
        26461
      ]
    ],
    [
      [
        26691,
        26706
      ],
      [
        26780,
        26795
      ]
    ],
    [
      [
        26844,
        26851
      ]
    ],
    [
      [
        26954,
        26969
      ],
      [
        26990,
        26997
      ]
    ],
    [
      [
        27090,
        27097
      ],
      [
        27102,
        27109
      ],
      [
        30364,
        30372
      ],
      [
        31009,
        31016
      ],
      [
        31059,
        31066
      ],
      [
        32389,
        32396
      ],
      [
        33211,
        33218
      ],
      [
        33292,
        33295
      ],
      [
        33385,
        33393
      ],
      [
        34174,
        34181
      ],
      [
        34514,
        34521
      ],
      [
        36145,
        36152
      ],
      [
        36641,
        36648
      ]
    ],
    [
      [
        27231,
        27249
      ],
      [
        27814,
        27832
      ]
    ],
    [
      [
        27295,
        27302
      ],
      [
        27366,
        27375
      ]
    ],
    [
      [
        27313,
        27319
      ],
      [
        27398,
        27404
      ],
      [
        27438,
        27445
      ]
    ],
    [
      [
        27377,
        27384
      ]
    ],
    [
      [
        27446,
        27465
      ],
      [
        27470,
        27480
      ],
      [
        27731,
        27741
      ]
    ],
    [
      [
        27912,
        27935
      ]
    ],
    [
      [
        27961,
        27963
      ],
      [
        27968,
        27975
      ]
    ],
    [
      [
        28013,
        28034
      ],
      [
        28052,
        28057
      ]
    ],
    [
      [
        28024,
        28034
      ],
      [
        43967,
        43977
      ],
      [
        44083,
        44093
      ]
    ],
    [
      [
        28111,
        28126
      ]
    ],
    [
      [
        28129,
        28142
      ]
    ],
    [
      [
        28276,
        28286
      ],
      [
        28378,
        28386
      ],
      [
        28451,
        28459
      ]
    ],
    [
      [
        28335,
        28347
      ],
      [
        28906,
        28918
      ],
      [
        29176,
        29188
      ]
    ],
    [
      [
        28387,
        28394
      ]
    ],
    [
      [
        28487,
        28514
      ],
      [
        28845,
        28872
      ]
    ],
    [
      [
        28655,
        28660
      ],
      [
        28662,
        28669
      ]
    ],
    [
      [
        28690,
        28697
      ],
      [
        28769,
        28776
      ]
    ],
    [
      [
        28998,
        29013
      ]
    ],
    [
      [
        29026,
        29037
      ]
    ],
    [
      [
        29100,
        29116
      ],
      [
        29473,
        29488
      ]
    ],
    [
      [
        29132,
        29138
      ],
      [
        29146,
        29153
      ]
    ],
    [
      [
        29322,
        29337
      ]
    ],
    [
      [
        29339,
        29399
      ]
    ],
    [
      [
        29664,
        29671
      ],
      [
        29673,
        29680
      ]
    ],
    [
      [
        29750,
        29770
      ]
    ],
    [
      [
        29775,
        29792
      ]
    ],
    [
      [
        30132,
        30153
      ]
    ],
    [
      [
        30156,
        30185
      ]
    ],
    [
      [
        30276,
        30306
      ],
      [
        30284,
        30306
      ]
    ],
    [
      [
        30330,
        30333
      ]
    ],
    [
      [
        30376,
        30402
      ],
      [
        35819,
        35840
      ]
    ],
    [
      [
        30483,
        30515
      ],
      [
        30517,
        30524
      ]
    ],
    [
      [
        30590,
        30598
      ],
      [
        31557,
        31576
      ]
    ],
    [
      [
        30611,
        30622
      ],
      [
        30639,
        30648
      ]
    ],
    [
      [
        30658,
        30673
      ],
      [
        35800,
        35815
      ]
    ],
    [
      [
        30819,
        30828
      ],
      [
        30955,
        30964
      ],
      [
        31210,
        31219
      ],
      [
        31268,
        31277
      ],
      [
        31658,
        31667
      ],
      [
        31941,
        31950
      ]
    ],
    [
      [
        30995,
        31006
      ],
      [
        31115,
        31128
      ]
    ],
    [
      [
        31536,
        31544
      ]
    ],
    [
      [
        31877,
        31909
      ]
    ],
    [
      [
        31966,
        31968
      ]
    ],
    [
      [
        31979,
        31984
      ],
      [
        31990,
        31995
      ]
    ],
    [
      [
        32131,
        32144
      ],
      [
        32186,
        32188
      ]
    ],
    [
      [
        32231,
        32248
      ],
      [
        32316,
        32318
      ],
      [
        36807,
        36826
      ],
      [
        36840,
        36847
      ]
    ],
    [
      [
        32618,
        32639
      ],
      [
        32778,
        32800
      ],
      [
        32831,
        32834
      ]
    ],
    [
      [
        32688,
        32701
      ],
      [
        34734,
        34748
      ],
      [
        34851,
        34864
      ],
      [
        35028,
        35041
      ]
    ],
    [
      [
        32910,
        32934
      ]
    ],
    [
      [
        33319,
        33350
      ]
    ],
    [
      [
        33360,
        33375
      ]
    ],
    [
      [
        33406,
        33411
      ]
    ],
    [
      [
        33431,
        33438
      ],
      [
        34191,
        34198
      ],
      [
        34217,
        34224
      ]
    ],
    [
      [
        33532,
        33544
      ],
      [
        33575,
        33577
      ]
    ],
    [
      [
        33619,
        33632
      ],
      [
        33677,
        33680
      ],
      [
        33751,
        33754
      ]
    ],
    [
      [
        33644,
        33675
      ]
    ],
    [
      [
        33986,
        34000
      ],
      [
        34071,
        34073
      ]
    ],
    [
      [
        34593,
        34605
      ],
      [
        34890,
        34902
      ],
      [
        36863,
        36875
      ],
      [
        37803,
        37815
      ],
      [
        43665,
        43677
      ]
    ],
    [
      [
        34632,
        34634
      ]
    ],
    [
      [
        34646,
        34664
      ],
      [
        34681,
        34689
      ]
    ],
    [
      [
        34749,
        34776
      ],
      [
        34904,
        34928
      ],
      [
        35011,
        35014
      ]
    ],
    [
      [
        34971,
        34995
      ]
    ],
    [
      [
        35221,
        35244
      ]
    ],
    [
      [
        35353,
        35366
      ],
      [
        36518,
        36533
      ]
    ],
    [
      [
        35406,
        35431
      ],
      [
        35443,
        35455
      ]
    ],
    [
      [
        35642,
        35686
      ],
      [
        35947,
        35956
      ]
    ],
    [
      [
        35890,
        35902
      ],
      [
        36243,
        36252
      ]
    ],
    [
      [
        36044,
        36055
      ],
      [
        36185,
        36198
      ]
    ],
    [
      [
        36325,
        36332
      ]
    ],
    [
      [
        36348,
        36352
      ]
    ],
    [
      [
        36412,
        36423
      ],
      [
        36990,
        37001
      ],
      [
        37376,
        37387
      ],
      [
        37505,
        37516
      ],
      [
        37586,
        37597
      ],
      [
        37641,
        37652
      ]
    ],
    [
      [
        36493,
        36512
      ]
    ],
    [
      [
        36536,
        36562
      ]
    ],
    [
      [
        36579,
        36610
      ]
    ],
    [
      [
        37415,
        37425
      ],
      [
        37686,
        37696
      ]
    ],
    [
      [
        37564,
        37597
      ]
    ],
    [
      [
        37611,
        37625
      ]
    ],
    [
      [
        37854,
        37885
      ],
      [
        38118,
        38148
      ]
    ],
    [
      [
        37969,
        37983
      ],
      [
        37984,
        37995
      ],
      [
        38023,
        38026
      ]
    ],
    [
      [
        38262,
        38274
      ]
    ],
    [
      [
        38318,
        38329
      ],
      [
        44180,
        44191
      ],
      [
        45809,
        45820
      ],
      [
        46756,
        46767
      ],
      [
        46800,
        46811
      ]
    ],
    [
      [
        38561,
        38585
      ],
      [
        38589,
        38596
      ]
    ],
    [
      [
        38894,
        38910
      ]
    ],
    [
      [
        38921,
        38942
      ],
      [
        44377,
        44398
      ]
    ],
    [
      [
        38947,
        38966
      ],
      [
        38987,
        38993
      ],
      [
        39046,
        39054
      ]
    ],
    [
      [
        38974,
        38978
      ],
      [
        45764,
        45768
      ]
    ],
    [
      [
        39009,
        39016
      ],
      [
        39018,
        39025
      ]
    ],
    [
      [
        39304,
        39322
      ],
      [
        39326,
        39328
      ]
    ],
    [
      [
        39625,
        39657
      ],
      [
        46049,
        46080
      ]
    ],
    [
      [
        39702,
        39730
      ],
      [
        39732,
        39739
      ]
    ],
    [
      [
        39931,
        39955
      ]
    ],
    [
      [
        39958,
        39982
      ]
    ],
    [
      [
        40015,
        40028
      ],
      [
        40268,
        40281
      ],
      [
        43408,
        43421
      ],
      [
        43770,
        43783
      ],
      [
        43939,
        43952
      ],
      [
        44026,
        44040
      ]
    ],
    [
      [
        40071,
        40089
      ]
    ],
    [
      [
        40119,
        40137
      ],
      [
        40139,
        40146
      ],
      [
        43872,
        43888
      ]
    ],
    [
      [
        40242,
        40259
      ],
      [
        40344,
        40347
      ]
    ],
    [
      [
        40292,
        40297
      ]
    ],
    [
      [
        40398,
        40435
      ],
      [
        40439,
        40446
      ]
    ],
    [
      [
        43513,
        43527
      ]
    ],
    [
      [
        43688,
        43696
      ],
      [
        43727,
        43736
      ]
    ],
    [
      [
        43708,
        43736
      ],
      [
        43834,
        43847
      ]
    ],
    [
      [
        44045,
        44066
      ],
      [
        47374,
        47387
      ]
    ],
    [
      [
        44069,
        44093
      ]
    ],
    [
      [
        44366,
        44407
      ],
      [
        44493,
        44501
      ],
      [
        45464,
        45472
      ],
      [
        45512,
        45520
      ]
    ],
    [
      [
        44422,
        44441
      ],
      [
        44457,
        44460
      ]
    ],
    [
      [
        45442,
        45450
      ]
    ],
    [
      [
        45526,
        45556
      ],
      [
        45572,
        45574
      ]
    ],
    [
      [
        45548,
        45556
      ],
      [
        45600,
        45606
      ],
      [
        45638,
        45644
      ],
      [
        45769,
        45775
      ],
      [
        45856,
        45864
      ]
    ],
    [
      [
        45632,
        45637
      ]
    ],
    [
      [
        45654,
        45673
      ],
      [
        45677,
        45684
      ]
    ],
    [
      [
        45722,
        45730
      ]
    ],
    [
      [
        45749,
        45755
      ],
      [
        47614,
        47620
      ]
    ],
    [
      [
        45783,
        45790
      ],
      [
        45822,
        45835
      ]
    ],
    [
      [
        45792,
        45805
      ]
    ],
    [
      [
        46194,
        46207
      ],
      [
        46287,
        46300
      ]
    ],
    [
      [
        46391,
        46393
      ]
    ],
    [
      [
        46412,
        46432
      ],
      [
        46489,
        46507
      ],
      [
        46863,
        46883
      ]
    ],
    [
      [
        46525,
        46535
      ]
    ],
    [
      [
        46653,
        46671
      ]
    ],
    [
      [
        47191,
        47198
      ],
      [
        47248,
        47255
      ],
      [
        47275,
        47277
      ]
    ],
    [
      [
        47456,
        47486
      ]
    ],
    [
      [
        47523,
        47526
      ]
    ]
  ],
  "includes": [
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    [],
    []
  ],
  "text": "\n\nТеория вероятностей в машинном обучении. Часть 1: модель регрессии / Habr\n\n\n               Теория вероятностей в машинном обучении. Часть 1: модель регрессии  Reading time  \n    28 min\n   Views  14K Open Data Science corporate blog Mathematics *Machine learning *Statistics in IT Artificial Intelligence       В данной статье мы подробно рассмотрим вероятностную постановку задачи машинного обучения: что такое распределение данных, дискриминативная модель, i.i.d.-гипотеза и метод максимизации правдоподобия, что такое регрессия Пуассона и регрессия с оценкой уверенности, и как нормальное распределение связано с минимизацией среднеквадратичного отклонения.В следующей части рассмотрим метод максимизации правдоподобия в классификации: в чем роль кроссэнтропии, функций сигмоиды и softmax и как кроссэнтропия связана с \"расстоянием\" между распределениями вероятностей и почему модель регрессии тоже обучается через минимизацию кроссэнтропии.В третьей части (статья планируется) перейдем от метода максимизации правдоподобия к байесовскому выводу и его различным приближениям, таким как метод апостериорного максимума, методы Монте-Карло и вариационный вывод. Рассмотрим, как применение этих методов порождает типичные для машинного обучения понятия, такие как стохастический градиентный спуск, регуляризация, ансамблирование, подбор архитектуры и гиперпараметров. Также поговорим о роли априорных гипотез в машинном обучении.Данная серия статей не является введением в машинное обучение и предполагает знакомство читателя с основными понятиями. Задача статей - рассмотреть машинное обучение с точки зрения теории вероятностей, что позволит по новому взглянуть на проблему, понять связь машинного обучения со статистикой и лучше понимать формулы из научных статей. Также на описанном материале строятся более сложные темы, такие как вариационные автокодировщики (Kingma and Welling, 2013), нейробайесовские методы (Müller et al., 2021) и даже некоторые теории сознания (Friston et al., 2022).Бывало ли у вас такое, что разбираясь в некой сложной области вам сначала не удается систематизировать в голове всю имеющуюся информацию, а затем вы что-то узнаете или догадываетесь, и пазл внезапно складывается в стройную и непротиворечивую картину? Именно такую роль может сыграть понимание байесовского вывода в машинном обучении.Как писал Пьер-Симон Лаплас в начале XIX века, \"теория вероятностей - это здравый смысл, выраженный в вычислениях\". Поэтому разрабатывать алгоритмы машинного обучения можно и не опираясь на теорию вероятностей и байесовский вывод. Но с изучением этих областей то, что раньше казалось просто здравым смыслом, приобретает большую строгость и обоснования.Впрочем, в машинном обучении любые теории строятся на очень зыбкой почве, поскольку машинное обучение - это не чистая математика, а наука о применении алгоритмов обучения на данных к реальному миру. Любая математическая теория основана на аксиомах и условиях (например, в статистике таким условием часто является  \"независимость и одинаковая распределенность обучающих примеров\"). В реальности эти условия могут не выполняться, и иногда даже не иметь четкого смысла. Об этой теме мы тоже поговорим подробнее.Возможно, изложение в статье покажется слишком подробным и затянутым, но эти вещи невозможно объяснить в двух словах. Большое количество деталей и пояснений позволяет надеяться, что ничего важного не будет упущено, и в понимании не останется пробелов.Содержание текущей частиВ первом разделе мы рассмотрим связь между машинным обучением и статистическим выводом.Во втором разделе поговорим о моделях: рассмотрим вероятностную модель регрессии и ее обучение методом максимизации правдоподобия.В третьем разделе поговорим о данных: рассмотрим понятие вероятностного распределения данных, задачу максимизации метрики на распределении и i.i.d.-гипотезу.В четвертом разделе снова вернемся к методу максимизации правдоподобия, используя материал из третьего раздела, и введем понятие статистической модели.В пятом разделе рассмотрим модель регрессии с оценкой уверенности в виде формул и программного кода.Звездочкой* отмечены дополнительные разделы, которые не повлияют на понимание дальнейшего материала.1. Машинное обучение и статистический вывод2. Вероятностная модель регрессии        2.1. Регрессия, классификация и промежуточные варианты        2.2. Вероятностное моделирование        2.3. Модель регрессии        2.4. Обсуждение        2.5. Функция потерь Хьюбера*3. Вероятностное распределение данных        3.1. Понятие распределения данных        3.2. Дискриминативные модели        3.3. Генеративные модели*        3.4. i.i.d.-гипотеза и качество обобщения        3.5. Проблемы i.i.d.-гипотезы*4. Статистические модели        4.1. Простые статистические модели        4.2. Статистические модели в машинном обучении5. Регрессия с оценкой уверенности*        5.1. Моделирование дисперсии в модели регрессии*        5.2. Регрессия с константной оценкой дисперсии*1. Машинное обучение и статистический выводСтатистический вывод (оценка параметров и проверка гипотез) часто включается в курсы машинного обучения. Но многими он воспринимается лишь как досадная заноза, которая только отнимает время и далее нигде не используется. Однако машинное обучение и статистический вывод тесно связаны и решают почти одну и ту же задачу.Машинное обучение заключается в написании и применении алгоритмов, которые обучаются на данных, автоматически выводя общие закономерности из частных примеров.Статистический вывод заключается в оценке параметров распределений и проверке гипотез на основе наблюдений, то есть опять-таки в получении общих выводов из частных примеров.Процесс выведения общих правил из частных примеров называется обобщением (generalization), или индуктивным выводом (inductive inference). Чем же тогда отличаются эти два раздела науки? Граница между ними довольно расплывчата. Вообще говоря, большую часть машинного обучения можно считать статистическим выводом, что мы более формально рассмотрим в дальнейшем.Иногда говорят, что в статистике целью обычно является вывод (inference) о том, верна ли гипотеза или как связаны между собой переменные, а в машинном обучении целью обычно является предсказание (prediction) или генерация чего-либо (Bzdok et al., 2018), хотя эти две цели часто близки. В машинном обучении, как правило, используются более сложные модели, тогда как в традиционной статистике модели обычно проще, но за счет этого они более интерпретируемы и  больше внимания уделяется оценке уверенности в сделанных выводах (Breiman, 2001).2. Вероятностная модель регрессии2.1. Регрессия, классификация и промежуточные вариантыЗадачи классификации и регрессии отличаются типом целевого признака: в регрессии целевой признак количественный (иногда его называют \"числовой\"), в классификации - категориальный. Отличие категориального от количественного признака заключается не в его типе (int или float), а скорее в предполагаемой метрике сходства на множестве его значений:В количественном признаке чем больше модуль разности между двумя значениями, тем сильнее они непохожи друг на друга. Например, предсказать значение 2 вместо 1 будет в меньшей степени ошибкой, чем предсказать 10 вместо 1.В категориальном признаке все значения, называемые классами, одинаково непохожи друг на друга. Например, предсказать 2-й класс вместо 1-го будет в той же степени ошибкой, что и предсказать 10-й класс вместо 1-го. При этом множество значений дискретно и, как правило, конечно.Все остальные отличия в алгоритмах (в формате выходных данных, функции потерь и метрике качества) обусловлены описанной выше разницей между этими типами. Вообще, при желании задачу классификации технически можно решать как регрессию, то есть напрямую предсказывать номер класса и округлять его до целого числа. Регрессию, наоборот, можно свести к классификации, выполнив дискретизацию множества значений целевого признака. Такие модели кое-как обучатся, но чаще всего их качество на валидации будет существенно ниже, впрочем бывают и исключения (Müller et al., 2021).В случае иерархической классификации (Silla and Freitas, 2011) целевой признак на первый взгляд является категориальным, но на деле некоторые классы могут быть ближе по смыслу и положению в иерархии друг к другу, чем другие. Например, в классификации животных предсказать овчарку вместо лабрадора является в меньшей степени ошибкой, чем предсказать бегемота вместо лабрадора. Это означает, что на множестве значений целевого признака есть какая-то нетривиальная метрика сходства, и ее желательно учесть в алгоритме обучения.Будет ли это по прежнему задачей классификации? Сложно сказать. Базовые понятия \"классификации\" и \"регрессии\" не покрывают все разнообразие промежуточных вариантов (так же как есть промежуточные варианты между молотком, топором и другими инструментами) (Bernholdt et al., 2019, Twomey et al., 2019). Важно лишь то, что при выборе формата выходных данных и функции потерь нужно учитывать метрику сходства на множестве значений целевого признака. Это один из способов внесения в модель априорной (известной или предполагаемой заранее) информации, к чему мы будем еще не раз возвращаться.2.2. Вероятностное моделированиеВ задачах классификации и регрессии (и многих других задачах) требуется найти зависимость между исходными данными  и целевыми данными  в виде функции . Обычно модель имеет параметры, которые подбираются в ходе обучения, поэтому модель можно записывать как функцию от входных данных  и параметров . Поскольку параметров обычно много, то  - это некий массив чисел. Предсказанное моделью значение  (в отличие от истинного значения) обычно обозначается крышечкой (циркумфлексом): .Общая идея вероятностного моделирования заключается в том, что вместо одного числа модель должна предсказывать распределение вероятностей на множестве  при заданном значении . В теории вероятностей это называется условным распределением и записывается как  или просто . Поскольку модель имеет параметры, то вероятностную модель записывают как  - эта запись читается как \"вероятность  при  и \" (позже рассмотрим конкретные примеры).Таким образом мы позволим модели \"сомневаться\" в предсказании. Выполнив такой переход, мы ничего не теряем: распределение вероятностей  несет больше информации, чем точечная оценка . Зато мы получаем важное преимущество. Величина  определена для всех , поэтому мы можем количественно оценить ошибку модели: чем меньшую вероятность модель назначает верному , тем сильнее ошибка. Так мы естественным образом можем задать функцию потерь.2.3. Модель регрессииПусть мы имеем нейронную сеть с одним скрытым слоем шириной в  нейронов и хотим применить ее для решения задачи регрессии с = 10 входными признаками. Такая сеть имеет 2 матрицы весов и 2 вектора bias'ов: . Нетрудно посчитать, что, например, матрица  содержит = 10000 весов, а всего количество весов равно 12001. Пусть в качестве функции активации используется . Преобразование входных данных в выходные осуществляется по следующей формуле (всю сеть обозначим как функцию ):Примечание. Умножение вектора на матрицу может выполняться слева () или справа (), в зависимости от того, работаем мы с векторами-строками или векторами-столбцами. Например, в TensorFlow используется умножение справа, и матрица весов имеет размер (in, out), в PyTorch - умножение слева, и матрица весов имеет размер (out, in).Меняя параметры , можно получить совершенно разные функции . Обучение сети заключается в том, что мы подбираем такие , чтобы минимизировать ошибку предсказания на обучающей выборке. Но способ расчета ошибки предсказания можно выбрать по-разному:Алгоритмический подход. Мы выбираем способ расчета ошибки произвольным образом на основании здравого смысла, руководствуясь любыми соображениями. Например можем взять в качестве ошибки среднеквадратичное отклонение  или среднее абсолютное отклонение . Нетрудно видеть, что . Поэтому MSE, в сравнении с MAE, не так сильно штрафует несущественные ошибки, но сильнее штрафует большие ошибки. Таким образом, MAE более устойчив к выбросам (подробнее см., например, здесь).Определившись с тем, как сильно нам нужно штрафовать большие ошибки в сравнении с маленькими, мы можем выбрать функцию потерь. Далее, поскольку в обучающей выборке много примеров, сложим величину ошибки на всех примерах и будем минимизировать полученную сумму.Вероятностный подход. Будем рассматривать значение  как мат. ожидание нормального распределения с некой фиксированной дисперсией . Так мы получим распределение вероятностей для  при данном :Иногда то же самое записывают другим способом:Формула плотности вероятности нормального распределения:Эта формула выглядит несколько громоздкой, но на самом деле она несложная. Если мы обозначим , то основу формулы (4) составляет выражение . График этой функции выглядит как характерный симметричный \"колокол\" с центром в . Коэффициент  определяет \"сжатие\" колокола по горизонтальной оси, коэффициент  определяет \"сжатие\" по вертикальной оси. Эти коэффициенты выбраны так, чтобы интеграл функции от  до  был равен единице (что требуется для всех распределений), и чтобы распределение имело дисперсию .Итак, мы считаем, что для каждого  величина  распределена нормально, и мат. ожидание распределения является функцией от , которую требуется найти. В отличие от предыдущего подхода, теперь мы можем рассчитать вероятность для любого значения  при  и .Для каждого обучающего примера: чем меньше вероятность истинного значения  при заданных  и , тем сильнее ошибка предсказания на данном примере. Отсюда естественным образом вытекает функция потерь: нам нужно максимизировать . Формула (2) задает вид этого распределения, а формула (4) помогает подсчитать конкретное численное значение  для каждого обучающего примера.Поиск значений параметров , при которых вероятность (правдоподобие) наблюдаемых данных  максимальна, называется методом максимизации правдоподобия (maximum likelihood estimation, MLE). Параметры, максимизирующие правдоподобие, часто обозначают как , мы будем обозначать их как .Пока что мы рассмотрели только один пример, но в обучающей выборке много примеров. Будем искать такие параметры , чтобы максимизировать произведение вероятностей для всех примеров (позже рассмотрим, почему именно произведение, а не сумму). Максимизация некой величины эквивалентна максимизации ее логарифма, а логарифм произведения равен сумме логарифмов множителей:Формула (5) говорит, что нам нужно минимизировать функцию потерь, равную сумме  по всем обучающим примерам , где  мы моделируем нормальным распределением (2). Подставив выражение для нормального распределения (4) получим:Первые два слагаемых в (6) являются константами и поэтому не влияют на положение максимума по , значит их можно удалить. В третьем слагаемом знаменатель является константой, поэтому он тоже не влияет на положение максимума по , его можно заменить на единицу. После этих действий подставим (6) в (5) и получим:Ранее мы вводили обозначение . Согласно формуле (7) нам надо минимизировать сумму среднеквадратичных отклонений  по всем обучающим примерам. Вспомним, что изначально в модели регрессии мы считали  константой, для которой выбрано произвольное значение. Теперь выясняется, что константа  не влияет на оптимальные параметры , поэтому в задаче поиска оптимальных параметров ее значение не играет роли.Примечание. Значение влияет на результат в байесовском выводе, который мы будем рассматривать в следующих частях.Резюме. В алгоритмическом подходе модель машинного обучения - это некая параметризованная функция . Например, это может быть линейная регрессия, нейронная сеть, ансамбль решающих деревьев или машина опорных векторов (хотя две последние модели имеют переменное число параметров, но это не принципиально). Такая модель напрямую предсказывает значение . В вероятностном подходе модель по-прежнему предсказывает число , но теперь это число считается не окончательным предсказанием , а мат. ожиданием нормального распределения с некой фиксированной дисперсией . Таким образом мы моделируем условное распределение .Важно понять, что между этими двумя подходами нет принципиальной разницы. Если вероятностная модель предсказывает, что \" находится где-то вокруг точки  со среднеквадратичным отклонением \", то при точечной оценке модель предсказывает, что \" равно \", но при этом мы понимаем, что модель обычно не выдает идеально точных предсказаний, и интерпретируем ее предсказание как \" находится где-то вокруг точки \".Таким образом, вероятностная модель просто формализует то, что при точечной оценке мы предполагали неформально. Вспомним цитату Лапласа о том, что теория вероятностей - лишь здравый смысл, выраженный в вычислениях.2.4. ОбсуждениеНа примере регрессии мы увидели, что вероятностный подход позволяет вывести выражение для функции потерь, которое в алгоритмическом подходе мы выбирали произвольно. Однако нормальное распределение в (2) мы выбрали произвольно. Вместо него мы могли бы взять, например, распределение Пуассона  или Лапласа. В случае распределения Лапласа мы пришли бы к тому, что надо минимизировать среднее абсолютное отклонение . Отсюда получается, что выбор распределения  в вероятностном подходе эквивалентен выбору функции ошибки  в алгоритмическом подходе.Какой же подход лучше - вероятностный или алгоритмический? Вероятностный подход удобно применять в тех случаях, когда есть объективные причины предполагать, что  имеет тот или иной вид:Пример 1. Иногда мы знаем, что  - это количество неких событий, которые произошли в условиях  - например, количество посетителей тренажерного зала в зависимости от погоды. Тогда распределение  скорее всего похоже на распределение Пуассона, параметр которого является функцией от . Мы можем расписать аналог формулы (2) для распределения Пуассона и из него вывести функцию потерь, которую следует применять. В результате мы получаем регрессию Пуассона. Кроме того, преимущество в том, что мы получаем не точечную оценку, а распределение вероятностей на .Пример 2. Иногда мы знаем, что распределение  гетероскедастично, то есть дисперсия  непостоянна и зависит от . Например, пусть  - доходы, а  - расходы человека на питание. Понятно что в среднем  растет с ростом , но дисперсия  также растет: более богатый человек может тратить на еду много, а может тратить мало, в зависимости от предпочтений, тогда как более бедный человек скорее всего тратит мало. Поэтому в формуле (2) мы можем сделать дисперсию тоже функцией от . Например, если мы используем нейронную сеть, то пусть она имеет 2 выходных нейрона: один нейрон предсказывает мат. ожидание нормального распределения для , а другой нейрон предсказывает дисперсию. Такой способ называется регрессией с оценкой (не)уверенности, в последнем разделе мы рассмотрим его подробнее. Пока что вы можете попробовать сами вывести требуемые формулы.2.5. Функция потерь Хьюбера*Часто мы используем нормальное распределение для . Обоснован ли этот выбор? Иногда да. По центральной предельной теореме, если некая случайная величина  является суммой множества независимых случайных факторов, и каждый фактор вносит исчезающе малый вклад в сумму, то величина  распределена приблизительно нормально, что говорит о комбинаторной природе нормального распределения. Например, сумма большого числа независимых величин, каждая из которых с вероятностью 0.5 принимает значение +1 или -1, распределена приблизительно нормально, что замечательно демонстрирует игрушка Galton Board:В реальности многие величины распределены приблизительно нормально. Однако распределения, которые встречаются в реальном мире, часто имеют более тяжелые хвосты, то есть большую вероятность встретить крайние значения. Скажем, рост или вес человека в популяции распределен приблизительно нормально, но рекордсмены по росту или весу (в ту или иную сторону) встречаются намного чаще, чем если бы распределение было строго нормальным.Поэтому разумно было бы моделировать  распределением, похожим на нормальное, но с более тяжелыми хвостами. Это снизило бы влияние выбросов. Например, распределение Лапласа имеет более тяжелые хвосты, чем нормальное распределение. Мы можем \"склеить\" эти два распределения, взяв центральную часть от нормального распределения и хвосты от распределения Лапласа. Если далее вывести функцию потерь по формуле (6) как минус логарифм плотности вероятности, то получим часто используемую функцию потерь Хьюбера. Эта функция потерь имеет гиперпараметр . Она не так сильно \"штрафует\" большие выбросы, как среднеквадратичная ошибка.В целом, из формулы (5) видно, что функция потерь равна минус логарифму плотности вероятности для выбранного распределения. На самом деле графики плотности для многих распределений удобнее смотреть в логарифмическом масштабе по вертикальной оси. Например, нормальное распределение в логарифмическом масштабе выглядит как парабола с направленными вниз ветвями, а распределение Лапласа в логарифмическом масштабе выглядит как функция . Зеркально отразив эти графики по оси , мы увидим график функции потерь. Скомбинировав нормальное распределение с хвостами от распределения Лапласа, получаем функцию потерь Хьюбера.Интересно, что функцию потерь Хьюбера можно рассматривать как среднеквадратичную ошибку + gradient clipping, применяемый к градиенту функции потерь по . Gradient clipping означает, что если при обратном проходе  по модулю больше некоего порога , то он обрезается до этого порога:Gradient clipping также применяется при обучении нейронных сетей, но в этом случае он действует не на градиент по , а на градиент по весам.3. Вероятностное распределение данных3.1. Понятие распределения данныхКак правило считается, что обучающие и тестовые данные берутся из одного и того же совместного распределения , называемого распределением данных или генеральной совокупностью. Говоря о распределении  или , мы условно предполагаем наличие некоего \"бесконечного генератора пар \", из которого взяты обучающая и тестовая выборка.Конечно, генеральная совокупность данных - это условность, и вопрос ее близости к истине довольно философский. Обычно мы имеем лишь конечную выборку данных, добытую тем или иным способом, но не имеем строгого определения для . Но в целом мы считаем, что  наиболее велико для \"типичных\" пар , и равно нулю для невозможных пар (в которых либо , либо ).Например, пусть мы имеем датасет из объявлений о продаже автомобилей. Для нашего датасета верно, например, следующее:Количество авто \"Lada Granta\" превосходит количество авто \"Москвич-412\"Количество авто \"Победа\" с двигателем мощностью 500 л. с. равно нулюТогда мы можем считать датасет выборкой из распределения, в котором для  верно следующее:Если распределение  невырождено (то есть не назначает всю вероятность одной точке), то это значит, что  не может быть однозначно определен из , но может быть определен приблизительно.3.2. Дискриминативные моделиЗапишем одну из базовых формул теории вероятностей:Модели, которые моделируют , то есть ищут некое приближение для истинного , называют дискриминативными моделями. Иногда используют другую терминологию: Murphy, 2023 называет модели  предиктивными моделями, которые делятся на дискриминативные (классификация) и модели регрессии.Важная особенность всех таких моделей в том, что они не моделируют вероятность входных данных . Это означает, что дискриминативная модель не может оценить то, насколько реалистичны входные данные. Например, мы могли обучать нейронную сеть распознавать животных на фотографиях. Получив на вход изображение с \"белым шумом\", сеть тоже выдаст какой-то ответ (скорее всего, с высокой уверенностью предскажет одно из животных) и никак не сможет сказать вам, что фотография вообще не является животным.3.3. Генеративные модели*Кроме дискриминативных, существуют еще генеративные модели - в статистике так называются модели, которые моделируют распределение для всех используемых переменных, то есть , либо , если  отсутствует. Иногда \"генеративными моделями\" также называют модели, способные генерировать что-то сложное, вроде картинок или текста - это близко к предыдущему определению, но не всегда одно и то же, так что здесь есть неоднозначность в терминологии. В целом, генеративные модели - это тема для отдельной статьи. Подробно о генеративных моделях можно почитать в книге Murphy, 2023, раздел 20 и далее. Дискриминативные модели рассматриваются там же в разделе 14 и далее.С одной стороны кажется, что в задаче предсказания  по  \"информативность\" модели растет в ряду: , поскольку имея  мы можем рассчитать , а имея   мы можем получить точечную оценку , но не наоборот.Но здесь не учитываются некоторые тонкости. Моделировать  можно по-разному, например, одна модель может оценивать для любой пары  плотность вероятности , а другая модель не имеет такой возможности, но позволяет вычислительно эффективно семплировать пары  из совместного распределения. Если бы мы имели бесконечные вычислительные ресурсы, то эти случаи были бы эквивалентны (то есть умея оценивать плотность вероятности мы могли бы семплировать, и наоборот), но на деле вычислительные ресурсы ограничены, и эти случаи различаются. Если наша модель  может только семплировать пары , то задача оценки  может стать невыполнимой.3.4. i.i.d.-гипотеза и качество обобщенияЧасто выборку данных рассматривают как выборку из независимых и одинаково распределенных величин (independent and identically distributed, i.i.d.). Это означает, что каждый пример  из обучающей и тестовой выборки является случайной величиной, взятой из некоего общего распределения . Отсюда автоматически следует, что все примеры независимы друг от друга.По моему мнению, данная гипотеза сочетает в себе \"блеск и нищету\" машинного обучения. С одной стороны, она позволяет формализовать задачу машинного обучения как максимизация ожидаемой метрики качества  на всем распределении данных (подробнее можно почитать, например, здесь):i.i.d.-гипотеза дает очень простой способ оценки качества модели: достаточно разделить данные на train и test, и поскольку примеры в test взяты из того же распределения, что и train, то оценка качества модели на test является оценкой Монте-Карло для ожидаемой метрики на всем распределении данных.При большом размере тестовой выборки  эта оценка будет достаточно точной. Правда, бывают исключения, когда даже при большом размере тестовой выборки оценка качества будет иметь большую погрешность. Это может происходить в том случае, когда влияние отдельных примеров на метрику качества сопоставимо с суммой влияния всех остальных примеров: например в случае несбалансированных классов или сильных выбросов.В случае регрессии формула (12) обычно подразумевает точечную оценку для , а не распределение вероятностей. Если же модель выдает распределение вероятностей , то мы легко можем преобразовать его в точечную оценку. Например, если целевая метрика  - среднеквадратичное отклонение, то нужно взять мат. ожидание , если же целевая метрика - среднее абсолютное отклонение, то нужно взять медиану (подробнее см., например, здесь).3.5. Проблемы i.i.d.-гипотезы*В простоте оценки качества заключается внешний \"блеск\" i.i.d.-гипотезы, за которым часто скрывается \"нищета\". Дело в том, что она часто не выполняется: обучающая выборка и та выборка, на которой модели предстоит работать по назначению, часто оказываются распределены по-разному.Например, это практически неизбежно в кредитном скоринге: нам нужно предсказать вероятность того, что заемщик не вернет кредит, то обучающие данные собираются только по тем заемщикам, которым ранее выдали кредит. Если раньше в банке не выдавали кредиты людям моложе 20 лет, то такие люди не попадут в обучающее распределение данных. Модель может не научиться корректно работать на таких примерах. Если тестовая выборка была случайно отделена от обучающей, то на ней эта проблема никак не будет заметна, но при работе модели по назначению такие люди, конечно, будут часто встречаться. О способах решения данной проблемы в кредитном скоринге можно почитать, например, в Ehrhardt et al., 2021.Часто бывают ситуации, когда обучающее распределение  менее разнообразно (чем то, на котором модели предстоит работать), содержит паразитные корреляции, и избавиться от этого иногда не представляется возможным. Отсюда возникают проблемы сдвига и утечки данных. Подробно и с большим количеством примеров я рассказывал об этом в статье \"Проблемы современного машинного обучения\".В целом понятие \"обучения\" является более широким, чем задача максимизации метрики (11), особенно когда дело касается обучения агентов (роботов, ботов в компьютерных играх и т. д). Во многом обучение связано с умением выявлять причинно-следственные связи (Pearl, 2009), что помогает функционировать в изменяющейся среде и не обращать внимание на нерелевантные детали окружающей среды. Например, робот, который метко бросает мяч в корзину в лаборатории, но впадает в ступор на уличной площадке из-за того, что корзина стала другого цвета, вряд ли будет полезен. Но способность выявлять причинно-следственные связи никак не связана с максимизацией метрики (11), да и само понятие \"распределения данных\" часто неоднозначно, поскольку обычно источник данных не является генератором независимых и одинаково распределенных примеров.Таким образом, i.i.d.-обучение, то есть любые методы, целью которых является максимизация метрики (11), имеет свои границы применимости. Во многих случаях, когда очевидно что i.i.d.-гипотеза неверна, применяют другие подходы (например, марковские цепи, авторегрессионные модели для прогнозирования временных рядов). Но существуют и пограничные случаи, когда непонятно можно ли применять i.i.d.-обучение. На мой взгляд, в табличных задачах слишком часто полагаются на i.i.d.-гипотезу. Возможно, что когда-нибудь в табличном ML в обиход войдут новые (или \"хорошо забытые старые\") подходы, которые могут учесть зависимость примеров друг от друга, например, с помощью латентных переменных или теории информации.4. Статистические моделиВ первой части мы рассмотрели метод максимизации правдоподобия, но за кадром остался вопрос о том, почему мы максимизируем именно произведение вероятностей, а не их сумму. В этой части мы еще вернемся к этому методу, используя материал из второй части, и соединим все элементы пазла в единую картину, введя понятие статистической модели и вероятности обучающей выборки.4.1. Простые статистические моделиСтатистический вывод (statistical inference) занимается оценкой неизвестных параметров распределений. Пусть у нас есть данные, которые являются выборкой из неизвестного распределения. Нам нужно ответить на вопрос - из какого распределения они получены? Мы вводим набор \"распределений-кандидатов\", который называется статистической моделью. Рассмотрим несколько примеров:Пример 1. У нас есть набор чисел . Мы знаем, что эти числа являются i.i.d.-выборкой из нормального распределения, его дисперсия равна 1, а среднее неизвестно (его нужно оценить из данных). Тогда наша статистическая модель  - это множество всех одномерных нормальных распределений с дисперсией 1 и всеми возможными значениями среднего:Возьмем любое распределение из множества . Поскольку нам известно, что все примеры в выборке независимы, то плотность вероятности всей выборки  равна произведению плотностей вероятностей для всех примеров:Отсюда, зная , мы можем рассчитать . Это значит, что для любого распределения из множества  мы можем рассчитать . Теперь просто выберем из множества  то распределение, для которого  максимально. Для этого нужно просто подставить формулу нормального распределения в (15) и отыскать максимум полученного выражения по  (формулы подробно расписывать здесь не будем). Это и есть метод максимизации правдоподобия.Пример 2. Аналогично предыдущему примеру, но мы не знаем ни среднее, ни дисперсию. Тогда наша статистическая модель - это множество всех одномерных нормальных распределений:Здесь метод максимизации правдоподобия можно выполнить полностью аналогично, только формулы получатся сложнее.Таким образом, статистическая модель ничего не говорит об оптимальных значениях параметров, а лишь описывает среди какого множества распределений  мы выполняем поиск. Сам поиск может выполняться методом максимизации правдоподобия, как в предыдущих примерах. Однако такой подход не всегда хорошо работает, есть и другие методы (байесовский вывод и его аппроксимации), их мы рассмотрим в следующих частях.На какие \"подводные камни\" мы можем натолкнуться, используя метод максимизации правдоподобия? Их несколько.Проблема 1. На самом деле распределение, из которого пришла выборка  может не быть нормальным. Вообще, практически ни одно из распределений реального мира не является строго нормальным (кроме, наверное, квантовой механики). Значит ли это что наша модель неверна? Строго говоря да. Но если искомое распределение хорошо аппроксимируется нормальным, то выполнив аппроксимацию мы практически ничего не потеряем. То есть, наша модель  может не содержать искомого распределения, но если элементами из  можно его хорошо аппроксимировать - этого уже достаточно (если же нельзя - говорят о \"model misspecification\"). Вообще, модель - это некое приближение реальности. Она может быть не идеально точна, но все равно полезна: вся наука и инженерия основаны на моделях как упрощениях наблюдаемых явлений. Как говорил один из статистиков, \"all models are wrong, but some are useful\".Проблема 2. Выборка может не быть независимой и одинаково распределенной (i.i.d.). Например, она может быть результатом двухступенчатного семплирования или даже временным рядом, то есть выборкой из , где  - время, разное для разных семплов. Здесь опять получается, что наша модель  неверна, но она все равно может быть полезна, если зависимостью от  можно пренебречь. Если же ей принебречь нельзя, то нужно использовать другие методы, такие как прогнозирование временных рядов. Они тоже могут быть основаны на теории вероятностей, но i.i.d.-гипотеза в них не используется.Проблема 3. Мы можем заранее иметь предположения о том, какие значения параметров более вероятны, а какие менее вероятны. В этом случае метод максимизации правдоподобия применять не стоит, поскольку он не учитывает эту информацию. В следующей части мы рассмотрим байесовский вывод, который позволяет ее учесть.Проблема 4. Если мы будем искать не 1-2 параметра (, ), а, скажем, 100 параметров, а в нашей выборке всего 10 семплов, то наверное этих семплов недостаточно. Мы легко сможем подогнать параметры под данные, и мы даже сможем найти много разных значений параметров, одинаково хорошо подогнанных под данные. Но некоторые из таких значений параметров будут очень плохо соответствовать всему распределению данных, при хорошем соответствии выборке. Таким образом, в методе максимизации правдоподобия возникает проблема переобучения (1, 2, 3).В третьей части мы рассмотрим байесовский подход, с точки зрения которого метод максимума правдоподобия является лишь аппроксимацией полного байесовского вывода. При этом чем сложнее модель и меньше данных, тем менее точной получается аппроксимация, что и является причиной переобучения. Полный байесовский вывод часто невозможно выполнить ввиду огромной вычислительной сложности, но существуют его более точные аппроксимации: ансамблирование, вариационный вывод и другие методы.4.2. Статистические модели в машинном обученииВ предыдущем разделе мы рассмотрели статистическую модель как параметризованное семейство распределений. Тот же подход можно применить в машинном обучении.Модель машинного обучения (такую как линейная регрессия, случайный лес, нейронная сеть) можно рассмотреть как параметризованную функцию. С помощью этой функции мы оцениваем параметры распределения . Например, пусть мы предполагаем, что условное распределение  является нормальным распределением с   и мат. ожиданием, зависящим от . За  обозначим множество всех возможных значений параметров. Наша статистическая модель будет иметь следующий вид:У нас есть набор пар , и мы предполагаем, что они являются i.i.d.-выборкой из некоего распределения . Это предположение мы подробно рассматривали во второй части. Все, то нам осталось сделать - найти такие параметры , чтобы вероятность обучающей выборки была максимальна: Поскольку мы предполагаем, что все примеры независимы друг от друга (i.i.d.), то по базовым законам теории вероятностей вероятность выборки равна произведению вероятностей всех примеров, так же как в формуле (5). Именно поэтому в разделе 2 мы максимизировали произведение вероятностей (а не, например, сумму или минимум).Все остальное ними уже было рассмотрено ранее: осталось сформулировать задачу оптимизации (5-7) и итеративно решить каким-либо способом (для нейронных сетей - градиентным спуском, для случайного леса - построением новых деревьев и т. д.), найдя оптимальные значения параметров , максимизирующие вероятность выборки.Как видим, здесь есть полная аналогия с простыми статистическими моделями, которые мы рассмотрели в предыдущем разделе. Вместе с этим мы получаем те же самые три подводных камня, основным из которых часто является переобучение: существует множество значений параметров, дающих очень малую ошибку на train и высокую ошибку на test, и процесс оптимизации часто сходится именно к таким решениям.Зачем нам нужны были сложности с логарифмом, и почему нельзя напрямую максимизировать вероятность ? Пожалуй, мы могли бы так сделать. Но во-первых, в случае нормального распределения взятие логарифма упрощает формулы (5-7). Во-вторых, в этом случае мы должны максимизировать уже не сумму, а произведение величин  по всем примерам (5). Оптимизация произведения сотен и тысяч множителей может быть численно нестабильной и вызывать переполнение float. В-третьих, для оптимизации суммы большого количества слагаемых существуют методы стохастической оптимизации с доказанной эффективностью, тогда как для оптимизации произведения большого количества множителей аналогичных методов может не быть.Казалось бы, мы уже рассмотрели много формул и терминов, но от проблемы переобучения никуда не делись, и  вообще переход к вероятностной постановке задачи не принес осоой пользы (кроме регрессии Пуассона). Пока что это действительно так. Фундамент дома сам по себе не приносит пользы: жить в нем ничуть не более удобно, чем в вагончике на стройке. В следующих разделах мы рассмотрим как вероятностная постановка задач классификации и регрессии помогает задавать функцию потерь в сложных случаях, когда данные размечены несколькими аннотаторами по-разному, либо когда нам нужно оценивать уверенность в задаче регрессии. В следующей части рассмотрим байесовский вывод, который основывается на материале данной части.5. Регрессия с оценкой уверенности*5.1. Моделирование дисперсии в модели регрессии*В разделе части 1 мы рассмотрели вариант модели регрессии, в котором  моделируется нормальным распределением, для которого мат. ожидание предсказывается с помощью , а дисперсия является константой. Но нам ничего не мешает предсказывать и дисперсию. Для этого нужно, чтобы модель регрессии выдавала не одно, а два числа:  и . В случае нейронной сети мы можем взять 2 выходных нейрона. В случае градиентного бустинга над решающими деревьями каждый лист каждого дерева может выдавать 2 числа, которые суммируются по всем деревьям.Но дисперсия не может быть меньше нуля, а модель может выдавать любое число. Поэтому удобнее, чтобы модель выдавала логарифм дисперсии. Ее дополнительно можно умножить на константу , которая будет гиперпараметром функции потерь. Тогда мат. ожидание целевой переменной  и ее дисперсия  выразим через  и  следующим образом:Так же как и в первой части, мы минимизируем минус логарифм вероятности для каждого примера. Подставив в (6) выражения для  и , получим выражение для функции потерь:Если функцию потерь умножить на константу или сложить с константой, то положение ее локальных и глобальных минимумов не изменится. Поэтому в формуле (19) удалим второе и четвертое слагаемое, которые являются константами. Затем первое и третье слагаемое умножим на 2. Получим упрощенное выражение для функции потерь:Мы снова видим среднеквадратичное отклонение, но теперь к нему добавляется дополнительный множитель и дополнительное слагаемое. Высокое значение предсказанной неуверенности  повышает значение функции потерь за счет второго слагаемого, но при этом уменьшает вклад первого слагаемого, которое штрафует ошибку предсказания . Это дает модели некий компромисс: модель может предсказывать на каких-то примерах высокую неуверенность, уменьшая штраф от неточного предсказания самого значения  на них.Проверим этот метод на практике, обучив модель на табличном датасете California Housing, в котором нужно предсказывать цену недвижимости в разных районах Калифорнии, имея 8 исходных признаков. В данном случае хорошо работает значение .Кодfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import pearsonr\nimport tensorflow as tf, numpy as np, matplotlib.pyplot as plt\nfrom tensorflow.keras import Sequential, layers, optimizers, callbacks\nfrom tensorflow.keras.optimizers.schedules import InverseTimeDecay\nfrom tqdm.notebook import tqdm\ndef custom_loss(y_true, y_pred, C=0.2):\n  y_true = tf.squeeze(y_true)\n  mu, log_sigma = y_pred[:, 0], y_pred[:, 1]\n  loss = (y_true - mu)**2 / tf.math.exp(log_sigma) / C + log_sigma\n  return tf.math.reduce_mean(loss)\ndef custom_mse(y_true, y_pred):\n  y_true = tf.squeeze(y_true)\n  mu, log_sigma = y_pred[:, 0], y_pred[:, 1]\n  metrics = (y_true - mu)**2\n  return tf.math.reduce_mean(metrics)\n# downloading and preprocessing dataset\nX, y = fetch_california_housing(return_X_y=True)\ny = np.log(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\ny_train = scaler.fit_transform(y_train[:, None])[:, 0]\ny_test = scaler.transform(y_test[:, None])[:, 0]\n#training models\ndef train_simple_model(epochs=50, dropout=0.5):\n  model = Sequential([\n      layers.Dense(1000, 'relu'),\n      layers.Dropout(dropout),\n      layers.Dense(1)\n  ])\n  model.compile(loss='mse', optimizer=\n                optimizers.Adam(InverseTimeDecay(1e-3, 1000, 2)))\n  model.fit(X_train, y_train, validation_data=(X_test, y_test),\n            batch_size=128, epochs=epochs, verbose=0)\n  return model, model.history.history['val_loss']\nn_tests = 10\nsimple = [train_simple_model() for i in tqdm(range(n_tests))]\nuncertainty = [train_model_with_uncertainty() for i in tqdm(range(n_tests))]\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 3))\nfrom matplotlib.lines import Line2D\nfor model, history in simple:\n  ax1.plot(history, color='C0', label='simple models with MSE loss')\nfor model, history in uncertainty:\n  ax1.plot(history, color='C1', label='models with uncertainty')\nax1.set_ylim(0.2, 0.4)\nax1.legend([Line2D([0], [0], color='C0', lw=4),\n                Line2D([0], [0], color='C1', lw=4)],\n           ['simple models with MSE loss', 'models with uncertainty'])\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Validation MSE')\nmodel, _ = uncertainty[0]\npreds_test = model.predict(X_test)\nax2.hexbin(np.abs(preds_test[:, 0] - y_test),\n           preds_test[:, 1], bins='log', extent=(0, 3, -3, 3), gridsize=50)\nax2.set_xlabel('Absolute error')\nax2.set_ylabel('Predicted log sigma')\ncorr = pearsonr(np.abs(preds_test[:, 0] - y_test), preds_test[:, 1])[0]\nax2.set_title(f'Correlation: {corr:g}')\nplt.show()Результаты эксперимента. Мы обучили 10 моделей с простой функцией потерь MSE и еще 10 моделей с оценкой неуверенности. На рисунке слева - динамика изменения MSE на валидации для всех моделей, в зависимости от эпохи обучения. Видно, что наши модели превысили по качеству обычные модели, обучаемые с функцией потерь MSE. Возможно это связано с тем, что снижается переобучение модели на выбросах, так как в областях с частыми выбросами предсказывается высокое значение неуверенности, что смягчает ошибку предсказания мат. ожидания в этих областях, уменьшая множитель при первом слагаемом.На втором графике взяли одну из моделей с оценкой неуверенности и рассмотрели корреляцию фактической ошибки предсказания с предсказанной неуверенностью (на валидационной выборке). Положительная корреляция говорит о том, что модель в какой-то степени справляется с задачей оценки собственной уверенности в предсказании.Описанный в этом разделе подход давно применяется в разных задачах (см. например Nix and Weigend, 1994). Недавно аналогичная функция потерь была реализована в библиотеке градиентного бустинга CatBoost под названием RMSEWithUncertainty. Посмотрим как она работает на практике, используя CatBoost версии 1.1.1:Кодfrom catboost import CatBoostRegressor\nmodel = CatBoostRegressor(depth=3, iterations=1000,\n                          loss_function='RMSEWithUncertainty')\nmodel.fit(X_train, y_train, verbose=0)\nstaged_preds = list(model.staged_predict(X_test, eval_period=1))\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 3))\nfrom sklearn.metrics import mean_squared_error\nmetrics = [mean_squared_error(y_test, p[:, 0]) for p in staged_preds]\nax1.plot(metrics, label='catboost')\nax1.set_xlabel('N trees')\nax1.set_ylabel('validation MSE')\nax1.legend()\nlast_preds = staged_preds[-1]\nax2.hexbin(np.abs(last_preds[:, 0] - y_test),\n           last_preds[:, 1], bins='log', extent=(0, 3, -3, 3), gridsize=50)\nax2.set_xlabel('Absolute error')\nax2.set_ylabel('Predicted log sigma')\ncorr = pearsonr(np.abs(last_preds[:, 0] - y_test), last_preds[:, 1])[0]\nax2.set_title(f'Correlation: {corr:g}')\nplt.show()В целом мы видим похожую картину, хотя точность предсказаний CatBoost получилась выше, чем нейронной сети. В CatBoost есть функция для отрисовки деревьев, воспользуемся ей, чтобы нарисовать первое дерево:model.plot_tree(0)Каждое звено дерева содержит разделяющее правило, в котором проверяется, что значение указанного признака больше указанного порога. Каждый лист дерева выдает 2 числа: само значение  и уверенность. Оба эти числа суммируются по всем деревьям.5.2. Регрессия с константной оценкой дисперсии*Вспомним вероятностную модель регрессии, которую мы рассматривали во второй части (2). В ней   является константой. Для любого значения  локальные и глобальные минимумы функции потерь одни и те же, и изменение  эквивалентно масштабированию функции потерь, что равносильно изменению learning rate. Отсюда получается, что какое бы  мы не брали - ничего не изменится (поскольку learning rate выбирается совсем по другим соображениям). Но что если мы хотим знать реальное ?Для этого мы могли бы сделать  обучаемым параметром, не зависящим от . Заменим в формуле (28) выражение  на обучаемый параметр . Видно, что при стремлении  к  или  все выражение стремится к   , поэтому функция потерь никогда не стремится к  и для параметров () существует глобальный оптимум. Например, в нейронной сети мы можем сделать веса, идущие к нейрону, который выдает уверенность, нулевыми и необучаемыми. Тогда уверенность будет равна bias'у этого нейрона, который является обучаемым параметром.Имеет ли это смысл? Возможно в каких-то случаях да. Но если дать модели выучивать параметр , то он будет рассчитываться на обучающей выборке и поэтому может быть переобученным (если модель хорошо запомнила все обучающие примеры, то и  будет низким). Кроме того, при обучении нейронных сетей часто применяют dropout, и значение  будет рассчитано в условиях наличия dropout (хотя на инференсе он обычно не используется). Поэтому обычно нам гораздо удобнее оценивать дисперсию предсказаний на валидационной или тестовой выборке.Существует еще один похожий подход, называемый inductive conformal prediction (Vovk et al., 2005, Sousa, 2022). В нем на отдельной выборке рассчитывается вероятность модели допустить ошибку выше заданного порога, что близко по смыслу к оценке дисперсии.Конец части 1. Ссылка на вторую часть. Спасибо@ivankomarovи @yorkoза ценные комментарии, которые были учтены при подготовке статьи.      Tags: статистическая модельметод максимального правдоподобиярегрессиянормальное распределениестатистикатеория вероятностейсреднеквадратичное отклонение  Hubs: Open Data Science corporate blogMathematicsMachine learningStatistics in ITArtificial Intelligence          \n\n\n"
}