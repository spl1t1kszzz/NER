{
  "articles": [
    {
      "text_804515": [
        {
          "text": "Хабр, привет! Меня зовут Антон Разжигаев, я аспирант Сколтеха и участник научной группы Fusion Brain в институте AIRI.",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "Хабр",
              "start_pos": 0,
              "normal_form": "хабр"
            },
            {
              "index": 0,
              "class": "Person",
              "value": "Антон Разжигаев",
              "start_pos": 25,
              "normal_form": "антон разжигаев"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "Сколтеха",
              "start_pos": 53,
              "normal_form": "сколтеха"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "AIRI",
              "start_pos": 113,
              "normal_form": "airi"
            },
            {
              "index": 0,
              "class": "Activity",
              "value": "Fusion Brain",
              "start_pos": 88,
              "normal_form": "fusion brain"
            }
          ],
          "relations": [],
          "id": 804515
        },
        {
          "text": "С момента выхода первой статьи «Attention is All You Need» я с жадностью и любопытством, присущими любому исследователю, пытаюсь углубиться во все особенности и свойства моделей на базе архитектуры трансформер. Но, если честно, я до сих пор не понимаю, как они работают и почему так хорошо обучаются. Очень хочу разобраться, в чём же причина такой эффективности этих моделей, и есть ли предел их возможностей?",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "Attention is All You Need",
              "start_pos": 32,
              "normal_form": "attention is all you need"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей на базе архитектуры трансформер",
              "start_pos": 170,
              "normal_form": "модель на база архитектуры трансформер"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей",
              "start_pos": 367,
              "normal_form": "модель"
            }
          ],
          "relations": [],
          "id": 804516
        },
        {
          "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "трансформеров",
              "start_pos": 16,
              "normal_form": "трансформеров"
            },
            {
              "index": 0,
              "class": "Activity",
              "value": "конференции EACL 2024",
              "start_pos": 110,
              "normal_form": "конференция eacl 2024"
            },
            {
              "index": 0,
              "class": "InfoResource",
              "value": "The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models",
              "start_pos": 164,
              "normal_form": "The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддингов",
              "start_pos": 317,
              "normal_form": "эмбеддинг"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "активаций",
              "start_pos": 330,
              "normal_form": "активация"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "языковых моделей",
              "start_pos": 401,
              "normal_form": "языковая модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LM",
              "start_pos": 419,
              "normal_form": "lm"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "языковых моделей",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Model",
                "value": "LM",
                "index": 0
              }
            }
          ],
          "id": 804517
        },
        {
          "text": "Итак, приступим!",
          "terms": [],
          "relations": [],
          "id": 804518
        },
        {
          "text": "Начнём с рассказа о данных — это нужно для того, чтобы было проще понять, что мы сделали и что обнаружили. Т.к. нас интересовало пространство контекстуализированных эмбеддингов (в т.ч. промежуточных), надо было их где-то добыть.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "контекстуализированных эмбеддингов",
              "start_pos": 142,
              "normal_form": "контекстуализированный эмбеддинг"
            }
          ],
          "relations": [],
          "id": 804519
        },
        {
          "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
          "terms": [
            {
              "index": 0,
              "class": "Dataset",
              "value": "enwik8",
              "start_pos": 9,
              "normal_form": "enwik8"
            },
            {
              "index": 0,
              "class": "InfoResource",
              "value": "Википедии",
              "start_pos": 45,
              "normal_form": "википедия"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английском языке",
              "start_pos": 58,
              "normal_form": "английский язык"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 115,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "токена",
              "start_pos": 173,
              "normal_form": "токен"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "«пространство эмбеддингов»",
              "start_pos": 215,
              "normal_form": "пространство эмбеддингов"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "многомерное облако точек",
              "start_pos": 264,
              "normal_form": "многомерное облако точек"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Object",
                "value": "«пространство эмбеддингов»",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Object",
                "value": "многомерное облако точек",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "InfoResource",
                "value": "Википедии",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "английском языке",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Dataset",
                "value": "enwik8",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "английском языке",
                "index": 0
              }
            }
          ],
          "id": 804520
        },
        {
          "text": "Чтобы исключить зависимость наблюдений от выбранного датасета, мы повторили эксперименты на случайных последовательностях токенов, и все выводы повторились. Поэтому в дальнейшем не буду акцентировать на этом внимание, а лучше сразу перейду к результатам.",
          "terms": [
            {
              "index": 0,
              "class": "Dataset",
              "value": "датасета",
              "start_pos": 53,
              "normal_form": "датасет"
            },
            {
              "index": 0,
              "class": "Activity",
              "value": "эксперименты",
              "start_pos": 76,
              "normal_form": "эксперимент"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "токенов",
              "start_pos": 122,
              "normal_form": "токен"
            }
          ],
          "relations": [],
          "id": 804521
        },
        {
          "text": "Один из самых главных вопросов, которые мы себе задали в процессе исследования — а какая вообще форма у этих облаков точек? Визуализировать их сложно — пространство эмбеддингов очень многомерно, — а методы снижения размерности тут не сильно помогают. Поэтому мы решили использовать анизотропию в качестве нашего «микроскопа». Анизотропия — это мера, показывающая насколько облако точек вытянуто, насколько оно неоднородно. Чем выше это значение, тем сильнее вытягивается пространство.",
          "terms": [
            {
              "index": 0,
              "class": "Activity",
              "value": "исследования",
              "start_pos": 66,
              "normal_form": "исследование"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "Визуализировать",
              "start_pos": 124,
              "normal_form": "визуализировать"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддингов",
              "start_pos": 165,
              "normal_form": "эмбеддинг"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "методы снижения размерности",
              "start_pos": 199,
              "normal_form": "метод снижения размерности"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "анизотропию",
              "start_pos": 282,
              "normal_form": "анизотропия"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "Анизотропия",
              "start_pos": 326,
              "normal_form": "анизотропия"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "методы снижения размерности",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "Визуализировать",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "анизотропию",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "Визуализировать",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "методы снижения размерности",
                "index": 0
              },
              "predicate": "isAppliedTo",
              "term2": {
                "class": "Object",
                "value": "эмбеддингов",
                "index": 0
              }
            }
          ],
          "id": 804522
        },
        {
          "text": "К примеру, уже давно известно (см. статью Representation Degeneration Problem in Training Natural Language Generation Models), что эмбеддинги трансформеров-энкодеров лежат в «узком конусе» — из-за этого косинусы между текстовыми репрезентациями всегда очень высокие. Но тем не менее, если вычесть среднее значение и отцентрировать это облако точек, оно становится вполне изотропным, то есть похожим на многомерный шарик. Поэтому говорят, что эмбеддинги трансформеров-энкодеров (Bert, RoBERTa,  Albert, …) локально изотропны.А в случае с декодерами (GPT, Llama, Mistral, …) мы обнаружили, что это совершенно не так! Даже после центрирования и использования более устойчивых к смещению методов на базе сингулярных чисел мы видим, что на средних слоях языковых моделей анизотропия практически равна 1. Это означает, что облако точек там вытянуто вдоль прямой линии. Но почему? Это же так сильно снижает ёмкость модели, она из-за этого практически не использует ТЫСЯЧИ других размерностей.",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "Representation Degeneration Problem in Training Natural Language Generation Models",
              "start_pos": 42,
              "normal_form": "representation degeneration problem in training natural language generation models"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддинги",
              "start_pos": 131,
              "normal_form": "эмбеддинг"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "трансформеров-энкодеров",
              "start_pos": 142,
              "normal_form": "трансформер-энкодер"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "текстовыми репрезентациями",
              "start_pos": 218,
              "normal_form": "текстовая репрезентация"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддинги",
              "start_pos": 442,
              "normal_form": "эмбеддинг"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "трансформеров-энкодеров",
              "start_pos": 453,
              "normal_form": "трансформер-энкодер"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Bert",
              "start_pos": 478,
              "normal_form": "bert"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "RoBERTa",
              "start_pos": 484,
              "normal_form": "roberta"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Albert",
              "start_pos": 494,
              "normal_form": "albert"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "декодерами",
              "start_pos": 537,
              "normal_form": "декодер"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "GPT",
              "start_pos": 549,
              "normal_form": "gpt"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Llama",
              "start_pos": 554,
              "normal_form": "llama"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Mistral",
              "start_pos": 561,
              "normal_form": "mistral"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "центрирования",
              "start_pos": 626,
              "normal_form": "центрирование"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "методов на базе сингулярных",
              "start_pos": 684,
              "normal_form": "метод на базе сингулярных"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "языковых моделей",
              "start_pos": 749,
              "normal_form": "языковая модель"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "анизотропия",
              "start_pos": 766,
              "normal_form": "анизотропия"
            },
            {
              "index": 0,
              "class": "Value",
              "value": "практически равна 1",
              "start_pos": 778,
              "normal_form": "практически равный 1"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 908,
              "normal_form": "модель"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "анизотропия",
                "index": 0
              },
              "predicate": "hasValue",
              "term2": {
                "class": "Value",
                "value": "практически равна 1",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "анизотропия",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "декодерами",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "центрирования",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "декодерами",
                "index": 0
              }
            }
          ],
          "id": 804523
        },
        {
          "text": "Откуда берётся эта неоднородность пространства репрезентаций в декодерах, мы пока не знаем, но предполагаем, что это связано с процессом их обучения, задачей предсказания следующего токена и треугольной маской внимания. Это одна из исследовательских задач, которая сейчас стоит перед нами.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "декодерах",
              "start_pos": 63,
              "normal_form": "декодер"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучения",
              "start_pos": 140,
              "normal_form": "обучение"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "предсказания следующего токена",
              "start_pos": 158,
              "normal_form": "предсказание следующего токена"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "треугольной маской внимания",
              "start_pos": 191,
              "normal_form": "треугольная маска внимания"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "декодерах",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "предсказания следующего токена",
                "index": 0
              }
            }
          ],
          "id": 804524
        },
        {
          "text": "Если посмотреть на профиль анизотропии по слоям, то становится видно, что в начале и в конце декодеров эмбеддинги гораздо более изотропны, а экстремально высокая анизотропия наблюдается только в середине, где и должен происходить весь мыслительный процесс.",
          "terms": [
            {
              "index": 0,
              "class": "Metric",
              "value": "анизотропии",
              "start_pos": 27,
              "normal_form": "анизотропия"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "декодеров",
              "start_pos": 93,
              "normal_form": "декодер"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддинги",
              "start_pos": 103,
              "normal_form": "эмбеддинг"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "анизотропия",
              "start_pos": 162,
              "normal_form": "анизотропия"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "анизотропии",
                "index": 0
              },
              "predicate": "isUsedIn",
              "term2": {
                "class": "Model",
                "value": "декодеров",
                "index": 0
              }
            }
          ],
          "id": 804525
        },
        {
          "text": "Мы проследили за тем, как меняется анизотропия от чекпоинта к чекпоинту по мере обучения моделей (мы взяли все модели с промежуточными весами из того, что было опубликовано на тот момент). Оказалось, что все модели класса трансформер-декодер постепенно сходятся к одной и той же форме пространства и одному и тому же куполообразному профилю анизотропии.",
          "terms": [
            {
              "index": 0,
              "class": "Metric",
              "value": "анизотропия",
              "start_pos": 35,
              "normal_form": "анизотропия"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей",
              "start_pos": 89,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 111,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 208,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "трансформер-декодер",
              "start_pos": 222,
              "normal_form": "трансформер-декодер"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "анизотропии",
              "start_pos": 341,
              "normal_form": "анизотропия"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "анизотропия",
                "index": 0
              },
              "predicate": "isUsedIn",
              "term2": {
                "class": "Model",
                "value": "моделей",
                "index": 0
              }
            }
          ],
          "id": 804526
        },
        {
          "text": "Следующий наш «микроскоп» для наблюдения за активациями — внутренняя размерность. Это довольно красивое математическое понятие, описывающее «сложность» фигуры (многообразия или манифолда), на котором располагаются точки в многомерном пространстве.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "активациями",
              "start_pos": 44,
              "normal_form": "активация"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "внутренняя размерность",
              "start_pos": 58,
              "normal_form": "внутренняя размерность"
            }
          ],
          "relations": [],
          "id": 804527
        },
        {
          "text": "Чтобы было понятнее, рассмотрим трёхмерную фигуру в виде ленты, свёрнутой в спираль (см. картинку ниже). Если мы приблизимся к какому-либо её участку, то обнаружим, что в малой окрестности точки будто бы лежат на плоскости. Следовательно, локальная внутренняя размерность тут равна двум.",
          "terms": [
            {
              "index": 0,
              "class": "Metric",
              "value": "внутренняя размерность",
              "start_pos": 249,
              "normal_form": "внутренняя размерность"
            }
          ],
          "relations": [],
          "id": 804528
        },
        {
          "text": "Самое главное, что внутреннюю размерность довольно легко оценить, так как она сильно связана со скоростью роста «объёма» многомерного шара (количества точек данных, попадающих внутрь шара) по мере увеличения радиуса. Измерение зависимости количества точек от радиуса позволяет определить внутреннюю размерность в локальной области облака данных.",
          "terms": [
            {
              "index": 0,
              "class": "Metric",
              "value": "внутреннюю размерность",
              "start_pos": 19,
              "normal_form": "внутренняя размерность"
            }
          ],
          "relations": [],
          "id": 804529
        },
        {
          "text": "Итак, что же мы обнаружили? Во-первых, размерность довольно низкая, но это не новость, т.к. это было обнаружено и до нас. Во-вторых, — и это гораздо интереснее — эта размерность изменяется одинаково для всех моделей по мере обучения! Этот процесс состоит из двух фаз — сначала рост, а затем падение (см. график).",
          "terms": [
            {
              "index": 0,
              "class": "Metric",
              "value": "размерность",
              "start_pos": 39,
              "normal_form": "размерность"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "размерность",
              "start_pos": 166,
              "normal_form": "размерность"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей",
              "start_pos": 208,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучения",
              "start_pos": 224,
              "normal_form": "обучение"
            }
          ],
          "relations": [],
          "id": 804530
        },
        {
          "text": "Похоже, что первая часть обучения переводит фичи в более высокие измерения, чтобы «запомнить» как можно больше информации, а во второй фазе — фичи начинают сжиматься, позволяя выявлять больше закономерностей, усиливая обобщающие способности модели.",
          "terms": [
            {
              "index": 0,
              "class": "Task",
              "value": "обучения",
              "start_pos": 25,
              "normal_form": "обучение"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 241,
              "normal_form": "модель"
            }
          ],
          "relations": [],
          "id": 804531
        },
        {
          "text": "Ещё раз — у всех LLM во время обучения присутствуют две фазы: инфляция эмбеддингов и их последующая компрессия.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 17,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучения",
              "start_pos": 30,
              "normal_form": "обучение"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддингов",
              "start_pos": 71,
              "normal_form": "эмбеддинг"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "инфляция",
              "start_pos": 62,
              "normal_form": "инфляция"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "компрессия",
              "start_pos": 100,
              "normal_form": "компрессия"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "инфляция",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "обучения",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "компрессия",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "обучения",
                "index": 0
              }
            }
          ],
          "id": 804532
        },
        {
          "text": "Мы верим, что, вооружившись новым знанием, мы сможем улучшить процесс обучения языковых моделей (и не только), сделать его эффективнее, а сами трансформеры — быстрее и компактнее. Ведь если эмбеддинги проходят стадию компрессии и вообще стремятся расположиться вдоль одной линии, то почему бы просто не повыкидывать неиспользуемые измерения? Или помочь модели с более быстрым преодолением первой фазы.",
          "terms": [
            {
              "index": 0,
              "class": "Activity",
              "value": "обучения",
              "start_pos": 70,
              "normal_form": "обучение"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "языковых моделей",
              "start_pos": 79,
              "normal_form": "языковая модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "трансформеры",
              "start_pos": 143,
              "normal_form": "трансформер"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "эмбеддинги",
              "start_pos": 190,
              "normal_form": "эмбеддинг"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "компрессии",
              "start_pos": 217,
              "normal_form": "компрессия"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 353,
              "normal_form": "модель"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "компрессии",
                "index": 0
              },
              "predicate": "isAppliedTo",
              "term2": {
                "class": "Object",
                "value": "эмбеддинги",
                "index": 0
              }
            }
          ],
          "id": 804533
        },
        {
          "text": "Также мы обнаружили, что незадолго до взрывов лосса во время обучения (больная тема всех, кто учит LLM) внутренняя размерность сильно подрастает. Возможно, у нас получится предсказывать взрывы лосса и не тратить вычислительные ресурсы впустую, или вообще победить эти нестабильности, поняв их природу.",
          "terms": [
            {
              "index": 0,
              "class": "Activity",
              "value": "учит",
              "start_pos": 94,
              "normal_form": "учить"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 99,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "внутренняя размерность",
              "start_pos": 104,
              "normal_form": "внутренняя размерность"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "внутренняя размерность",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            }
          ],
          "id": 804534
        },
        {
          "text": "Хотя кого я обманываю, всё это нужно только ради удовлетворения своего любопытства! ",
          "terms": [],
          "relations": [],
          "id": 804535
        },
        {
          "text": "Подписывайтесь на каналы авторов в телеграме  AbstractDL, CompleteAI, Dendi Math&AI, Ivan Oseledets. В работе также принимали участие коллеги из AIRI, Сбера, Сколтеха, МГУ, ВШЭ и Самарского университета.",
          "terms": [
            {
              "index": 0,
              "class": "Organization",
              "value": "AIRI",
              "start_pos": 145,
              "normal_form": "airi"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "Сбера",
              "start_pos": 151,
              "normal_form": "сбер"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "Сколтеха",
              "start_pos": 158,
              "normal_form": "сколтех"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "МГУ",
              "start_pos": 168,
              "normal_form": "мгу"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "ВШЭ",
              "start_pos": 173,
              "normal_form": "вшэ"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "Самарского университета",
              "start_pos": 179,
              "normal_form": "самарский университет"
            }
          ],
          "relations": [],
          "id": 804536
        }
      ],
      "text_803945": [
        {
          "text": "Сейчас, когда каждый чих в интернете может привести к новому стартапу или технологическому прорыву, большие языковые модели (LLM) занимают своё законное место на передовой научно-технического прогресса. Они умнее, быстрее и эффективнее человека в ряде задач: написание кода, создание контента, перевод текстов и многое другое. Однако, такая высокая степень умения ставит нас перед новым набором проблем – их безопасностью и устойчивостью.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "большие языковые модели",
              "start_pos": 100,
              "normal_form": "большая языковая модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 125,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "написание кода",
              "start_pos": 259,
              "normal_form": "написание кода"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "создание контента",
              "start_pos": 275,
              "normal_form": "создание контента"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "перевод текстов",
              "start_pos": 294,
              "normal_form": "перевод текста"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "большие языковые модели",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "написание кода",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "большие языковые модели",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "создание контента",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "большие языковые модели",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "перевод текстов",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "большие языковые модели",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            }
          ],
          "id": 803945
        },
        {
          "text": "Кто бы подумал, что искусственный интеллект кусается? На деле, конечно, дело не в физическом нападении, а в уязвимостях, которые могут быть использованы злоумышленниками. Большие языковые модели действительно могут попасть под угрозу, и влияние таких событий может оказаться далеко не виртуальным.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "Большие языковые модели",
              "start_pos": 171,
              "normal_form": "большая языковая модель"
            }
          ],
          "relations": [],
          "id": 803946
        },
        {
          "text": "Меня зовут Дарья Лютова, я data scientist в ЦАД ВАВТ, также я учусь в магистратуре AI Talent Hub ИТМО и интересуюсь вопросами обучения и безопасности языковых моделей. В этом посте, вместе с вами, хочу пойти дальше простого обсуждения существования уязвимостей в LLM и предлагаю вникнуть в тему проблем безопасности, касающуюся больших языковых моделей, выявить слабые места и прийти к пониманию методов их укрепления. Очень надеюсь, что эта информация поможет тем, кто преследует цель не только достичь новых высот в области AI, но и удостовериться, что их достижения надежны и устойчивы к киберугрозам.",
          "terms": [
            {
              "index": 0,
              "class": "Person",
              "value": "Дарья Лютова",
              "start_pos": 11,
              "normal_form": "дарья лютова"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "ЦАД ВАВТ",
              "start_pos": 44,
              "normal_form": "цад вавт"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "AI Talent Hub ИТМО",
              "start_pos": 83,
              "normal_form": "ai talent hub ИТМО"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "языковых моделей",
              "start_pos": 150,
              "normal_form": "языковая модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 263,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "больших языковых моделей",
              "start_pos": 328,
              "normal_form": "большая языковая модель"
            }
          ],
          "relations": [],
          "id": 803947
        },
        {
          "text": "Большие языковые модели (Large Language Models – LLM) – это сегодняшняя реальность, и наше будущее, без которого уже сложно представить свою жизнь. Кажется, что уже не только каждый разработчик слышал о LLM, но и многие школьники даже младших классов уже сталкиваются с этим явлением. Всем известно, что это крутые нейронные сети, которые учатся на огромном количестве текстов, чтобы понимать, как люди говорят и пишут, они умеют понимать и говорить на разных языках. Самые сейчас известные LLM: GPT4, Mistral 7B OpenChat, Claude 3 Opus и другие, с одним из вариантов лидерборда моделей можно ознакомиться по ссылке (https://chat.lmsys.org/?leaderboard).",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "Большие языковые модели",
              "start_pos": 0,
              "normal_form": "большая языковая модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Large Language Models",
              "start_pos": 25,
              "normal_form": "large language model"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 49,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 203,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "нейронные сети",
              "start_pos": 315,
              "normal_form": "нейронная сеть"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 491,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "GPT4",
              "start_pos": 496,
              "normal_form": "gpt4"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Mistral 7B OpenChat",
              "start_pos": 502,
              "normal_form": "mistral 7b openchat"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Claude 3 Opus",
              "start_pos": 523,
              "normal_form": "claude 3 opus"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "Large Language Models",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Model",
                "value": "Большие языковые модели",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Model",
                "value": "Large Language Models",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Model",
                "value": "Большие языковые модели",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT4",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "Mistral 7B OpenChat",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "Claude 3 Opus",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            }
          ],
          "id": 803948
        },
        {
          "text": "Про LLM знают почти все, но не очень многим известно, что эти модели могут быть уязвимы, причем данном случае речь идет не о сбоях в работе модели или выдаче неверного ответа, речь о целенаправленных атаках злоумышленников с целью получения конфиденциальной информации, утечкам данных (много и интересно про уязвимости в статье), выдачи вредоносного контента, например, рекомендации посетить нежелательный сайт, а также манипуляции и дезинформации. В LLM могут быть заложены предвзятости, обусловленные предвзятостью в наборах данных для обучения, а это может привести к дискриминационной, ошибочной или предвзятой генерации контента. Большие языковые модели могут стать мишенью для специфических программных атак, нацеленных на эксплуатацию уязвимостей в алгоритмах или инфраструктуре, что может нарушить их функционирование или повлиять на качество генерируемого контента.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 4,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 62,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 140,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 451,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Большие языковые модели",
              "start_pos": 635,
              "normal_form": "большая языковая модель"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "генерации контента",
                "index": 0
              }
            }
          ],
          "id": 803949
        },
        {
          "text": "Т.к. сейчас LLM используют очень многие, важно понимать эти уязвимости и принимать меры для обеспечения безопасности моделей и данных, с которыми они работают.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 12,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обеспечения безопасности",
              "start_pos": 92,
              "normal_form": "обеспечение безопасность"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей",
              "start_pos": 117,
              "normal_form": "модель"
            }
          ],
          "relations": [],
          "id": 803950
        },
        {
          "text": "По какой причине в таком мощном инструменте могут возникать уязвимости, некоторые из которых могут быть использованы злоумышленниками? Это происходит из-за сложности и размера самих моделей, которые могут стать мишенью для специально разработанных атак. Кроме того, часто данные, на которых обучаются эти модели, могут быть неочищенными и содержать предвзятость, которая затем отражается в работе LLM. Недостаточное тестирование моделей на безопасность также может стать причиной возникновения уязвимостей.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 397,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 305,
              "normal_form": "модель"
            }
          ],
          "relations": [],
          "id": 803951
        },
        {
          "text": "Авторы статьи Jailbroken: How does llm safety training fail? говорят о двух, на их взгляд, основных причинах того, что взлом моделей возможен.",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "Jailbroken: How does llm safety training fail",
              "start_pos": 14,
              "normal_form": "jailbroken: how does llm safety training fail"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей",
              "start_pos": 125,
              "normal_form": "модель"
            }
          ],
          "relations": [],
          "id": 803952
        },
        {
          "text": "Конкурирующие цели (competing objectives) возникают в ситуациях, когда задачи предварительного обучения и следования инструкциям модели противоречат задаче безопасности.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 129,
              "normal_form": "модель"
            }
          ],
          "relations": [],
          "id": 803953
        },
        {
          "text": "Например, мы конструируем промпт (запрос к модели) так, чтобы модель сначала ответила на какой-то простой и безопасный вопрос или выполнила какую-то простую инструкцию. Это базируется на том, что модели наказывают за отказ от безвредных инструкций. Если следующей частью промпта идет какой-то небезопасный вопрос, то модель скорее всего ответит и на него, поскольку отказ после старта генерации в предварительном обучении маловероятен и основная задача предварительного обучения заключается в наказании за отказ. В результате модель продолжает отвечать уже на небезопасный запрос.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "промпт",
              "start_pos": 26,
              "normal_form": "промпт"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "запрос к модели",
              "start_pos": 34,
              "normal_form": "запрос к модели"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 43,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 62,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "промпта",
              "start_pos": 271,
              "normal_form": "промпт"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 317,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "генерации",
              "start_pos": 385,
              "normal_form": "генерация"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "наказании за отказ",
              "start_pos": 493,
              "normal_form": "наказание за отказ"
            }
          ],
          "relations": [],
          "id": 803954
        },
        {
          "text": "Несоответствующее обобщение (mismatched generalization) возникает, когда входные данные не попали в корпус для обучения модели на безопасность, но находятся в рамках более широкого и разнообразного набора данных предварительного обучения модели на разные задачи. Такое несоответствие может быть использовано для взлома путем конструирования запросов, на которых предварительное обучение и следование инструкциям обобщаются, но обучение на безопасность не работает. В таких случаях модель отвечает, но без учёта безопасности.",
          "terms": [
            {
              "index": 0,
              "class": "Dataset",
              "value": "корпус",
              "start_pos": 100,
              "normal_form": "корпус"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 120,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучения модели на безопасность",
              "start_pos": 111,
              "normal_form": "обучение модели на безопасность"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 238,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 481,
              "normal_form": "модель"
            }
          ],
          "relations": [],
          "id": 803955
        },
        {
          "text": "В данном случае нам достаточно перевести небезопасный запрос на один из малоресурсных языков, и, с большой вероятностью успеха, модель выдаст нам вредоносный ответ.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "небезопасный запрос",
              "start_pos": 41,
              "normal_form": "небезопасный запрос"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 128,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "малоресурсных языков",
              "start_pos": 72,
              "normal_form": "малоресурсный язык"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "модель",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Lang",
                "value": "малоресурсных языков",
                "index": 0
              }
            }
          ],
          "id": 803956
        },
        {
          "text": "Комбинация этих двух атак также может быть успешной. Но если для первой угрозы порой надо поломать голову и придумать что-то, что заставить модель отвечать нам положительно, то вторая атака порой требует только качественного переводчика на один из малоресурсных языков.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "атак",
              "start_pos": 21,
              "normal_form": "атака"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "первой угрозы",
              "start_pos": 65,
              "normal_form": "первая угроза"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "вторая атака",
              "start_pos": 177,
              "normal_form": "вторая атака"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 140,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "малоресурсных языков",
              "start_pos": 248,
              "normal_form": "малоресурсный язык"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "первой угрозы",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Task",
                "value": "атак",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "вторая атака",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Task",
                "value": "атак",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "модель",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Lang",
                "value": "малоресурсных языков",
                "index": 0
              }
            }
          ],
          "id": 803957
        },
        {
          "text": "Второй тип атаки хочется рассмотреть подробнее.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "Второй тип атаки",
              "start_pos": 0,
              "normal_form": "второй тип атаки"
            }
          ],
          "relations": [],
          "id": 803958
        },
        {
          "text": "Для обеспечения безопасности и защиты от злоупотреблений, компании разработчики языковых моделей, такие как OpenAI и Anthropic, применяют подходы, основанные на обучении с подкреплением от человека (RLHF), и стратегии \"red team\". В рамках RLHF модели обучаются на данных, связанных с безопасностью, стремясь максимизировать вознаграждения, аналогичные человеческому суждению о безопасном контенте. Задача \"red team\" включает в себя идентификацию и устранение уязвимостей защиты через ряд методов, включая переобучение модели и фильтрацию данных, для предотвращения генерации вредоносного контента перед выпуском моделей. \"Red team\" играют важную роль в безопасности больших языковых моделей, проводя симулированные атаки и тестирование на проникновение для выявления уязвимостей и разработки мер по устранению рисков безопасности. Их участие помогает повысить уровень защиты и обеспечить безопасное использование LLM.",
          "terms": [
            {
              "index": 0,
              "class": "Task",
              "value": "обеспечения безопасности",
              "start_pos": 4,
              "normal_form": "обеспечение безопасности"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "защиты от злоупотреблений",
              "start_pos": 31,
              "normal_form": "защита от злоупотреблений"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "OpenAI",
              "start_pos": 108,
              "normal_form": "openai"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "Anthropic",
              "start_pos": 117,
              "normal_form": "anthropic"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "обучении с подкреплением от человека",
              "start_pos": 161,
              "normal_form": "обучение с подкреплением от человека"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "RLHF",
              "start_pos": 199,
              "normal_form": "rlhf"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "\"red team\"",
              "start_pos": 218,
              "normal_form": "red team"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "RLHF",
              "start_pos": 241,
              "normal_form": "rlhf"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "\"red team\"",
              "start_pos": 407,
              "normal_form": "red team"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "идентификацию",
              "start_pos": 432,
              "normal_form": "идентификация"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "устранение уязвимостей защиты",
              "start_pos": 448,
              "normal_form": "устранение уязвимостей защиты"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "переобучение модели",
              "start_pos": 505,
              "normal_form": "переобучение модели"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "фильтрацию данных",
              "start_pos": 527,
              "normal_form": "фильтрация данных"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "предотвращения генерации вредоносного контента",
              "start_pos": 550,
              "normal_form": "предотвращение генерации вредоносного контента"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "\"Red team\"",
              "start_pos": 621,
              "normal_form": "red team"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "симулированные атаки",
              "start_pos": 700,
              "normal_form": "симулированная атака"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "тестирование на проникновение",
              "start_pos": 723,
              "normal_form": "тестирование на проникновение"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "выявления уязвимостей",
              "start_pos": 757,
              "normal_form": "выявления уязвимостей"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "устранению рисков безопасности",
              "start_pos": 799,
              "normal_form": "устранение рисков безопасности"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "повысить уровень защиты",
              "start_pos": 851,
              "normal_form": "повысить уровень защиты"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обеспечить безопасное использование",
              "start_pos": 877,
              "normal_form": "обеспечить безопасное использование"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "обучении с подкреплением от человека",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "обеспечения безопасности",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "переобучение модели",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "идентификацию",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "переобучение модели",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "устранение уязвимостей защиты",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "переобучение модели",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "предотвращения генерации вредоносного контента",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "фильтрацию данных",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "идентификацию",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "фильтрацию данных",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "устранение уязвимостей защиты",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "фильтрацию данных",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "предотвращения генерации вредоносного контента",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "симулированные атаки",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "выявления уязвимостей",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "симулированные атаки",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "устранению рисков безопасности",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "симулированные атаки",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "повысить уровень защиты",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "тестирование на проникновение",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "выявления уязвимостей",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "тестирование на проникновение",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "устранению рисков безопасности",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "тестирование на проникновение",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "повысить уровень защиты",
                "index": 0
              }
            }
          ],
          "id": 803959
        },
        {
          "text": "Исследования показали, что атаки, основанные на использовании нетрадиционных или неанглийских языков, включая обфускацию с помощью кодировки base64, азбуки Морзе и специальных шифров, могут успешно обходить защитные механизмы LLM. Такие методы позволяют скрыто ввести вредоносные подсказки или запросы, не будучи распознанными системами безопасности.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "атаки",
              "start_pos": 27,
              "normal_form": "атака"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "нетрадиционных или неанглийских языков",
              "start_pos": 62,
              "normal_form": "нетрадиционный или неанглийский язык"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 226,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "вредоносные подсказки или запросы",
              "start_pos": 268,
              "normal_form": "вредоносная подсказка или запрос"
            }
          ],
          "relations": [],
          "id": 803960
        },
        {
          "text": "Но уже есть готовые инструменты проверки или защиты LLM от таких перекодированных взломов. Например, в сканере уязвимостей для больших языковых моделей Garak (https://github.com/leondz/garak) существует большое количество проб на перекодированные запросы, включая запросы на юникоде, азбуке Морзе, Вase(16, 32, 64, 2048), ROT13, NATO phonetic alphabet и многих других. Такие проверки не требуют носителя языка и достаточно просто проверяются кодом.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 52,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "больших языковых моделей Garak",
              "start_pos": 127,
              "normal_form": "большая языковая модель garak"
            }
          ],
          "relations": [],
          "id": 803961
        },
        {
          "text": "Это подчеркивает сложности в обеспечении безопасности AI, особенно против замаскированных или кодированных атак и демонстрирует, что естественный язык может представлять более сложную задачу для безопасности по сравнению с более простыми или строго формализованными кодами вроде азбуки Морзе или base64. Важность этих находок заключается в необходимости разработки методов защиты, способных эффективно распознавать и противостоять разнообразным формам ввода, включая маскировку под безобидный или нетипичный контент.",
          "terms": [
            {
              "index": 0,
              "class": "Task",
              "value": "обеспечении безопасности AI",
              "start_pos": 29,
              "normal_form": "обеспечение безопасности ai"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "разработки методов защиты",
              "start_pos": 354,
              "normal_form": "разработка методов защиты"
            }
          ],
          "relations": [],
          "id": 803962
        },
        {
          "text": "При разработке решений искусственного интеллекта на разных языках критически важно иметь доступ к данным на всех языках. Согласно \"Этнологу\", в мире используется 7164 языка, при этом китайский, испанский и английский являются языками с наибольшим числом носителей. Тем не менее, только около 20 языков обладают обширными текстовыми корпусами, с английским на первом месте. Большинство азиатских и африканских языков страдают от нехватки данных, что делает их малоресурсными и затрудняет разработку ИИ-решений. Языки, включая суахили и хинди, меньше представлены в интернете, что усложняет создание решений в области обработки естественного языка для них. Наличие большого объема текстовых данных и специализированных ресурсов, таких как семантические базы данных, является ключевым для разработки эффективных языковых решений. Подходы к преодолению проблем малоресурсных языков включают увеличение данных, мета-трансферное обучение и межъязыковые аннотации, что позволяет улучшить работу ИИ на разных языках и расширить его возможности для малоресурсных языков.",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "\"Этнологу\"",
              "start_pos": 130,
              "normal_form": "этнолог"
            },
            {
              "index": 0,
              "class": "Dataset",
              "value": "корпусами",
              "start_pos": 332,
              "normal_form": "корпус"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "китайский",
              "start_pos": 183,
              "normal_form": "китайский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "испанский",
              "start_pos": 194,
              "normal_form": "испанский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английский",
              "start_pos": 206,
              "normal_form": "английский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английским",
              "start_pos": 345,
              "normal_form": "английский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "азиатских и африканских языков",
              "start_pos": 385,
              "normal_form": "азиатский и африканский язык"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "разработку ИИ-решений",
              "start_pos": 487,
              "normal_form": "разработка ИИ-решений"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "суахили",
              "start_pos": 525,
              "normal_form": "суахили"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "хинди",
              "start_pos": 535,
              "normal_form": "хинди"
            },
            {
              "index": 0,
              "class": "Science",
              "value": "обработки естественного языка",
              "start_pos": 616,
              "normal_form": "обработка естественного языка"
            },
            {
              "index": 0,
              "class": "InfoResource",
              "value": "семантические базы данных",
              "start_pos": 737,
              "normal_form": "семантическая база данных"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "малоресурсных языков",
              "start_pos": 859,
              "normal_form": "малоресурсный язык"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "увеличение данных",
              "start_pos": 887,
              "normal_form": "увеличение данных"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "мета-трансферное обучение",
              "start_pos": 906,
              "normal_form": "мета-трансферное обучение"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "межъязыковые аннотации",
              "start_pos": 934,
              "normal_form": "межъязыковая аннотация"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "малоресурсных языков",
              "start_pos": 1042,
              "normal_form": "малоресурсный язык"
            }
          ],
          "relations": [],
          "id": 803963
        },
        {
          "text": "Таким образом, существует большое количество методов, чтобы обеспечить носителям малоресурсных языков работу с большими языковыми моделями. При этом в области безопасности ИИ существует лингвистическим неравенство, т.к. в «red team» не включены носители малоресурсных языков.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "большими языковыми моделями",
              "start_pos": 111,
              "normal_form": "большая языковая модель"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "«red team»",
              "start_pos": 222,
              "normal_form": "red team"
            }
          ],
          "relations": [],
          "id": 803964
        },
        {
          "text": "Получается, что модель на малоресурсных языках обучена, но почти не защищена от вредоносных запросов на этих языках!",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 16,
              "normal_form": "модель"
            }
          ],
          "relations": [],
          "id": 803965
        },
        {
          "text": "В этой области довольно иллюстративным примером является эксперимент объединенной команды авторов из Брауновского университета (Zheng-Xin Yong, CristinaMenghini, and Stephen H. Bach.), которые разделили языки на 3 категории:",
          "terms": [
            {
              "index": 0,
              "class": "Activity",
              "value": "эксперимент",
              "start_pos": 57,
              "normal_form": "эксперимент"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "Брауновского университета",
              "start_pos": 101,
              "normal_form": "брауновский университет"
            },
            {
              "index": 0,
              "class": "Person",
              "value": "Zheng-Xin Yong",
              "start_pos": 128,
              "normal_form": "Zheng-Xin Yong"
            },
            {
              "index": 0,
              "class": "Person",
              "value": "CristinaMenghini",
              "start_pos": 144,
              "normal_form": "cristinamenghini"
            },
            {
              "index": 0,
              "class": "Person",
              "value": "Stephen H. Bach.",
              "start_pos": 166,
              "normal_form": "Stephen H. Bach."
            },
            {
              "index": 0,
              "class": "Object",
              "value": "языки",
              "start_pos": 203,
              "normal_form": "язык"
            }
          ],
          "relations": [],
          "id": 803966
        },
        {
          "text": "Низкоресурсные языки - это такие языки, которые почти не имеют доступа к данным для обучения ИИ. Сюда входят, например, зулу, шотландский гэльский, хмонг, и гуарани. Они представляют большую часть мировых языков (94%) и имеют около 1,2 миллиарда пользователей.",
          "terms": [
            {
              "index": 0,
              "class": "Lang",
              "value": "Низкоресурсные языки",
              "start_pos": 0,
              "normal_form": "низкоресурсный язык"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "зулу",
              "start_pos": 120,
              "normal_form": "зулу"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "шотландский гэльский",
              "start_pos": 126,
              "normal_form": "шотландский гэльский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "хмонг",
              "start_pos": 148,
              "normal_form": "хмонг"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "гуарани",
              "start_pos": 157,
              "normal_form": "гуарани"
            }
          ],
          "relations": [],
          "id": 803967
        },
        {
          "text": "Среднересурсные языки - языки с некоторым количеством доступных данных: украинский, бенгальский, тайский и иврит. Они занимают 4,5% от языков мира, насчитывая 1,8 миллиарда пользователей.",
          "terms": [
            {
              "index": 0,
              "class": "Lang",
              "value": "Среднересурсные языки",
              "start_pos": 0,
              "normal_form": "среднересурсный язык"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "украинский",
              "start_pos": 72,
              "normal_form": "украинский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "бенгальский",
              "start_pos": 84,
              "normal_form": "бенгальский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "тайский",
              "start_pos": 97,
              "normal_form": "тайский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "иврит",
              "start_pos": 107,
              "normal_form": "иврит"
            }
          ],
          "relations": [],
          "id": 803968
        },
        {
          "text": "Высокоресурсные языки имеют обширную базу данных, как неразмеченных, так и c метками: упрощенный мандаринский, современный арабский, итальянский, хинди и английский. Эти языки составляют 1,5% от языков мира, но на них приходится 4,7 миллиарда пользователей.",
          "terms": [
            {
              "index": 0,
              "class": "Lang",
              "value": "Высокоресурсные языки",
              "start_pos": 0,
              "normal_form": "высокоресурсный язык"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "упрощенный мандаринский",
              "start_pos": 86,
              "normal_form": "упрощённый мандаринский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "современный арабский",
              "start_pos": 111,
              "normal_form": "современный арабский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "итальянский",
              "start_pos": 133,
              "normal_form": "итальянский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "хинди",
              "start_pos": 146,
              "normal_form": "хинди"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английский",
              "start_pos": 154,
              "normal_form": "английский"
            }
          ],
          "relations": [],
          "id": 803969
        },
        {
          "text": "Авторы перевели каждую опасную инструкцию из набора данных AdvBench Harmful Behaviors на 12 языков трех разных уровней ресурсов, выбрав языки из различных географических мест и языковых групп, чтобы результаты были универсальными. Непереведенные запросы на английском были добавлены как бейзлайн для сравнения.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "опасную инструкцию",
              "start_pos": 23,
              "normal_form": "опасная инструкция"
            },
            {
              "index": 0,
              "class": "Dataset",
              "value": "AdvBench Harmful Behaviors",
              "start_pos": 59,
              "normal_form": "advbench harmful behaviors"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английском",
              "start_pos": 257,
              "normal_form": "английский"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Dataset",
                "value": "AdvBench Harmful Behaviors",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "английском",
                "index": 0
              }
            }
          ],
          "id": 803970
        },
        {
          "text": "Чтобы оценить степень угрозы атак на основе перевода, авторы сравнили их с наиболее успешными методами взлома: AIM, base64, инъекция префикса и подавление отказа.",
          "terms": [
            {
              "index": 0,
              "class": "Task",
              "value": "оценить степень угрозы атак",
              "start_pos": 6,
              "normal_form": "оценить степень угрозы атак"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "AIM",
              "start_pos": 111,
              "normal_form": "aim"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "base64",
              "start_pos": 116,
              "normal_form": "base64"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "инъекция префикса",
              "start_pos": 124,
              "normal_form": "инъекция префикса"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "подавление отказа",
              "start_pos": 144,
              "normal_form": "подавление отказа"
            }
          ],
          "relations": [],
          "id": 803971
        },
        {
          "text": "Таким образом можно сделать вывод о том, что, переводя опасные запросы на малоресурсные языки, такие как зулу или шотландский гэльский, можно обойти меры безопасности GPT-4 и вызвать вредоносные ответы примерно в половине случаев, тогда как для исходных запросов на английском языке успех составляет менее 1%. Другие малоресурсные языки, например, хмонг и гуарани, показывают более низкий процент успеха, так как GPT-4 часто либо не может определить язык, либо переводит запросы на английский. Однако комбинирование разных малоресурсных языков увеличивает шанс обхода до 79%.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "опасные запросы",
              "start_pos": 55,
              "normal_form": "опасный запрос"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "малоресурсные языки",
              "start_pos": 74,
              "normal_form": "малоресурсный язык"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "зулу",
              "start_pos": 105,
              "normal_form": "зулу"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "шотландский гэльский",
              "start_pos": 114,
              "normal_form": "шотландский гэльский"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "GPT-4",
              "start_pos": 167,
              "normal_form": "gpt-4"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английском языке",
              "start_pos": 266,
              "normal_form": "английский язык"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "хмонг",
              "start_pos": 348,
              "normal_form": "хмонг"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "гуарани",
              "start_pos": 356,
              "normal_form": "гуарани"
            },
            {
              "index": 0,
              "class": "Value",
              "value": "менее 1%",
              "start_pos": 300,
              "normal_form": "менее 1%"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "малоресурсные языки",
              "start_pos": 317,
              "normal_form": "малоресурсный язык"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "GPT-4",
              "start_pos": 413,
              "normal_form": "gpt-4"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английский",
              "start_pos": 482,
              "normal_form": "английский"
            },
            {
              "index": 0,
              "class": "Value",
              "value": "79%",
              "start_pos": 571,
              "normal_form": "79%"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "зулу",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "шотландский гэльский",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "хмонг",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "гуарани",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "английском языке",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "английский",
                "index": 0
              }
            }
          ],
          "id": 803972
        },
        {
          "text": "В отличие от этого, языки со средним и высоким уровнем ресурсов оказались надежнее защищенными, с индивидуальным процентом успеха атак менее 15%. Была замечена разница в успехе атак в зависимости от языка, например, хинди, тайский и бенгальский показывают более высокие показатели.",
          "terms": [
            {
              "index": 0,
              "class": "Lang",
              "value": "языки со средним и высоким уровнем ресурсов",
              "start_pos": 20,
              "normal_form": "язык со средним и высоким уровнем ресурсов"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "хинди",
              "start_pos": 216,
              "normal_form": "хинди"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "тайский",
              "start_pos": 223,
              "normal_form": "тайский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "бенгальский",
              "start_pos": 233,
              "normal_form": "бенгальский"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "индивидуальным процентом успеха атак",
              "start_pos": 98,
              "normal_form": "индивидуальный процент успеха атак"
            },
            {
              "index": 0,
              "class": "Value",
              "value": "менее 15%",
              "start_pos": 135,
              "normal_form": "менее 15%"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "индивидуальным процентом успеха атак",
                "index": 0
              },
              "predicate": "hasValue",
              "term2": {
                "class": "Value",
                "value": "менее 15%",
                "index": 0
              }
            }
          ],
          "id": 803973
        },
        {
          "text": "Небезопасные запросы из набора данных AdvBench были классифицированы по 16 темам, и был проанализирован успех атак в зависимости от тематики и уровня ресурсов языков. С переводом на малоресурсные языки, обход средств безопасности оказался более успешным по всем темам, за исключением материалов, связанных с сексуальным насилием над детьми, где атаки на мало- и среднересурсных языках показали одинаковый успех из-за успешного обхода на тайском языке. Три темы с самым высоким процентом успешных атак через перевод на малоресурсные языки были: терроризм, финансовая манипуляция и дезинформация. ",
          "terms": [
            {
              "index": 0,
              "class": "Dataset",
              "value": "AdvBench",
              "start_pos": 38,
              "normal_form": "advbench"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "темам",
              "start_pos": 75,
              "normal_form": "тема"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "атак",
              "start_pos": 110,
              "normal_form": "атака"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "тематики",
              "start_pos": 132,
              "normal_form": "тематика"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "уровня ресурсов языков",
              "start_pos": 143,
              "normal_form": "уровень ресурсов языков"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "темам",
              "start_pos": 262,
              "normal_form": "тема"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "темы",
              "start_pos": 456,
              "normal_form": "тема"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "материалов",
              "start_pos": 284,
              "normal_form": "материал"
            },
            {
              "index": 0,
              "class": "Activity",
              "value": "классифицированы",
              "start_pos": 52,
              "normal_form": "классифицированный"
            },
            {
              "index": 0,
              "class": "Activity",
              "value": "проанализирован",
              "start_pos": 88,
              "normal_form": "проанализированный"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "тайском языке",
              "start_pos": 437,
              "normal_form": "тайский язык"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "атак",
              "start_pos": 496,
              "normal_form": "атака"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Dataset",
                "value": "AdvBench",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "тайском языке",
                "index": 0
              }
            }
          ],
          "id": 803974
        },
        {
          "text": "Недостаточное внимание к малоресурсным языкам может также увеличить риски смешанных атак. Вероятность встретить вредоносный контент на языках с низким уровнем доступности примерно в три раза выше, чем на языках с высоким уровнем доступности, как в случае с ChatGPT, так и с GPT-4. В преднамеренном сценарии многоязычные подсказки могут усугубить негативное влияние вредоносных инструкций, при этом частота появления небезопасных сообщений поразительно высока: 80,92 % для ChatGPT и 40,71 % для GPT-4 (Multilingual jailbreak challenges in large language models).",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "ChatGPT",
              "start_pos": 257,
              "normal_form": "chatgpt"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "GPT-4",
              "start_pos": 274,
              "normal_form": "gpt-4"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "Multilingual jailbreak challenges in large language models",
              "start_pos": 501,
              "normal_form": "multilingual jailbreak challenges in large language models"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "частота появления небезопасных сообщений",
              "start_pos": 398,
              "normal_form": "частота появления небезопасных сообщений"
            },
            {
              "index": 0,
              "class": "Value",
              "value": "80,92 %",
              "start_pos": 460,
              "normal_form": "80,92 %"
            },
            {
              "index": 0,
              "class": "Value",
              "value": "40,71 %",
              "start_pos": 482,
              "normal_form": "40,71 %"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "Multilingual jailbreak challenges in large language models",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "ChatGPT",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "Multilingual jailbreak challenges in large language models",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "GPT-4",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "Multilingual jailbreak challenges in large language models",
                "index": 0
              },
              "predicate": "hasValue",
              "term2": {
                "class": "Value",
                "value": "80,92 %",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "Multilingual jailbreak challenges in large language models",
                "index": 0
              },
              "predicate": "hasValue",
              "term2": {
                "class": "Value",
                "value": "40,71 %",
                "index": 0
              }
            }
          ],
          "id": 803975
        },
        {
          "text": "Малоресурсные языки представляют собой особый вызов для механизмов безопасности GPT-4, позволяя обойти их с высокой долей успеха, что связано со случаями не соответствующего обобщения, когда обучение безопасности не обобщается на языковую область с низкими ресурсами, для которой существуют возможности LLM. Это подчеркивает необходимость улучшения моделей обнаружения и перевода для языков с низким уровнем ресурсов, чтобы повысить безопасность и эффективность защиты от вредоносных действий.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "GPT-4",
              "start_pos": 80,
              "normal_form": "gpt-4"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучение безопасности",
              "start_pos": 191,
              "normal_form": "обучение безопасности"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 303,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "улучшения моделей обнаружения и перевода",
              "start_pos": 339,
              "normal_form": "улучшение моделей обнаружения и перевода"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "повысить безопасность",
              "start_pos": 424,
              "normal_form": "повысить безопасность"
            }
          ],
          "relations": [],
          "id": 803976
        },
        {
          "text": "Разработчики приложений, интегрирующих такие мощные модели, должны быть настороже и принимать активные меры, чтобы устранить возможные уязвимости. Одним из путей может быть использование чат-ботов с функциями предварительной проверки и ограничения запросов, особенно для малоресурсных языков. Важно не просто ограничиваться контролем русско -и англоязычного контента, но и обеспечивать тщательный мониторинг вводимых данных на всех поддерживаемых языках.",
          "terms": [
            {
              "index": 0,
              "class": "Application",
              "value": "чат-ботов",
              "start_pos": 187,
              "normal_form": "чат-бот"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "предварительной проверки и ограничения запросов",
              "start_pos": 209,
              "normal_form": "предварительная проверка и ограничение запросов"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "устранить возможные уязвимости",
              "start_pos": 115,
              "normal_form": "устранить возможные уязвимости"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "мониторинг вводимых данных",
              "start_pos": 397,
              "normal_form": "мониторинг вводимых данных"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "предварительной проверки и ограничения запросов",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "устранить возможные уязвимости",
                "index": 0
              }
            }
          ],
          "id": 803977
        },
        {
          "text": "Включение дополнительных слов проверки запросов на малоресурсных языках может предотвратить обработку вредоносных инструкций. Подходы вроде применения инструментов типа Lakera Guard могут служить эффективным средством предотвращения выполнения вредоносных запросов. Реализуя механизмы перехвата опасных команд еще до активации модели, можно существенно повысить уровень цифровой безопасности.",
          "terms": [
            {
              "index": 0,
              "class": "Application",
              "value": "Lakera Guard",
              "start_pos": 169,
              "normal_form": "lakera guard"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "предотвращения выполнения вредоносных запросов",
              "start_pos": 218,
              "normal_form": "предотвращение выполнения вредоносных запросов"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "перехвата опасных команд",
              "start_pos": 285,
              "normal_form": "перехват опасных команд"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "повысить уровень цифровой безопасности",
              "start_pos": 353,
              "normal_form": "повысить уровень цифровой безопасности"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Application",
                "value": "Lakera Guard",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "предотвращения выполнения вредоносных запросов",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Application",
                "value": "Lakera Guard",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "перехвата опасных команд",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Application",
                "value": "Lakera Guard",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "повысить уровень цифровой безопасности",
                "index": 0
              }
            }
          ],
          "id": 803978
        },
        {
          "text": "Есть еще более простой выход из ситуации: если приложение ориентировано на использование только определенных языков – например, русского и английского – устанавливается ясная и строгая директива модели: обрабатывать запросы исключительно на этих языках. Можно внести четкое указание в системный промпт: “Эта модель предназначена для работы только с русским и английским языками. Ты не можешь принимать запросы ни на каких других языка, кроме русского и английского”. Такой подход использует базовый навык моделей следовать предоставленным инструкциям и создает барьер для запросов, выполненных на любых других языках. Эта мера, простая в реализации, может стать первой линией защиты от несанкционированных манипуляций и укрепит безопасность приложений, в которых используются большие языковые модели.",
          "terms": [
            {
              "index": 0,
              "class": "Lang",
              "value": "русского",
              "start_pos": 128,
              "normal_form": "русский"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английского",
              "start_pos": 139,
              "normal_form": "английский"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "инструкциям",
              "start_pos": 539,
              "normal_form": "инструкция"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "системный промпт",
              "start_pos": 285,
              "normal_form": "системный промпт"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "большие языковые модели",
              "start_pos": 776,
              "normal_form": "большая языковая модель"
            }
          ],
          "relations": [],
          "id": 803979
        },
        {
          "text": "С другой стороны, стоит ожидать и надеяться, что компании-разработчики LLM, такие как OpenAI, будут вкладывать существенные ресурсы в совершенствование тестирования и устранение недочетов, связанных с малоресурсными языками. Уточнение и совершенствование моделей перевода и обнаружения языков с ограниченными данными является ключевой задачей, поскольку это напрямую влияет на безопасность AI-платформ в целом. Для сохранения информативности и полезности таких моделей, данных на малоресурсных языках нельзя исключать из обучения. Вместо этого следует настраивать системы таким образом, чтобы с одной стороны обеспечивалась должная обработка данных на этих языках, а с другой — предотвращались потенциальные угрозы безопасности до того, как механизмы безопасности соответствуют необходимым стандартам. Этот процесс является динамичным и требует постоянного внимания, так как только через неустанное мониторинг и обновление систем мы можем гарантировать, что искусственный интеллект останется надежным и безопасным для всех пользователей, независимо от популярности их языка.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 71,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "OpenAI",
              "start_pos": 86,
              "normal_form": "openai"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "Уточнение и совершенствование",
              "start_pos": 225,
              "normal_form": "уточнение и совершенствование"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей перевода и обнаружения языков",
              "start_pos": 255,
              "normal_form": "модель перевода и обнаружения языков"
            },
            {
              "index": 0,
              "class": "Application",
              "value": "AI-платформ",
              "start_pos": 390,
              "normal_form": "AI-платформа"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 461,
              "normal_form": "модель"
            }
          ],
          "relations": [],
          "id": 803980
        },
        {
          "text": "Безопасность LLM является сложной и многогранной проблемой, требующей комплексного подхода и сотрудничества различных сторон – от разработчиков и исследователей до пользователей и законодателей. Только совместными усилиями можно обеспечить безопасное и эффективное использование больших языковых моделей в различных областях и предотвратить возможные негативные последствия и угрозы для общества.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 13,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "больших языковых моделей",
              "start_pos": 279,
              "normal_form": "большая языковая модель"
            }
          ],
          "relations": [],
          "id": 803981
        },
        {
          "text": "Обфускация - это процесс изменения промпта (входных данных) таким образом, чтобы он выглядел непонятным или запутанным, но при этом сохранял свою семантику (смысл). Обфускация может быть использована для защиты информации или для создания препятствий для анализа и понимания промпта.",
          "terms": [
            {
              "index": 0,
              "class": "Object",
              "value": "промпта",
              "start_pos": 35,
              "normal_form": "промпт"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Обфускация",
              "start_pos": 0,
              "normal_form": "обфускация"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "семантику",
              "start_pos": 146,
              "normal_form": "семантика"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "смысл",
              "start_pos": 157,
              "normal_form": "смысл"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Обфускация",
              "start_pos": 165,
              "normal_form": "обфускация"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "защиты информации",
              "start_pos": 204,
              "normal_form": "защита информации"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "создания препятствий",
              "start_pos": 230,
              "normal_form": "создание препятствий"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "промпта",
              "start_pos": 275,
              "normal_form": "промпт"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "Обфускация",
                "index": 0
              },
              "predicate": "isAppliedTo",
              "term2": {
                "class": "Object",
                "value": "промпта",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Обфускация",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "защиты информации",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Обфускация",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "создания препятствий",
                "index": 0
              }
            }
          ],
          "id": 803982
        }
      ],
      "text_797561": [
        {
          "text": "Привет, Хабр! Если вы интересуетесь NLP или просто современными DL моделями, то приглашаю вас узнать, как можно, имея всего лишь одну A100, около 30 гигабайтов текста и несколько дней обучения, решить проблему ограниченного окна контекста для русскоязычных трансформеров. А ещё сделаем несколько оптимизаций и добьёмся почти лучших метрик в бенчмарке encodechka.",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "Хабр",
              "start_pos": 8,
              "normal_form": "хабр"
            },
            {
              "index": 0,
              "class": "Science",
              "value": "NLP",
              "start_pos": 36,
              "normal_form": "nlp"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "DL моделями",
              "start_pos": 64,
              "normal_form": "dl модель"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русскоязычных",
              "start_pos": 243,
              "normal_form": "русскоязычный"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "трансформеров",
              "start_pos": 257,
              "normal_form": "трансформеров"
            },
            {
              "index": 0,
              "class": "Dataset",
              "value": "encodechka",
              "start_pos": 351,
              "normal_form": "encodechka"
            }
          ],
          "relations": [],
          "id": 797561
        },
        {
          "text": "Получившиеся русскоязычные модели доступны в открытом доступе на HuggingFace 🤗:",
          "terms": [
            {
              "index": 0,
              "class": "Lang",
              "value": "русскоязычные",
              "start_pos": 13,
              "normal_form": "русскоязычный"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 27,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "InfoResource",
              "value": "HuggingFace",
              "start_pos": 65,
              "normal_form": "huggingface"
            }
          ],
          "relations": [],
          "id": 797562
        },
        {
          "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "Методы позиционного кодирования",
              "start_pos": 0,
              "normal_form": "метод позиционного кодирования"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 114,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "расширения контекста",
              "start_pos": 207,
              "normal_form": "расширение контекста"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "декодеров",
              "start_pos": 381,
              "normal_form": "декодер"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "англоязычных",
              "start_pos": 410,
              "normal_form": "англоязычный"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей",
              "start_pos": 366,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей",
              "start_pos": 442,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "методов позиционного кодирования",
              "start_pos": 450,
              "normal_form": "метод позиционного кодирования"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "энкодерах",
              "start_pos": 485,
              "normal_form": "энкодер"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "DeBERTa",
              "start_pos": 519,
              "normal_form": "deberta"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "disentangled attention",
              "start_pos": 533,
              "normal_form": "disentangled attention"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "DeBERTa",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "энкодерах",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "DeBERTa",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "англоязычных",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "disentangled attention",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "DeBERTa",
                "index": 0
              }
            }
          ],
          "id": 797563
        },
        {
          "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
          "terms": [
            {
              "index": 0,
              "class": "Date",
              "value": "2021",
              "start_pos": 22,
              "normal_form": "2021"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "RoFormer",
              "start_pos": 48,
              "normal_form": "roformer"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Rotary Position Embedding",
              "start_pos": 79,
              "normal_form": "rotary position embedding"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "LLM",
              "start_pos": 151,
              "normal_form": "llm"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Mistral",
              "start_pos": 156,
              "normal_form": "mistral"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Llama",
              "start_pos": 165,
              "normal_form": "llama"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "GPT-NeoX",
              "start_pos": 172,
              "normal_form": "GPT-NeoX"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "RoFormer",
              "start_pos": 201,
              "normal_form": "roformer"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русского языка",
              "start_pos": 233,
              "normal_form": "русский язык"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русском",
              "start_pos": 326,
              "normal_form": "русский"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "cointegrated/rubert-tiny2",
              "start_pos": 406,
              "normal_form": "cointegrated/rubert-tiny2"
            },
            {
              "index": 0,
              "class": "Person",
              "value": "Дэвида Дале",
              "start_pos": 435,
              "normal_form": "дэвид дале"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "kazzand/ru-longformer-tiny-16384",
              "start_pos": 449,
              "normal_form": "kazzand/ru-longformer-tiny-16384"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "MTS",
              "start_pos": 485,
              "normal_form": "mts"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "адаптация",
              "start_pos": 528,
              "normal_form": "адаптация"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей",
              "start_pos": 315,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "увеличение контекстного окна",
              "start_pos": 585,
              "normal_form": "увеличение контекстного окна"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "улучшение их производительности",
              "start_pos": 616,
              "normal_form": "улучшение их производительности"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "RoPE",
              "start_pos": 579,
              "normal_form": "rope"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели длинного контекста",
              "start_pos": 732,
              "normal_form": "модель длинного контекста"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русского языка",
              "start_pos": 776,
              "normal_form": "русский язык"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "cointegrated/rubert-tiny2",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "русского языка",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "kazzand/ru-longformer-tiny-16384",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "русского языка",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Date",
                "value": "2021",
                "index": 0
              },
              "predicate": "isDateOf",
              "term2": {
                "class": "Model",
                "value": "RoFormer",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Rotary Position Embedding",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "RoFormer",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Rotary Position Embedding",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "Mistral",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Rotary Position Embedding",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "Llama",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Rotary Position Embedding",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "GPT-NeoX",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "GPT-NeoX",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "Llama",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "Mistral",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "LLM",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Lang",
                "value": "русского языка",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Model",
                "value": "RoFormer",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "cointegrated/rubert-tiny2",
                "index": 0
              },
              "predicate": "hasAuthor",
              "term2": {
                "class": "Person",
                "value": "Дэвида Дале",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "kazzand/ru-longformer-tiny-16384",
                "index": 0
              },
              "predicate": "hasAuthor",
              "term2": {
                "class": "Organization",
                "value": "MTS",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "RoPE",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Method",
                "value": "Rotary Position Embedding",
                "index": 0
              }
            }
          ],
          "id": 797564
        },
        {
          "text": "Как же адаптировать модели с выученным позиционным кодированием для работы с RoPE? Очевидно, что нам нужно просто заменить одно на другое в архитектуре модели. Для этого мы можем удалить nn.Embedding слой, отвечающий за старое позиционное кодирование, а далее модифицировать SelfAttention блок в трансформере так, чтобы position_ids передавались на каждый слой и там же применялся RoPE к query и key проекциям, ведь именно так он и устроен.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 20,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "адаптировать",
              "start_pos": 7,
              "normal_form": "адаптировать"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 20,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "трансформере",
              "start_pos": 296,
              "normal_form": "трансформер"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "модифицировать",
              "start_pos": 260,
              "normal_form": "модифицировать"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "RoPE",
              "start_pos": 77,
              "normal_form": "rope"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "модифицировать",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "адаптировать",
                "index": 0
              }
            }
          ],
          "id": 797565
        },
        {
          "text": "Применив такое преобразование, мы сломаем модель почти полностью. Но это не мешает нам восстановить её способности, так как большинство весов мы не изменяли. Для этого, как простой вариант, нам потребуется просто предобучить модель в режиме RoBERTы — то есть учить MLM задачу, применив поверх модели соответствующую голову.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "RoBERTы",
              "start_pos": 241,
              "normal_form": "robertа"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 42,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "предобучить",
              "start_pos": 213,
              "normal_form": "предобучить"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 225,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "MLM задачу",
              "start_pos": 265,
              "normal_form": "mlm задача"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 293,
              "normal_form": "модель"
            }
          ],
          "relations": [],
          "id": 797566
        },
        {
          "text": "Но также, мы хотим именно восстановить способности модели, а не просто обучить её на новом датасете — это значит, дистиллировать оригинальную модель-учитель, в нашего RoPE-ученика. Мне кажется, сейчас уместнее это называть клонированием, так как размеры и архитектура остаются одинаковыми. Для того чтобы это осуществить, достаточно к MLM-лоссу добавить дистилляционный лосс, который может быть, по большому счёту, любым, кроме KL-дивергенции, так как MLM-головы никто не кладёт с весами модели, и распределения учителя на токены у нас не будет. Я решил использовать 2 лосса, по отдельности: Cosine Similarity усредненый по токенам, для каждого слоя, и InfoNCE, в симметричном режиме, только для последнего слоя, именно он используется в CLIP для сближения модальностей текста и картинок. В обоих лоссах также подбиралась и своя температура для увеличения важности близости.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "модели",
              "start_pos": 51,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "восстановить способности модели",
              "start_pos": 26,
              "normal_form": "восстановить способности модели"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучить",
              "start_pos": 71,
              "normal_form": "обучить"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "оригинальную модель-учитель",
              "start_pos": 129,
              "normal_form": "оригинальная модель-учитель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "RoPE-ученика",
              "start_pos": 167,
              "normal_form": "RoPE-ученик"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "клонированием",
              "start_pos": 223,
              "normal_form": "клонирование"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "MLM-лоссу",
              "start_pos": 335,
              "normal_form": "MLM-лосс"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "дистилляционный лосс",
              "start_pos": 354,
              "normal_form": "дистилляционный лосс"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Cosine Similarity",
              "start_pos": 592,
              "normal_form": "cosine similarity"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "InfoNCE",
              "start_pos": 653,
              "normal_form": "infonce"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "CLIP",
              "start_pos": 738,
              "normal_form": "clip"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "сближения модальностей",
              "start_pos": 747,
              "normal_form": "сближение модальностей"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "текста",
              "start_pos": 770,
              "normal_form": "текст"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "картинок",
              "start_pos": 779,
              "normal_form": "картинка"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "клонированием",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "восстановить способности модели",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "MLM-лоссу",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Method",
                "value": "клонированием",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "дистилляционный лосс",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Method",
                "value": "клонированием",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "InfoNCE",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "клонированием",
                "value": "CLIP",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "InfoNCE",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "сближения модальностей",
                "index": 0
              }
            }
          ],
          "id": 797567
        },
        {
          "text": "В качестве данных я взял ~32 гигабайта из первых 10 русскоязычных фолдов датасета CulturaX. В семплах это составило около 5 миллионов. Данный датасет хорош тем, что довольно хорошо подготовлен и отфильтрован, в русских семплах действительно мало других языков. А ещё он имеет хорошее распределение длин, тут найдутся примеры для обучения модели с практически любым контекстным окном до 16к.",
          "terms": [
            {
              "index": 0,
              "class": "Dataset",
              "value": "CulturaX",
              "start_pos": 82,
              "normal_form": "CulturaX"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русскоязычных",
              "start_pos": 52,
              "normal_form": "русскоязычный"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "семплах",
              "start_pos": 94,
              "normal_form": "семпл"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русских",
              "start_pos": 211,
              "normal_form": "русский"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "семплах",
              "start_pos": 219,
              "normal_form": "семпл"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Dataset",
                "value": "CulturaX",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "русскоязычных",
                "index": 0
              }
            }
          ],
          "id": 797568
        },
        {
          "text": "Эта первая модель проекта является клоном ai-forever/ruBert-base. Для её клонирования использовался послойный cosine similarity лосс с температурой 0.5 между скрытыми представлениями ученика и учителя.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "ai-forever/ruBert-base",
              "start_pos": 42,
              "normal_form": "ai-forever/ruBert-base"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "клонирования",
              "start_pos": 73,
              "normal_form": "клонирование"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "послойный cosine similarity лосс",
              "start_pos": 100,
              "normal_form": "послойный cosine similarity лосс"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "послойный cosine similarity лосс",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "клонирования",
                "index": 0
              }
            }
          ],
          "id": 797569
        },
        {
          "text": "Эта же модель является клоном hivaze/ru-e5-base, а та в свою очередь русифицированной версией intfloat/multilingual-e5-base, полученой с помощью удаления большинства лишних токенов из других языков, по методу Дэвида Дале, снижая таким образом размер словаря с 250k до 69k, что существенно облегчило её тренировку. Кстати, модель, в связи с изначальной мультиязычностью, сохраняет свойства и на английском языке, но он не являлся основным.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "hivaze/ru-e5-base",
              "start_pos": 30,
              "normal_form": "hivaze/ru-e5-base"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "intfloat/multilingual-e5-base",
              "start_pos": 94,
              "normal_form": "intfloat/multilingual-e5-base"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "методу Дэвида Дале",
              "start_pos": 202,
              "normal_form": "метод дэвида дале"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "тренировку",
              "start_pos": 302,
              "normal_form": "тренировка"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "английском языке",
              "start_pos": 394,
              "normal_form": "английский язык"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "мультиязычностью",
              "start_pos": 352,
              "normal_form": "мультиязычность"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 7,
              "normal_form": "модель"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "hivaze/ru-e5-base",
                "index": 0
              },
              "predicate": "isModificationOf",
              "term2": {
                "class": "Model",
                "value": "intfloat/multilingual-e5-base",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "методу Дэвида Дале",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "тренировку",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "модель",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "мультиязычностью",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "модель",
                "index": 0
              },
              "predicate": "Language",
              "term2": {
                "class": "Lang",
                "value": "английском языке",
                "index": 0
              }
            }
          ],
          "id": 797570
        },
        {
          "text": "Из-за своей особенности представления похожести текстов, e5 клонировалась с симметричным InfoNCE лоссом (как в CLIP) поверх последнего слоя и температурой 0.1. Выбор этого лосса обоснован тем, что модель должна гораздо лучше понимать разницу между разными по смыслу текстами, а не просто сближать косинусную близость со своим оригиналом. Кстати, этот же лосс использовался авторами и при обучении оригинальной e5.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "e5",
              "start_pos": 57,
              "normal_form": "e5"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "симметричным InfoNCE лоссом",
              "start_pos": 76,
              "normal_form": "симметричный infonce лосс"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "CLIP",
              "start_pos": 111,
              "normal_form": "clip"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 197,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "текстами",
              "start_pos": 266,
              "normal_form": "текст"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "косинусную близость",
              "start_pos": 297,
              "normal_form": "косинусная близость"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучении",
              "start_pos": 388,
              "normal_form": "обучение"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "оригинальной e5",
              "start_pos": 397,
              "normal_form": "оригинальная e5"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "симметричным InfoNCE лоссом",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "e5",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "симметричным InfoNCE лоссом",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "оригинальной e5",
                "index": 0
              }
            }
          ],
          "id": 797571
        },
        {
          "text": "Я не буду подробно описывать как работает RoPE, а также его формулы и почему работают методы его интерполяции/экстраполяции. Для этого я предлагаю изучить статью Scaling Laws of RoPE-based Extrapolation или пост на хабре О методах позиционного кодирования в Transformer, а ещё оригинальную статью Roformer.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "RoPE",
              "start_pos": 42,
              "normal_form": "rope"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "методах позиционного кодирования",
              "start_pos": 223,
              "normal_form": "метод позиционного кодирования"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Transformer",
              "start_pos": 258,
              "normal_form": "transformer"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Roformer",
              "start_pos": 297,
              "normal_form": "roformer"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "методах позиционного кодирования",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "Transformer",
                "index": 0
              }
            }
          ],
          "id": 797572
        },
        {
          "text": "Итак, использовать клонирование оригинальной модели, очевидно, можно только на длинах до 512 токенов. Но нам нужно больше, поэтому вторым этапом в пайплайне подготовки моделей стало расширение контекста — дообучение моделей только с MLM лоссом и только на текстах с длинами 512-2048 токенов.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "клонирование оригинальной модели",
              "start_pos": 19,
              "normal_form": "клонирование оригинальной модели"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "расширение контекста",
              "start_pos": 182,
              "normal_form": "расширение контекста"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "дообучение",
              "start_pos": 205,
              "normal_form": "дообучение"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "MLM лоссом",
              "start_pos": 233,
              "normal_form": "mlm лосс"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "MLM лоссом",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "дообучение",
                "index": 0
              }
            }
          ],
          "id": 797573
        },
        {
          "text": "В ходе экспериментов, я пришёл к выводу, что тут лучше не использовать какие-либо техники интерполяции или экстраполяции RoPE эмбедингов при обучении. Дело в том, что модель и так довольно легко сходилась на увеличенном контексте к качеству, как на 512 токенах, ещё где-то на ~200к семплах. Техники, где мы как-то влияем на эмбединги, конечно, помогают, но они нужны скорее для значительного увеличения контекста, например с 4к до 16к, так как там потребовалось бы больше времени и данных, если ничего не использовать.",
          "terms": [
            {
              "index": 0,
              "class": "Activity",
              "value": "экспериментов",
              "start_pos": 7,
              "normal_form": "эксперимент"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "интерполяция",
              "start_pos": 90,
              "normal_form": "интерполяция"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "экстраполяция",
              "start_pos": 107,
              "normal_form": "экстраполяция"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучении",
              "start_pos": 141,
              "normal_form": "обучение"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 167,
              "normal_form": "модель"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "интерполяция",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "обучении",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "экстраполяция",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "обучении",
                "index": 0
              }
            }
          ],
          "id": 797574
        },
        {
          "text": "Тут вы могли заметить: если мы будем дообучать модель на длинный контекст без клонирующего лосса, то как мы убедимся, что наша модель будет всё ещё похожа на оригинальную? На самом деле, очень просто — нам достаточно обучать только attention блоки, не трогая последующий MLP блок. RoPE затрагивает именно query и key, соответственно, только их проекции нам и нужно дообучать, MLP будет мапить получившиеся вектора примерно в то же место в пространстве, что и раньше, а значит характеристики вроде косинусной близости между скрытыми представлениями с оригиналом, почти не изменятся.",
          "terms": [
            {
              "index": 0,
              "class": "Task",
              "value": "дообучать",
              "start_pos": 37,
              "normal_form": "дообучать"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "клонирующего лосса",
              "start_pos": 78,
              "normal_form": "клонирующий лосс"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "RoPE",
              "start_pos": 281,
              "normal_form": "rope"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "дообучать",
              "start_pos": 365,
              "normal_form": "дообучать"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "косинусной близости",
              "start_pos": 497,
              "normal_form": "косинусная близость"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "query",
              "start_pos": 305,
              "normal_form": "query"
            },
            {
              "index": 0,
              "class": "Object",
              "value": "key",
              "start_pos": 313,
              "normal_form": "key"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "MLP",
              "start_pos": 271,
              "normal_form": "mlp"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "RoPE",
                "index": 0
              },
              "predicate": "isAppliedTo",
              "term2": {
                "class": "Object",
                "value": "query",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "RoPE",
                "index": 0
              },
              "predicate": "isAppliedTo",
              "term2": {
                "class": "Object",
                "value": "key",
                "index": 0
              }
            }
          ],
          "id": 797575
        },
        {
          "text": "Для дообучения мы не используем никакие трюки со скейлингом RoPE. Но вот для инференса вполне можем, есть как минимум метод Dynamic-NTK, который позволяет применять его без дообучения. Это означает, что потенциально, модели могут использоваться и на более длинном контексте чем 2k и не очень сильно терять в качестве.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "скейлингом RoPE",
              "start_pos": 49,
              "normal_form": "скейлинг rope"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "дообучения",
              "start_pos": 4,
              "normal_form": "дообучение"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Dynamic-NTK",
              "start_pos": 124,
              "normal_form": "Dynamic-NTK"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "инференса",
              "start_pos": 77,
              "normal_form": "инференс"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "Dynamic-NTK",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "инференса",
                "index": 0
              }
            }
          ],
          "id": 797576
        },
        {
          "text": "Ниже приведена таблица с различными оценками модели ruRoPEBert-e5-base-2k, которая была дополнительно дообучена на 512-2048 токенах, на разных длинах контекста с Dynamic-NTK скейлингом и без него.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "ruRoPEBert-e5-base-2k",
              "start_pos": 52,
              "normal_form": "ruRoPEBert-e5-base-2k"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Dynamic-NTK скейлингом",
              "start_pos": 162,
              "normal_form": "скейлинг"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "дообучена",
              "start_pos": 102,
              "normal_form": "дообученный"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "Dynamic-NTK скейлингом",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "ruRoPEBert-e5-base-2k",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Dynamic-NTK скейлингом",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "дообучена",
                "index": 0
              }
            }
          ],
          "id": 797577
        },
        {
          "text": "Как видно, скейлинг помогает адаптироваться на длины больше, чем модель видела при обучении, но тем не менее, качество всё ещё ощутимо деградирует, критичность такой деградации будет зависеть от конкретных задач. Мне кажется, для улучшения результатов, тут стоит попробовать другие мощные методы, вроде xPos или Alibi, подробнее о них можно узнать в статье A Length-Extrapolatable Transformer.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 6,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "скейлинг",
              "start_pos": 11,
              "normal_form": "скейлинг"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "xPos",
              "start_pos": 303,
              "normal_form": "xpos"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Alibi",
              "start_pos": 312,
              "normal_form": "alibi"
            }
          ],
          "relations": [],
          "id": 797578
        },
        {
          "text": "Так как ручное вычисление внимания (eager) является абсолютно неоптимизированным, квадратичным по вычислительной сложности и слабым местом по памяти, его нельзя просто использовать как он есть, если мы хотим увеличить контекст модели в несколько раз.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "вычисление внимания",
              "start_pos": 15,
              "normal_form": "вычисление внимания"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "увеличить контекст модели",
              "start_pos": 208,
              "normal_form": "увеличить контекст модели"
            }
          ],
          "relations": [],
          "id": 797579
        },
        {
          "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
          "terms": [
            {
              "index": 0,
              "class": "Library",
              "value": "FlashAttention",
              "start_pos": 122,
              "normal_form": "flashattention"
            },
            {
              "index": 0,
              "class": "Library",
              "value": "xformers",
              "start_pos": 109,
              "normal_form": "xformers"
            },
            {
              "index": 0,
              "class": "Library",
              "value": "PyTorch",
              "start_pos": 244,
              "normal_form": "pytorch"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Memory-Efficient Attention",
              "start_pos": 445,
              "normal_form": "Memory-Efficient Attention"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "FlashAttention",
              "start_pos": 122,
              "normal_form": "flashattention"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Fused C++",
              "start_pos": 489,
              "normal_form": "fused c++"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "нативный метод",
              "start_pos": 201,
              "normal_form": "нативный метод"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "torch.nn.functional.scaled_dot_product_attention",
              "start_pos": 254,
              "normal_form": "torch.nn.functional.scaled_dot_product_attention"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "sdpa",
              "start_pos": 304,
              "normal_form": "sdpa"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "механизм вычисления внимания",
              "start_pos": 49,
              "normal_form": "механизм вычисления внимания"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "torch.nn.functional.scaled_dot_product_attention",
                "index": 0
              },
              "predicate": "isAlternativeNameFor",
              "term2": {
                "class": "Method",
                "value": "sdpa",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "torch.nn.functional.scaled_dot_product_attention",
                "index": 0
              },
              "predicate": "isUsedIn",
              "term2": {
                "class": "Library",
                "value": "PyTorch",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Memory-Efficient Attention",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Method",
                "value": "нативный метод",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "FlashAttention",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Method",
                "value": "нативный метод",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "Fused C++",
                "index": 0
              },
              "predicate": "isPartOf",
              "term2": {
                "class": "Method",
                "value": "нативный метод",
                "index": 0
              }
            }
          ],
          "id": 797580
        },
        {
          "text": "В недавнем релизе 2.2.0 авторы PyTorch добавили поддержку FlashAttention второй версии в эту функцию, тем самым она стала, наверное, самым удобным способом добавления оптимизации вычисления внимания в модель, так как использование нативного Flash не очень удобное.",
          "terms": [
            {
              "index": 0,
              "class": "Library",
              "value": "PyTorch",
              "start_pos": 31,
              "normal_form": "pytorch"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "FlashAttention",
              "start_pos": 58,
              "normal_form": "flashattention"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "оптимизации вычисления внимания",
              "start_pos": 167,
              "normal_form": "оптимизация вычисления внимания"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "нативного Flash",
              "start_pos": 231,
              "normal_form": "нативный flash"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "FlashAttention",
                "index": 0
              },
              "predicate": "isUsedIn",
              "term2": {
                "class": "Library",
                "value": "PyTorch",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "FlashAttention",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "оптимизации вычисления внимания",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "нативного Flash",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "оптимизации вычисления внимания",
                "index": 0
              }
            }
          ],
          "id": 797581
        },
        {
          "text": "В таблице ниже я привожу бенчмарки инференса, произведённые с помощью Pytorch Profiler 12-ти слойного ruRoPEBert в конфигурации base. Замеры производились на ноутбучной RTX 3070 Ti с процессором Intel Core i7 12700H. Размер батча во всех тестах = 1.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "Pytorch Profiler",
              "start_pos": 70,
              "normal_form": "pytorch profiler"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "ruRoPEBert",
              "start_pos": 102,
              "normal_form": "ruropebert"
            }
          ],
          "relations": [],
          "id": 797582
        },
        {
          "text": "Эффективность использования памяти при выборе sdpa вырастает в несколько раз, также модель получает ускорение относительно eager, пропорциональное увеличению размера контекста. При обучении также получается достичь ускорения в 2-3 раза и кратно сэкономить память.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "sdpa",
              "start_pos": 46,
              "normal_form": "sdpa"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "обучении",
              "start_pos": 181,
              "normal_form": "обучение"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "sdpa",
                "index": 0
              },
              "predicate": "solves",
              "term2": {
                "class": "Task",
                "value": "обучении",
                "index": 0
              }
            }
          ],
          "id": 797583
        },
        {
          "text": "Использовать модель, выбирая соответствующий способ вычисления внимания можно следующим образом:",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "модель",
              "start_pos": 13,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "вычисления внимания",
              "start_pos": 52,
              "normal_form": "вычисление внимания"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "модель",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "вычисления внимания",
                "index": 0
              }
            }
          ],
          "id": 797584
        },
        {
          "text": "За основу бенчмарка качества был выбран encodechka от Дэвида Дале, так как он обладает единым и простым интерфейсом запуска тестов, в нём есть немало современных моделей, а главное, что оценка производится исключительно на русскоязычных датасетах. К тому же он даёт возможность сравнить модели по их способности в семантическое представление предложений, что является важной целью данного проекта.",
          "terms": [
            {
              "index": 0,
              "class": "dataset",
              "value": "encodechka",
              "start_pos": 40,
              "normal_form": "encodechka"
            },
            {
              "index": 0,
              "class": "Person",
              "value": "Дэвида Дале",
              "start_pos": 54,
              "normal_form": "дэвид дале"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "современных моделей",
              "start_pos": 150,
              "normal_form": "современная модель"
            },
            {
              "index": 0,
              "class": "Dataset",
              "value": "русскоязычных датасетах",
              "start_pos": 223,
              "normal_form": "русскоязычный датасет"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "семантическое представление предложений",
              "start_pos": 314,
              "normal_form": "семантическое представление предложений"
            }
          ],
          "relations": [],
          "id": 797585
        },
        {
          "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "ruRoPEBert-e5-base-2k",
              "start_pos": 11,
              "normal_form": "ruRoPEBert-e5-base-2k"
            },
            {
              "index": 0,
              "class": "Dataset",
              "value": "encodechka",
              "start_pos": 75,
              "normal_form": "encodechka"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "S+W",
              "start_pos": 95,
              "normal_form": "s+w"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "sentence + word embeddings",
              "start_pos": 100,
              "normal_form": "sentence + word embeddings"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей e5",
              "start_pos": 157,
              "normal_form": "модель e5"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "STS",
              "start_pos": 171,
              "normal_form": "sts"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "взять температуру ниже, чем 0.1",
              "start_pos": 237,
              "normal_form": "взять температуру ниже, чем 0.1"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "использовать дополнительные датасеты",
              "start_pos": 273,
              "normal_form": "использовать дополнительный датасет"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "ruRoPEBert",
              "start_pos": 11,
              "normal_form": "ruropebert"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "оригинал e5-base",
              "start_pos": 381,
              "normal_form": "оригинал e5-base"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "text-embedding-ada-002",
              "start_pos": 459,
              "normal_form": "text-embedding-ada-002"
            },
            {
              "index": 0,
              "class": "Organization",
              "value": "OpenAI",
              "start_pos": 485,
              "normal_form": "openai"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "text-embedding-ada-002",
                "index": 0
              },
              "predicate": "hasAuthor",
              "term2": {
                "class": "Organization",
                "value": "OpenAI",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "STS",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "оригинал e5-base",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "STS",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "модели e5",
                "index": 0
              }
            }
          ],
          "id": 797586
        },
        {
          "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "RoPEBert модели",
              "start_pos": 13,
              "normal_form": "ropebert модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "ai-forever/ruBert-base",
              "start_pos": 72,
              "normal_form": "ai-forever/ruBert-base"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "ai-forever/ruRoberta-large",
              "start_pos": 117,
              "normal_form": "ai-forever/ruRoberta-large"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "NE1",
              "start_pos": 160,
              "normal_form": "ne1"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "NE2",
              "start_pos": 166,
              "normal_form": "ne2"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "classic модели",
              "start_pos": 171,
              "normal_form": "classic модель"
            },
            {
              "index": 0,
              "class": "Task",
              "value": "NER",
              "start_pos": 225,
              "normal_form": "ner"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "sentence клоны e5",
              "start_pos": 240,
              "normal_form": "sentence клон e5"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "RoPEBert",
                "index": 0
              },
              "predicate": "isModificationOf",
              "term2": {
                "class": "Model",
                "value": "ai-forever/ruBert-base",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "RoPEBert",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "NER",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "ai-forever/ruBert-base",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "NER",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "ai-forever/ruRoberta-large",
                "index": 0
              },
              "predicate": "isUsedForSolving",
              "term2": {
                "class": "Task",
                "value": "NER",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "NE1",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "classic модели",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "NE1",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "sentence клоны e5",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "NE2",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "classic модели",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Metric",
                "value": "NE2",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "sentence клоны e5",
                "index": 0
              }
            }
          ],
          "id": 797587
        },
        {
          "text": "Не всё получилось, как было задумано, например, в бенчмарке STS клонированная e5 модель проигрывает оригиналу, а также модели, на мой взгляд, не очень хорошо обобщаются на длины большие, чем видели при обучении.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "клонированная e5 модель",
              "start_pos": 64,
              "normal_form": "клонированная e5 модель"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "оригиналу",
              "start_pos": 100,
              "normal_form": "оригинал"
            },
            {
              "index": 0,
              "class": "Metric",
              "value": "STS",
              "start_pos": 60,
              "normal_form": "sts"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Metric",
                "value": "STS",
                "index": 0
              },
              "predicate": "isUsedFor",
              "term2": {
                "class": "Model",
                "value": "клонированная e5 модель",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "клонированная e5 модель",
                "index": 0
              },
              "predicate": "isModificationOf",
              "term2": {
                "class": "Model",
                "value": "оригиналу",
                "index": 0
              }
            }
          ],
          "id": 797588
        },
        {
          "text": "Эти проблемы можно решить усовершенствовав пайплайн тренировки или некоторые гиперпараметры. Также можно заменить механизм экстраполяции, например, на xPos, Alibi или совсем недавний LongRoPE, а можно сделать спарсификацию аттеншена, как это предлагается в статье Blockwise Self-Attention for Long Document Understanding, ещё от 2019 года. Кроме того, в самой статье RoFormer упоминается, что авторы попробовали объединить RoPE с архитектурой Performer и работать посимвольно и линейно по сложности.",
          "terms": [
            {
              "index": 0,
              "class": "Method",
              "value": "xPos",
              "start_pos": 151,
              "normal_form": "xpos"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "Alibi",
              "start_pos": 157,
              "normal_form": "alibi"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "LongRoPE",
              "start_pos": 183,
              "normal_form": "longrope"
            },
            {
              "index": 0,
              "class": "Date",
              "value": "2019",
              "start_pos": 329,
              "normal_form": "2019"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Performer",
              "start_pos": 443,
              "normal_form": "performer"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "RoFormer",
              "start_pos": 367,
              "normal_form": "roformer"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "механизм экстраполяции",
              "start_pos": 114,
              "normal_form": "механизм экстраполяции"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "усовершенствовав пайплайн тренировки или некоторые гиперпараметры",
              "start_pos": 26,
              "normal_form": "усовершенствовать пайплайн тренировки или некоторые гиперпараметры"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "спарсификацию аттеншена",
              "start_pos": 209,
              "normal_form": "спарсификация аттеншена"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "объединить RoPE с архитектурой Performer",
              "start_pos": 412,
              "normal_form": "объединить rope с архитектурой performer"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "работать посимвольно и линейно по сложности",
              "start_pos": 455,
              "normal_form": "работать посимвольно и линейно по сложности"
            }
          ],
          "relations": [],
          "id": 797589
        },
        {
          "text": "Удивляет, что существует огромное количество архитектур (от старого Reformer до новейших Biderectional SSM), которые могли бы эффективно использоваться уже долгое время в энкодерах, но стандартный вариант с квадратичным вниманием — всё ещё остаётся единственным путём, которым идёт большинство SOTA моделей, особенно на русском языке. В конечном итоге этот проект делает совсем небольшое усилие в сторону современных подходов в NLP и доказывает их относительную простоту реализации.",
          "terms": [
            {
              "index": 0,
              "class": "Model",
              "value": "архитектур",
              "start_pos": 45,
              "normal_form": "архитектура"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Reformer",
              "start_pos": 68,
              "normal_form": "reformer"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "Biderectional SSM",
              "start_pos": 89,
              "normal_form": "biderectional ssm"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "стандартный вариант с квадратичным вниманием",
              "start_pos": 185,
              "normal_form": "стандартный вариант с квадратичным вниманием"
            },
            {
              "index": 0,
              "class": "Lang",
              "value": "русском языке",
              "start_pos": 320,
              "normal_form": "русский язык"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "большинство SOTA моделей",
              "start_pos": 282,
              "normal_form": "большинство sota моделей"
            },
            {
              "index": 0,
              "class": "Science",
              "value": "NLP",
              "start_pos": 428,
              "normal_form": "nlp"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Model",
                "value": "Reformer",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "архитектур",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "Biderectional SSM",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "архитектур",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Method",
                "value": "стандартный вариант с квадратичным вниманием",
                "index": 0
              },
              "predicate": "isExampleOf",
              "term2": {
                "class": "Model",
                "value": "архитектур",
                "index": 0
              }
            }
          ],
          "id": 797590
        },
        {
          "text": "Веса всех моделей, их код и инструкции по запуску доступны в нашем аккаунте на HuggingFace (ссылки в начале статьи). В дальнейшем, у нас есть планы по выпуску large версий моделей и дотренировке с контекстом до 8k токенов, а также продолжение экспериментов с оптимизациями.",
          "terms": [
            {
              "index": 0,
              "class": "InfoResource",
              "value": "HuggingFace",
              "start_pos": 79,
              "normal_form": "huggingface"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "large версий",
              "start_pos": 159,
              "normal_form": "large версия"
            },
            {
              "index": 0,
              "class": "Model",
              "value": "моделей",
              "start_pos": 10,
              "normal_form": "модель"
            },
            {
              "index": 0,
              "class": "Method",
              "value": "дотренировке",
              "start_pos": 182,
              "normal_form": "дотренировка"
            },
            {
              "index": 0,
              "class": "Activity",
              "value": "экспериментов с оптимизациями",
              "start_pos": 243,
              "normal_form": "эксперимент с оптимизациями"
            }
          ],
          "relations": [
            {
              "term1": {
                "class": "Method",
                "value": "дотренировке",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "large версий",
                "index": 0
              }
            },
            {
              "term1": {
                "class": "Model",
                "value": "large версий",
                "index": 0
              },
              "predicate": "isUsedForTraining",
              "term2": {
                "class": "Model",
                "value": "моделей",
                "index": 0
              }
            }
          ],
          "id": 797591
        }
      ]
    }
  ]
}