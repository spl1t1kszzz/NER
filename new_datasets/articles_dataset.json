{
  "examples": [
    {
      "text": "Хабр, привет! Меня зовут Антон Разжигаев, я аспирант Сколтеха и участник научной группы Fusion Brain в институте AIRI.",
      "terms": [
        {
          "index": 0,
          "class": "InfoResource",
          "value": "Хабр",
          "start_pos": 0
        },
        {
          "index": 0,
          "class": "Person",
          "value": "Антон Разжигаев",
          "start_pos": 25
        },
        {
          "index": 0,
          "class": "Organization",
          "value": "Сколтеха",
          "start_pos": 53
        },
        {
          "index": 0,
          "class": "Organization",
          "value": "AIRI",
          "start_pos": 113
        },
        {
          "index": 0,
          "class": "Activity",
          "value": "Fusion Brain",
          "start_pos": 88
        }
      ],
      "relations": []
    },
    {
      "text": "С момента выхода первой статьи «Attention is All You Need» я с жадностью и любопытством, присущими любому исследователю, пытаюсь углубиться во все особенности и свойства моделей на базе архитектуры трансформер. Но, если честно, я до сих пор не понимаю, как они работают и почему так хорошо обучаются. Очень хочу разобраться, в чём же причина такой эффективности этих моделей, и есть ли предел их возможностей?",
      "terms": [
        {
          "index": 0,
          "class": "InfoResource",
          "value": "Attention is All You Need",
          "start_pos": 32
        },
        {
          "index": 0,
          "class": "Model",
          "value": "моделей на базе архитектуры трансформер",
          "start_pos": 170
        },
        {
          "index": 0,
          "class": "Model",
          "value": "моделей",
          "start_pos": 170
        }
      ],
      "relations": []
    },
    {
      "text": "Такому изучению трансформеров «под микроскопом» и посвящена наша научная работа, только что представленная на конференции EACL 2024, которая проходила на Мальте — «The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models». В этой работе мы сфокусировались на наблюдении за пространством эмбеддингов (активаций) на промежуточных слоях по мере обучения больших и маленьких языковых моделей (LM) и получили очень интересные результаты.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "трансформеров",
          "start_pos": 16
        },
        {
          "index": 0,
          "class": "Activity",
          "value": "конференции EACL 2024",
          "start_pos": 110
        },
        {
          "index": 0,
          "class": "InfoResource",
          "value": "The Shape of Learning: Anisotropy and Intrinsic Dimensions in Transformer-Based Models",
          "start_pos": 164
        },
        {
          "index": 0,
          "class": "Object",
          "value": "эмбеддингов",
          "start_pos": 317
        },
        {
          "index": 0,
          "class": "Object",
          "value": "активаций",
          "start_pos": 330
        },
        {
          "index": 0,
          "class": "Model",
          "value": "языковых моделей",
          "start_pos": 401
        },
        {
          "index": 0,
          "class": "Model",
          "value": "LM",
          "start_pos": 419
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Model",
            "value": "языковых моделей",
            "index": 0,
            "start_pos": 401
          },
          "predicate": "isAlternativeNameFor",
          "term2": {
            "class": "Model",
            "value": "LM",
            "index": 0,
            "start_pos": 419
          }
        }
      ]
    },
    {
      "text": "Итак, приступим!",
      "terms": [],
      "relations": []
    },
    {
      "text": "Начнём с рассказа о данных — это нужно для того, чтобы было проще понять, что мы сделали и что обнаружили. Т.к. нас интересовало пространство контекстуализированных эмбеддингов (в т.ч. промежуточных), надо было их где-то добыть.",
      "terms": [
        {
          "index": 0,
          "class": "Object",
          "value": "контекстуализированных эмбеддингов",
          "start_pos": 142
        }
      ],
      "relations": []
    },
    {
      "text": "Мы взяли enwik8 — аккуратно очищенные статьи Википедии на английском языке. Эти тексты мы прогнали через изучаемые модели, сохраняя все промежуточные активации (для каждого токена и с каждого слоя). Так мы получили «пространство эмбеддингов» или, другими словами, многомерное облако точек, с которым и стали дальше работать.",
      "terms": [
        {
          "index": 0,
          "class": "Dataset",
          "value": "enwik8",
          "start_pos": 9
        },
        {
          "index": 0,
          "class": "InfoResource",
          "value": "Википедии",
          "start_pos": 45
        },
        {
          "index": 0,
          "class": "Lang",
          "value": "английском языке",
          "start_pos": 58
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модели",
          "start_pos": 115
        },
        {
          "index": 0,
          "class": "Object",
          "value": "токена",
          "start_pos": 173
        },
        {
          "index": 0,
          "class": "Object",
          "value": "«пространство эмбеддингов»",
          "start_pos": 215
        },
        {
          "index": 0,
          "class": "Object",
          "value": "многомерное облако точек",
          "start_pos": 264
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Object",
            "value": "«пространство эмбеддингов»",
            "index": 0,
            "start_pos": 215
          },
          "predicate": "isAlternativeNameFor",
          "term2": {
            "class": "Object",
            "value": "многомерное облако точек",
            "index": 0,
            "start_pos": 264
          }
        },
        {
          "term1": {
            "class": "InfoResource",
            "value": "Википедии",
            "index": 0,
            "start_pos": 45
          },
          "predicate": "Language",
          "term2": {
            "class": "Lang",
            "value": "английском языке",
            "index": 0,
            "start_pos": 58
          }
        },
        {
          "term1": {
            "class": "Dataset",
            "value": "enwik8",
            "index": 0,
            "start_pos": 9
          },
          "predicate": "Language",
          "term2": {
            "class": "Lang",
            "value": "английском языке",
            "index": 0,
            "start_pos": 58
          }
        }
      ]
    },
    {
      "text": "Чтобы исключить зависимость наблюдений от выбранного датасета, мы повторили эксперименты на случайных последовательностях токенов, и все выводы повторились. Поэтому в дальнейшем не буду акцентировать на этом внимание, а лучше сразу перейду к результатам.",
      "terms": [
        {
          "index": 0,
          "class": "Dataset",
          "value": "датасета",
          "start_pos": 53
        },
        {
          "index": 0,
          "class": "Activity",
          "value": "эксперименты",
          "start_pos": 75
        },
        {
          "index": 0,
          "class": "Object",
          "value": "токенов",
          "start_pos": 122
        }
      ],
      "relations": []
    },
    {
      "text": "Один из самых главных вопросов, которые мы себе задали в процессе исследования — а какая вообще форма у этих облаков точек? Визуализировать их сложно — пространство эмбеддингов очень многомерно, — а методы снижения размерности тут не сильно помогают. Поэтому мы решили использовать анизотропию в качестве нашего «микроскопа». Анизотропия — это мера, показывающая насколько облако точек вытянуто, насколько оно неоднородно. Чем выше это значение, тем сильнее вытягивается пространство.",
      "terms": [
        {
          "index": 0,
          "class": "Activity",
          "value": "исследования",
          "start_pos": 66
        },
        {
          "index": 0,
          "class": "Task",
          "value": "Визуализировать",
          "start_pos": 124
        },
        {
          "index": 0,
          "class": "Object",
          "value": "эмбеддингов",
          "start_pos": 165
        },
        {
          "index": 0,
          "class": "Method",
          "value": "методы снижения размерности",
          "start_pos": 199
        },
        {
          "index": 0,
          "class": "Method",
          "value": "анизотропию",
          "start_pos": 282
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "Анизотропия",
          "start_pos": 326
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "методы снижения размерности",
            "index": 0,
            "start_pos": 199
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "Визуализировать",
            "index": 0,
            "start_pos": 124
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "анизотропию",
            "index": 0,
            "start_pos": 282
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "Визуализировать",
            "index": 0,
            "start_pos": 124
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "методы снижения размерности",
            "index": 0,
            "start_pos": 199
          },
          "predicate": "isAppliedTo",
          "term2": {
            "class": "Object",
            "value": "эмбеддингов",
            "index": 0,
            "start_pos": 165
          }
        }
      ]
    },
    {
      "text": "К примеру, уже давно известно (см. статью Representation Degeneration Problem in Training Natural Language Generation Models), что эмбеддинги трансформеров-энкодеров лежат в «узком конусе» — из-за этого косинусы между текстовыми репрезентациями всегда очень высокие. Но тем не менее, если вычесть среднее значение и отцентрировать это облако точек, оно становится вполне изотропным, то есть похожим на многомерный шарик. Поэтому говорят, что эмбеддинги трансформеров-энкодеров (Bert, RoBERTa,  Albert, …) локально изотропны.А в случае с декодерами (GPT, Llama, Mistral, …) мы обнаружили, что это совершенно не так! Даже после центрирования и использования более устойчивых к смещению методов на базе сингулярных чисел мы видим, что на средних слоях языковых моделей анизотропия практически равна 1. Это означает, что облако точек там вытянуто вдоль прямой линии. Но почему? Это же так сильно снижает ёмкость модели, она из-за этого практически не использует ТЫСЯЧИ других размерностей.",
      "terms": [
        {
          "index": 0,
          "class": "InfoResource",
          "value": "Representation Degeneration Problem in Training Natural Language Generation Models",
          "start_pos": 42
        },
        {
          "index": 0,
          "class": "Object",
          "value": "эмбеддинги",
          "start_pos": 131
        },
        {
          "index": 0,
          "class": "Model",
          "value": "трансформеров-энкодеров",
          "start_pos": 142
        },
        {
          "index": 0,
          "class": "Object",
          "value": "текстовыми репрезентациями",
          "start_pos": 218
        },
        {
          "index": 0,
          "class": "Object",
          "value": "эмбеддинги",
          "start_pos": 442
        },
        {
          "index": 0,
          "class": "Model",
          "value": "трансформеров-энкодеров",
          "start_pos": 453
        },
        {
          "index": 0,
          "class": "Model",
          "value": "Bert",
          "start_pos": 478
        },
        {
          "index": 0,
          "class": "Model",
          "value": "RoBERTa",
          "start_pos": 484
        },
        {
          "index": 0,
          "class": "Model",
          "value": "Albert",
          "start_pos": 494
        },
        {
          "index": 0,
          "class": "Model",
          "value": "декодерами",
          "start_pos": 537
        },
        {
          "index": 0,
          "class": "Model",
          "value": "GPT",
          "start_pos": 549
        },
        {
          "index": 0,
          "class": "Model",
          "value": "Llama",
          "start_pos": 554
        },
        {
          "index": 0,
          "class": "Model",
          "value": "Mistral",
          "start_pos": 561
        },
        {
          "index": 0,
          "class": "Method",
          "value": "центрирования",
          "start_pos": 626
        },
        {
          "index": 0,
          "class": "Method",
          "value": "методов на базе сингулярных",
          "start_pos": 684
        },
        {
          "index": 0,
          "class": "Model",
          "value": "языковых моделей",
          "start_pos": 749
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "анизотропия",
          "start_pos": 766
        },
        {
          "index": 0,
          "class": "Value",
          "value": "практически равна 1",
          "start_pos": 778
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модели",
          "start_pos": 908
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Metric",
            "value": "анизотропия",
            "index": 0,
            "start_pos": 766
          },
          "predicate": "hasValue",
          "term2": {
            "class": "Value",
            "value": "практически равна 1",
            "index": 0,
            "start_pos": 778
          }
        },
        {
          "term1": {
            "class": "Metric",
            "value": "анизотропия",
            "index": 0,
            "start_pos": 766
          },
          "predicate": "isUsedFor",
          "term2": {
            "class": "Model",
            "value": "декодерами",
            "index": 0,
            "start_pos": 537
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "центрирования",
            "index": 0,
            "start_pos": 626
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "декодерами",
            "index": 0,
            "start_pos": 537
          }
        }
      ]
    },
    {
      "text": "Откуда берётся эта неоднородность пространства репрезентаций в декодерах, мы пока не знаем, но предполагаем, что это связано с процессом их обучения, задачей предсказания следующего токена и треугольной маской внимания. Это одна из исследовательских задач, которая сейчас стоит перед нами.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "декодерах",
          "start_pos": 63
        },
        {
          "index": 0,
          "class": "Task",
          "value": "обучения",
          "start_pos": 140
        },
        {
          "index": 0,
          "class": "Task",
          "value": "предсказания следующего токена",
          "start_pos": 158
        },
        {
          "index": 0,
          "class": "Method",
          "value": "треугольной маской внимания",
          "start_pos": 192
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Model",
            "value": "декодерах",
            "index": 0,
            "start_pos": 63
          },
          "predicate": "isUsedForSolving",
          "term2": {
            "class": "Task",
            "value": "предсказания следующего токена",
            "index": 0,
            "start_pos": 158
          }
        }
      ]
    },
    {
      "text": "Если посмотреть на профиль анизотропии по слоям, то становится видно, что в начале и в конце декодеров эмбеддинги гораздо более изотропны, а экстремально высокая анизотропия наблюдается только в середине, где и должен происходить весь мыслительный процесс.",
      "terms": [
        {
          "index": 0,
          "class": "Metric",
          "value": "анизотропии",
          "start_pos": 27
        },
        {
          "index": 0,
          "class": "Model",
          "value": "декодеров",
          "start_pos": 93
        },
        {
          "index": 0,
          "class": "Object",
          "value": "эмбеддинги",
          "start_pos": 103
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "анизотропия",
          "start_pos": 162
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Metric",
            "value": "анизотропии",
            "index": 0,
            "start_pos": 27
          },
          "predicate": "isUsedIn",
          "term2": {
            "class": "Model",
            "value": "декодеров",
            "index": 0,
            "start_pos": 93
          }
        }
      ]
    },
    {
      "text": "Мы проследили за тем, как меняется анизотропия от чекпоинта к чекпоинту по мере обучения моделей (мы взяли все модели с промежуточными весами из того, что было опубликовано на тот момент). Оказалось, что все модели класса трансформер-декодер постепенно сходятся к одной и той же форме пространства и одному и тому же куполообразному профилю анизотропии.",
      "terms": [
        {
          "index": 0,
          "class": "Metric",
          "value": "анизотропия",
          "start_pos": 35
        },
        {
          "index": 0,
          "class": "Model",
          "value": "моделей",
          "start_pos": 89
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модели",
          "start_pos": 111
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модели",
          "start_pos": 208
        },
        {
          "index": 0,
          "class": "Model",
          "value": "трансформер-декодер",
          "start_pos": 222
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "анизотропии",
          "start_pos": 341
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Metric",
            "value": "анизотропия",
            "index": 0,
            "start_pos": 35
          },
          "predicate": "isUsedIn",
          "term2": {
            "class": "Model",
            "value": "моделей",
            "index": 0,
            "start_pos": 89
          }
        }
      ]
    },
    {
      "text": "Следующий наш «микроскоп» для наблюдения за активациями — внутренняя размерность. Это довольно красивое математическое понятие, описывающее «сложность» фигуры (многообразия или манифолда), на котором располагаются точки в многомерном пространстве.",
      "terms": [
        {
          "index": 0,
          "class": "Object",
          "value": "активациями",
          "start_pos": 44
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "внутренняя размерность",
          "start_pos": 58
        }
      ],
      "relations": []
    },
    {
      "text": "Чтобы было понятнее, рассмотрим трёхмерную фигуру в виде ленты, свёрнутой в спираль (см. картинку ниже). Если мы приблизимся к какому-либо её участку, то обнаружим, что в малой окрестности точки будто бы лежат на плоскости. Следовательно, локальная внутренняя размерность тут равна двум.",
      "terms": [
        {
          "index": 0,
          "class": "Metric",
          "value": "внутренняя размерность",
          "start_pos": 249
        }
      ],
      "relations": []
    },
    {
      "text": "Самое главное, что внутреннюю размерность довольно легко оценить, так как она сильно связана со скоростью роста «объёма» многомерного шара (количества точек данных, попадающих внутрь шара) по мере увеличения радиуса. Измерение зависимости количества точек от радиуса позволяет определить внутреннюю размерность в локальной области облака данных.",
      "terms": [
        {
          "index": 0,
          "class": "Metric",
          "value": "внутреннюю размерность",
          "start_pos": 19
        }
      ],
      "relations": []
    },
    {
      "text": "Итак, что же мы обнаружили? Во-первых, размерность довольно низкая, но это не новость, т.к. это было обнаружено и до нас. Во-вторых, — и это гораздо интереснее — эта размерность изменяется одинаково для всех моделей по мере обучения! Этот процесс состоит из двух фаз — сначала рост, а затем падение (см. график).",
      "terms": [
        {
          "index": 0,
          "class": "Metric",
          "value": "размерность",
          "start_pos": 39
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "размерность",
          "start_pos": 166
        },
        {
          "index": 0,
          "class": "Model",
          "value": "моделей",
          "start_pos": 208
        },
        {
          "index": 0,
          "class": "Task",
          "value": "обучения",
          "start_pos": 224
        }
      ],
      "relations": []
    },
    {
      "text": "Похоже, что первая часть обучения переводит фичи в более высокие измерения, чтобы «запомнить» как можно больше информации, а во второй фазе — фичи начинают сжиматься, позволяя выявлять больше закономерностей, усиливая обобщающие способности модели.",
      "terms": [
        {
          "index": 0,
          "class": "Task",
          "value": "обучения",
          "start_pos": 25
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модели",
          "start_pos": 241
        }
      ],
      "relations": []
    },
    {
      "text": "Ещё раз — у всех LLM во время обучения присутствуют две фазы: инфляция эмбеддингов и их последующая компрессия.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "LLM",
          "start_pos": 17
        },
        {
          "index": 0,
          "class": "Task",
          "value": "обучения",
          "start_pos": 30
        },
        {
          "index": 0,
          "class": "Object",
          "value": "эмбеддингов",
          "start_pos": 71
        },
        {
          "index": 0,
          "class": "Method",
          "value": "инфляция",
          "start_pos": 62
        },
        {
          "index": 0,
          "class": "Method",
          "value": "компрессия",
          "start_pos": 100
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "инфляция",
            "index": 0,
            "start_pos": 62
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "обучения",
            "index": 0,
            "start_pos": 30
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "компрессия",
            "index": 0,
            "start_pos": 100
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "обучения",
            "index": 0,
            "start_pos": 30
          }
        }
      ]
    },
    {
      "text": "Мы верим, что, вооружившись новым знанием, мы сможем улучшить процесс обучения языковых моделей (и не только), сделать его эффективнее, а сами трансформеры — быстрее и компактнее. Ведь если эмбеддинги проходят стадию компрессии и вообще стремятся расположиться вдоль одной линии, то почему бы просто не повыкидывать неиспользуемые измерения? Или помочь модели с более быстрым преодолением первой фазы.",
      "terms": [
        {
          "index": 0,
          "class": "Activity",
          "value": "обучения",
          "start_pos": 70
        },
        {
          "index": 0,
          "class": "Model",
          "value": "языковых моделей",
          "start_pos": 79
        },
        {
          "index": 0,
          "class": "Model",
          "value": "трансформеры",
          "start_pos": 143
        },
        {
          "index": 0,
          "class": "Object",
          "value": "эмбеддинги",
          "start_pos": 190
        },
        {
          "index": 0,
          "class": "Method",
          "value": "компрессии",
          "start_pos": 217
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модели",
          "start_pos": 353
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "компрессии",
            "index": 0,
            "start_pos": 217
          },
          "predicate": "isAppliedTo",
          "term2": {
            "class": "Object",
            "value": "эмбеддинги",
            "index": 0,
            "start_pos": 190
          }
        }
      ]
    },
    {
      "text": "Также мы обнаружили, что незадолго до взрывов лосса во время обучения (больная тема всех, кто учит LLM) внутренняя размерность сильно подрастает. Возможно, у нас получится предсказывать взрывы лосса и не тратить вычислительные ресурсы впустую, или вообще победить эти нестабильности, поняв их природу.",
      "terms": [
        {
          "index": 0,
          "class": "Activity",
          "value": "учит"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "LLM"
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "внутренняя размерность"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Metric",
            "value": "внутренняя размерность",
            "index": 0
          },
          "predicate": "isUsedFor",
          "term2": {
            "class": "Model",
            "value": "LLM",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "компрессии",
            "start_pos": 217
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "трансформеры",
            "start_pos": 143
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "компрессии",
            "start_pos": 217
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "языковых моделей",
            "start_pos": 79
          }
        }
      ]
    },
    {
      "text": "Хотя кого я обманываю, всё это нужно только ради удовлетворения своего любопытства! ",
      "terms": [],
      "relations": []
    },
    {
      "text": "Подписывайтесь на каналы авторов в телеграме  AbstractDL, CompleteAI, Dendi Math&AI, Ivan Oseledets. В работе также принимали участие коллеги из AIRI, Сбера, Сколтеха, МГУ, ВШЭ и Самарского университета.",
      "terms": [
        {
          "index": 0,
          "class": "Organization",
          "value": "AIRI"
        },
        {
          "index": 0,
          "class": "Organization",
          "value": "Сбера"
        },
        {
          "index": 0,
          "class": "Organization",
          "value": "Сколтеха"
        },
        {
          "index": 0,
          "class": "Organization",
          "value": "МГУ"
        },
        {
          "index": 0,
          "class": "Organization",
          "value": "ВШЭ"
        },
        {
          "index": 0,
          "class": "Organization",
          "value": "Самарского университета"
        }
      ],
      "relations": []
    },
    {
      "text": "Привет, Хабр! Если вы интересуетесь NLP или просто современными DL моделями, то приглашаю вас узнать, как можно, имея всего лишь одну A100, около 30 гигабайтов текста и несколько дней обучения, решить проблему ограниченного окна контекста для русскоязычных трансформеров. А ещё сделаем несколько оптимизаций и добьёмся почти лучших метрик в бенчмарке encodechka.",
      "terms": [
        {
          "index": 0,
          "class": "InfoResource",
          "value": "Хабр"
        },
        {
          "index": 0,
          "class": "Science",
          "value": "NLP"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "DL моделями"
        },
        {
          "index": 0,
          "class": "Lang",
          "value": "русскоязычных"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "трансформеров"
        },
        {
          "index": 0,
          "class": "Dataset",
          "value": "encodechka"
        }
      ],
      "relations": []
    },
    {
      "text": "Получившиеся русскоязычные модели доступны в открытом доступе на HuggingFace 🤗:",
      "terms": [
        {
          "index": 0,
          "class": "Lang",
          "value": "русскоязычные"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модели"
        },
        {
          "index": 0,
          "class": "InfoResource",
          "value": "HuggingFace"
        }
      ],
      "relations": []
    },
    {
      "text": "Методы позиционного кодирования сильно продвинулись за последние пару лет. Сейчас практически каждая появляющаяся LLM имеет контекст в 8k токенов и больше, к тому же появилось большое количество методов для расширения контекста, некоторые из них даже позволяют не дообучать предобученную модель. Только вот, к сожалению, все эти улучшения касаются в основном causal моделей, т. е. декодеров. Кажется, даже для англоязычных моделей, исследование SOTA методов позиционного кодирования в энкодерах застыло где-то на этапе DeBERTa, с её disentangled attention.",
      "terms": [
        {
          "index": 0,
          "class": "Method",
          "value": "Методы позиционного кодирования"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "LLM"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "расширения контекста"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "декодеров"
        },
        {
          "index": 0,
          "class": "Lang",
          "value": "англоязычных"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "моделей"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "моделей"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "методов позиционного кодирования"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "энкодерах"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "DeBERTa"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "disentangled attention"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Model",
            "value": "DeBERTa",
            "index": 0
          },
          "predicate": "isExampleOf",
          "term2": {
            "class": "Model",
            "value": "энкодерах",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "DeBERTa",
            "index": 0
          },
          "predicate": "Language",
          "term2": {
            "class": "Lang",
            "value": "англоязычных",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "disentangled attention",
            "index": 0
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "DeBERTa",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Однако ещё в середине 2021 года выходила модель RoFormer, которая использовала Rotary Position Embedding, именно этот метод сейчас применяется во всех LLM (Mistral, Llama, GPT-NeoX и тд). К сожалению, RoFormer не был адаптирован для русского языка, но потенциал, особенно сейчас, он имеет значительный. Большинство моделей на русском имеют стандартный размер контекста в 512 токенов, исключениями являются cointegrated/rubert-tiny2 от Дэвида Дале и kazzand/ru-longformer-tiny-16384 от MTS. Поэтому целью данного проекта и стала адаптация обычных моделей для возможности работы с RoPE, увеличение контекстного окна и улучшение их производительности. Как бонус — получившиеся в результате модели, по качеству превосходят существующие модели длинного контекста (и не только) для русского языка.",
      "terms": [
        {
          "index": 0,
          "class": "Date",
          "value": "2021"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "RoFormer"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "Rotary Position Embedding"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "LLM"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "Mistral"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "Llama"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "GPT-NeoX"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "RoFormer"
        },
        {
          "index": 0,
          "class": "Lang",
          "value": "русского языка"
        },
        {
          "index": 0,
          "class": "Lang",
          "value": "русском"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "cointegrated/rubert-tiny2"
        },
        {
          "index": 0,
          "class": "Person",
          "value": "Дэвида Дале"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "kazzand/ru-longformer-tiny-16384"
        },
        {
          "index": 0,
          "class": "Organization",
          "value": "MTS"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "адаптация"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "моделей"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "увеличение контекстного окна"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "улучшение их производительности"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "RoPE"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модели длинного контекста"
        },
        {
          "index": 0,
          "class": "Lang",
          "value": "русского языка"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Model",
            "value": "cointegrated/rubert-tiny2",
            "index": 0
          },
          "predicate": "Language",
          "term2": {
            "class": "Lang",
            "value": "русского языка",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "kazzand/ru-longformer-tiny-16384",
            "index": 0
          },
          "predicate": "Language",
          "term2": {
            "class": "Lang",
            "value": "русского языка",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Date",
            "value": "2021",
            "index": 0
          },
          "predicate": "isDateOf",
          "term2": {
            "class": "Model",
            "value": "RoFormer",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "Rotary Position Embedding",
            "index": 0
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "RoFormer",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "Rotary Position Embedding",
            "index": 0
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "Mistral",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "Rotary Position Embedding",
            "index": 0
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "Llama",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "Rotary Position Embedding",
            "index": 0
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "GPT-NeoX",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "GPT-NeoX",
            "index": 0
          },
          "predicate": "isExampleOf",
          "term2": {
            "class": "Model",
            "value": "LLM",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "Llama",
            "index": 0
          },
          "predicate": "isExampleOf",
          "term2": {
            "class": "Model",
            "value": "LLM",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "Mistral",
            "index": 0
          },
          "predicate": "isExampleOf",
          "term2": {
            "class": "Model",
            "value": "LLM",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Lang",
            "value": "русского языка",
            "index": 0
          },
          "predicate": "Language",
          "term2": {
            "class": "Model",
            "value": "RoFormer",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "cointegrated/rubert-tiny2",
            "index": 0
          },
          "predicate": "hasAuthor",
          "term2": {
            "class": "Person",
            "value": "Дэвида Дале",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "kazzand/ru-longformer-tiny-16384",
            "index": 0
          },
          "predicate": "hasAuthor",
          "term2": {
            "class": "Organization",
            "value": "MTS",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "RoPE",
            "index": 0
          },
          "predicate": "isAlternativeNameFor",
          "term2": {
            "class": "Method",
            "value": "Rotary Position Embedding",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Как же адаптировать модели с выученным позиционным кодированием для работы с RoPE? Очевидно, что нам нужно просто заменить одно на другое в архитектуре модели. Для этого мы можем удалить nn.Embedding слой, отвечающий за старое позиционное кодирование, а далее модифицировать SelfAttention блок в трансформере так, чтобы position_ids передавались на каждый слой и там же применялся RoPE к query и key проекциям, ведь именно так он и устроен.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "модели"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "адаптировать"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модели"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "трансформере"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "модифицировать"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "RoPE"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "модифицировать",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "адаптировать",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Применив такое преобразование, мы сломаем модель почти полностью. Но это не мешает нам восстановить её способности, так как большинство весов мы не изменяли. Для этого, как простой вариант, нам потребуется просто предобучить модель в режиме RoBERTы — то есть учить MLM задачу, применив поверх модели соответствующую голову.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "RoBERTы"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модель"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "предобучить"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модель"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "MLM задачу"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модели"
        }
      ],
      "relations": []
    },
    {
      "text": "Но также, мы хотим именно восстановить способности модели, а не просто обучить её на новом датасете — это значит, дистиллировать оригинальную модель-учитель, в нашего RoPE-ученика. Мне кажется, сейчас уместнее это называть клонированием, так как размеры и архитектура остаются одинаковыми. Для того чтобы это осуществить, достаточно к MLM-лоссу добавить дистилляционный лосс, который может быть, по большому счёту, любым, кроме KL-дивергенции, так как MLM-головы никто не кладёт с весами модели, и распределения учителя на токены у нас не будет. Я решил использовать 2 лосса, по отдельности: Cosine Similarity усредненый по токенам, для каждого слоя, и InfoNCE, в симметричном режиме, только для последнего слоя, именно он используется в CLIP для сближения модальностей текста и картинок. В обоих лоссах также подбиралась и своя температура для увеличения важности близости.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "модели"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "восстановить способности модели"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "обучить"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "оригинальную модель-учитель"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "RoPE-ученика"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "клонированием"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "MLM-лоссу"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "дистилляционный лосс"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "Cosine Similarity"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "InfoNCE"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "CLIP"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "сближения модальностей"
        },
        {
          "index": 0,
          "class": "Object",
          "value": "текста"
        },
        {
          "index": 0,
          "class": "Object",
          "value": "картинок"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "клонированием",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "восстановить способности модели",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "MLM-лоссу",
            "index": 0
          },
          "predicate": "isPartOf",
          "term2": {
            "class": "Method",
            "value": "клонированием",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "дистилляционный лосс",
            "index": 0
          },
          "predicate": "isPartOf",
          "term2": {
            "class": "Method",
            "value": "клонированием",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "InfoNCE",
            "index": 0
          },
          "predicate": "isPartOf",
          "term2": {
            "class": "клонированием",
            "value": "CLIP",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "InfoNCE",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "сближения модальностей",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "В качестве данных я взял ~32 гигабайта из первых 10 русскоязычных фолдов датасета CulturaX. В семплах это составило около 5 миллионов. Данный датасет хорош тем, что довольно хорошо подготовлен и отфильтрован, в русских семплах действительно мало других языков. А ещё он имеет хорошее распределение длин, тут найдутся примеры для обучения модели с практически любым контекстным окном до 16к.",
      "terms": [
        {
          "index": 0,
          "class": "Dataset",
          "value": "CulturaX"
        },
        {
          "index": 0,
          "class": "Lang",
          "value": "русскоязычных"
        },
        {
          "index": 0,
          "class": "Object",
          "value": "семплах"
        },
        {
          "index": 0,
          "class": "Lang",
          "value": "русских"
        },
        {
          "index": 0,
          "class": "Object",
          "value": "семплах"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Dataset",
            "value": "CulturaX",
            "index": 0
          },
          "predicate": "Language",
          "term2": {
            "class": "Lang",
            "value": "русскоязычных",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Эта первая модель проекта является клоном ai-forever/ruBert-base. Для её клонирования использовался послойный cosine similarity лосс с температурой 0.5 между скрытыми представлениями ученика и учителя.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "ai-forever/ruBert-base"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "клонирования"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "послойный cosine similarity лосс"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "послойный cosine similarity лосс",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "клонирования",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Эта же модель является клоном hivaze/ru-e5-base, а та в свою очередь русифицированной версией intfloat/multilingual-e5-base, полученой с помощью удаления большинства лишних токенов из других языков, по методу Дэвида Дале, снижая таким образом размер словаря с 250k до 69k, что существенно облегчило её тренировку. Кстати, модель, в связи с изначальной мультиязычностью, сохраняет свойства и на английском языке, но он не являлся основным.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "hivaze/ru-e5-base"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "intfloat/multilingual-e5-base"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "методу Дэвида Дале"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "тренировку"
        },
        {
          "index": 0,
          "class": "Lang",
          "value": "английском языке"
        },
        {
          "index": 0,
          "class": "Lang",
          "value": "мультиязычностью"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модель"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Model",
            "value": "hivaze/ru-e5-base",
            "index": 0
          },
          "predicate": "isModificationOf",
          "term2": {
            "class": "Model",
            "value": "intfloat/multilingual-e5-base",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "методу Дэвида Дале",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "тренировку",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "модель",
            "index": 0
          },
          "predicate": "Language",
          "term2": {
            "class": "Lang",
            "value": "мультиязычностью",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "модель",
            "index": 0
          },
          "predicate": "Language",
          "term2": {
            "class": "Lang",
            "value": "английском языке",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Из-за своей особенности представления похожести текстов, e5 клонировалась с симметричным InfoNCE лоссом (как в CLIP) поверх последнего слоя и температурой 0.1. Выбор этого лосса обоснован тем, что модель должна гораздо лучше понимать разницу между разными по смыслу текстами, а не просто сближать косинусную близость со своим оригиналом. Кстати, этот же лосс использовался авторами и при обучении оригинальной e5.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "e5"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "симметричным InfoNCE лоссом"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "CLIP"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модель"
        },
        {
          "index": 0,
          "class": "Object",
          "value": "текстами"
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "косинусную близость"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "обучении"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "оригинальной e5"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "симметричным InfoNCE лоссом",
            "index": 0
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "e5",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "симметричным InfoNCE лоссом",
            "index": 0
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "оригинальной e5",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Я не буду подробно описывать как работает RoPE, а также его формулы и почему работают методы его интерполяции/экстраполяции. Для этого я предлагаю изучить статью Scaling Laws of RoPE-based Extrapolation или пост на хабре О методах позиционного кодирования в Transformer, а ещё оригинальную статью Roformer.",
      "terms": [
        {
          "index": 0,
          "class": "Method",
          "value": "RoPE"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "методах позиционного кодирования"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "Transformer"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "Roformer"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "методах позиционного кодирования",
            "index": 0
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "Transformer",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Итак, использовать клонирование оригинальной модели, очевидно, можно только на длинах до 512 токенов. Но нам нужно больше, поэтому вторым этапом в пайплайне подготовки моделей стало расширение контекста — дообучение моделей только с MLM лоссом и только на текстах с длинами 512-2048 токенов.",
      "terms": [
        {
          "index": 0,
          "class": "Method",
          "value": "клонирование оригинальной модели"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "расширение контекста"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "дообучение"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "MLM лоссом"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "MLM лоссом",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "дообучение",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "В ходе экспериментов, я пришёл к выводу, что тут лучше не использовать какие-либо техники интерполяции или экстраполяции RoPE эмбедингов при обучении. Дело в том, что модель и так довольно легко сходилась на увеличенном контексте к качеству, как на 512 токенах, ещё где-то на ~200к семплах. Техники, где мы как-то влияем на эмбединги, конечно, помогают, но они нужны скорее для значительного увеличения контекста, например с 4к до 16к, так как там потребовалось бы больше времени и данных, если ничего не использовать.",
      "terms": [
        {
          "index": 0,
          "class": "Activity",
          "value": "экспериментов"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "интерполяция"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "экстраполяция"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "обучении"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модель"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "интерполяция",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "обучении",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "экстраполяция",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "обучении",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Тут вы могли заметить: если мы будем дообучать модель на длинный контекст без клонирующего лосса, то как мы убедимся, что наша модель будет всё ещё похожа на оригинальную? На самом деле, очень просто — нам достаточно обучать только attention блоки, не трогая последующий MLP блок. RoPE затрагивает именно query и key, соответственно, только их проекции нам и нужно дообучать, MLP будет мапить получившиеся вектора примерно в то же место в пространстве, что и раньше, а значит характеристики вроде косинусной близости между скрытыми представлениями с оригиналом, почти не изменятся.",
      "terms": [
        {
          "index": 0,
          "class": "Task",
          "value": "дообучать"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "клонирующего лосса"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "RoPE"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "дообучать"
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "косинусной близости"
        },
        {
          "index": 0,
          "class": "Object",
          "value": "query"
        },
        {
          "index": 0,
          "class": "Object",
          "value": "key"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "MLP"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "RoPE",
            "index": 0
          },
          "predicate": "isAppliedTo",
          "term2": {
            "class": "Object",
            "value": "query",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "RoPE",
            "index": 0
          },
          "predicate": "isAppliedTo",
          "term2": {
            "class": "Object",
            "value": "key",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Для дообучения мы не используем никакие трюки со скейлингом RoPE. Но вот для инференса вполне можем, есть как минимум метод Dynamic-NTK, который позволяет применять его без дообучения. Это означает, что потенциально, модели могут использоваться и на более длинном контексте чем 2k и не очень сильно терять в качестве.",
      "terms": [
        {
          "index": 0,
          "class": "Method",
          "value": "скейлингом RoPE"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "дообучения"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "Dynamic-NTK"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "инференса"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "Dynamic-NTK",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "инференса",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Ниже приведена таблица с различными оценками модели ruRoPEBert-e5-base-2k, которая была дополнительно дообучена на 512-2048 токенах, на разных длинах контекста с Dynamic-NTK скейлингом и без него.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "ruRoPEBert-e5-base-2k"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "Dynamic-NTK скейлингом"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "дообучена"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "Dynamic-NTK скейлингом",
            "index": 0
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "ruRoPEBert-e5-base-2k",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "Dynamic-NTK скейлингом",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "дообучена",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Как видно, скейлинг помогает адаптироваться на длины больше, чем модель видела при обучении, но тем не менее, качество всё ещё ощутимо деградирует, критичность такой деградации будет зависеть от конкретных задач. Мне кажется, для улучшения результатов, тут стоит попробовать другие мощные методы, вроде xPos или Alibi, подробнее о них можно узнать в статье A Length-Extrapolatable Transformer.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "модель"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "скейлинг"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "xPos"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "Alibi"
        }
      ],
      "relations": []
    },
    {
      "text": "Так как ручное вычисление внимания (eager) является абсолютно неоптимизированным, квадратичным по вычислительной сложности и слабым местом по памяти, его нельзя просто использовать как он есть, если мы хотим увеличить контекст модели в несколько раз.",
      "terms": [
        {
          "index": 0,
          "class": "Method",
          "value": "вычисление внимания"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "увеличить контекст модели"
        }
      ],
      "relations": []
    },
    {
      "text": "Существует много вариантов на что можно заменить механизм вычисления внимания с ручного, например, модули из xformers или FlashAttention v1/v2 и тд. В качестве простого варианта, я обратил внимание на нативный метод, который активно продвигает PyTorch — torch.nn.functional.scaled_dot_product_attention (sdpa). Этот метод поддерживает сразу 3 способа оптимизации аттеншена, которые можно включать и отключать по собственному желанию. Среди них: Memory-Efficient Attention, FlashAttention, Fused C++ имплементация от PyTorch.",
      "terms": [
        {
          "index": 0,
          "class": "Library",
          "value": "FlashAttention"
        },
        {
          "index": 0,
          "class": "Library",
          "value": "xformers"
        },
        {
          "index": 0,
          "class": "Library",
          "value": "PyTorch"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "Memory-Efficient Attention"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "FlashAttention"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "Fused C++"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "нативный метод"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "torch.nn.functional.scaled_dot_product_attention"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "sdpa"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "механизм вычисления внимания"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "torch.nn.functional.scaled_dot_product_attention",
            "index": 0
          },
          "predicate": "isAlternativeNameFor",
          "term2": {
            "class": "Method",
            "value": "sdpa",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "torch.nn.functional.scaled_dot_product_attention",
            "index": 0
          },
          "predicate": "isUsedIn",
          "term2": {
            "class": "Library",
            "value": "PyTorch",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "Memory-Efficient Attention",
            "index": 0
          },
          "predicate": "isPartOf",
          "term2": {
            "class": "Method",
            "value": "нативный метод",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "FlashAttention",
            "index": 0
          },
          "predicate": "isPartOf",
          "term2": {
            "class": "Method",
            "value": "нативный метод",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "Fused C++",
            "index": 0
          },
          "predicate": "isPartOf",
          "term2": {
            "class": "Method",
            "value": "нативный метод",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "В недавнем релизе 2.2.0 авторы PyTorch добавили поддержку FlashAttention второй версии в эту функцию, тем самым она стала, наверное, самым удобным способом добавления оптимизации вычисления внимания в модель, так как использование нативного Flash не очень удобное.",
      "terms": [
        {
          "index": 0,
          "class": "Library",
          "value": "PyTorch"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "FlashAttention"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "оптимизации вычисления внимания"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "нативного Flash"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "FlashAttention",
            "index": 0
          },
          "predicate": "isUsedIn",
          "term2": {
            "class": "Library",
            "value": "PyTorch",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "FlashAttention",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "оптимизации вычисления внимания",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "нативного Flash",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "оптимизации вычисления внимания",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "В таблице ниже я привожу бенчмарки инференса, произведённые с помощью Pytorch Profiler 12-ти слойного ruRoPEBert в конфигурации base. Замеры производились на ноутбучной RTX 3070 Ti с процессором Intel Core i7 12700H. Размер батча во всех тестах = 1.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "Pytorch Profiler"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "ruRoPEBert"
        }
      ],
      "relations": []
    },
    {
      "text": "Эффективность использования памяти при выборе sdpa вырастает в несколько раз, также модель получает ускорение относительно eager, пропорциональное увеличению размера контекста. При обучении также получается достичь ускорения в 2-3 раза и кратно сэкономить память.",
      "terms": [
        {
          "index": 0,
          "class": "Method",
          "value": "sdpa"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "обучении"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "sdpa",
            "index": 0
          },
          "predicate": "solves",
          "term2": {
            "class": "Task",
            "value": "обучении",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Использовать модель, выбирая соответствующий способ вычисления внимания можно следующим образом:",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "модель"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "вычисления внимания"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Model",
            "value": "модель",
            "index": 0
          },
          "predicate": "isUsedForSolving",
          "term2": {
            "class": "Task",
            "value": "вычисления внимания",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "За основу бенчмарка качества был выбран encodechka от Дэвида Дале, так как он обладает единым и простым интерфейсом запуска тестов, в нём есть немало современных моделей, а главное, что оценка производится исключительно на русскоязычных датасетах. К тому же он даёт возможность сравнить модели по их способности в семантическое представление предложений, что является важной целью данного проекта.",
      "terms": [
        {
          "index": 0,
          "class": "dataset",
          "value": "encodechka"
        },
        {
          "index": 0,
          "class": "Person",
          "value": "Дэвида Дале"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "современных моделей"
        },
        {
          "index": 0,
          "class": "Dataset",
          "value": "русскоязычных датасетах"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "семантическое представление предложений"
        }
      ],
      "relations": []
    },
    {
      "text": "Как видно, ruRoPEBert-e5-base-2k является лучшей моделью на всем бенчмарке encodechka по скору S+W (sentence + word embeddings). Однако несколько отстаёт от моделей e5 по STS, возможно, это связано с тем, что во время обучения следовало взять температуру ниже, чем 0.1 или использовать дополнительные датасеты. Тем не менее во всех остальных тестах ruRoPEBert оказалась лучше, чем оригинал e5-base, с этим и связано общее первенство. Также обе модели обходят text-embedding-ada-002 от OpenAI.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "ruRoPEBert-e5-base-2k"
        },
        {
          "index": 0,
          "class": "Dataset",
          "value": "encodechka"
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "S+W"
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "sentence + word embeddings"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "модели e5"
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "STS"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "взять температуру ниже, чем 0.1"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "использовать дополнительные датасеты"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "ruRoPEBert"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "оригинал e5-base"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "text-embedding-ada-002"
        },
        {
          "index": 0,
          "class": "Organization",
          "value": "OpenAI"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Model",
            "value": "text-embedding-ada-002",
            "index": 0
          },
          "predicate": "hasAuthor",
          "term2": {
            "class": "Organization",
            "value": "OpenAI",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Metric",
            "value": "STS",
            "index": 0
          },
          "predicate": "isUsedFor",
          "term2": {
            "class": "Model",
            "value": "оригинал e5-base",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Metric",
            "value": "STS",
            "index": 0
          },
          "predicate": "isUsedFor",
          "term2": {
            "class": "Model",
            "value": "модели e5",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Получившиеся RoPEBert модели во всех тестах превосходят свой оригинал - ai-forever/ruBert-base, и даже более крупную ai-forever/ruRoberta-large. Судя по скорам NE1 и NE2, classic модели обладают большим потенциалом в решении NER задач, чем sentence клоны e5.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "RoPEBert модели"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "ai-forever/ruBert-base"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "ai-forever/ruRoberta-large"
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "NE1"
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "NE2"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "classic модели"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "NER"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "sentence клоны e5"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Model",
            "value": "RoPEBert",
            "index": 0
          },
          "predicate": "isModificationOf",
          "term2": {
            "class": "Model",
            "value": "ai-forever/ruBert-base",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "RoPEBert",
            "index": 0
          },
          "predicate": "isUsedForSolving",
          "term2": {
            "class": "Task",
            "value": "NER",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "ai-forever/ruBert-base",
            "index": 0
          },
          "predicate": "isUsedForSolving",
          "term2": {
            "class": "Task",
            "value": "NER",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "ai-forever/ruRoberta-large",
            "index": 0
          },
          "predicate": "isUsedForSolving",
          "term2": {
            "class": "Task",
            "value": "NER",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Metric",
            "value": "NE1",
            "index": 0
          },
          "predicate": "isUsedFor",
          "term2": {
            "class": "Model",
            "value": "classic модели",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Metric",
            "value": "NE1",
            "index": 0
          },
          "predicate": "isUsedFor",
          "term2": {
            "class": "Model",
            "value": "sentence клоны e5",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Metric",
            "value": "NE2",
            "index": 0
          },
          "predicate": "isUsedFor",
          "term2": {
            "class": "Model",
            "value": "classic модели",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Metric",
            "value": "NE2",
            "index": 0
          },
          "predicate": "isUsedFor",
          "term2": {
            "class": "Model",
            "value": "sentence клоны e5",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Не всё получилось, как было задумано, например, в бенчмарке STS клонированная e5 модель проигрывает оригиналу, а также модели, на мой взгляд, не очень хорошо обобщаются на длины большие, чем видели при обучении.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "клонированная e5 модель"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "оригиналу"
        },
        {
          "index": 0,
          "class": "Metric",
          "value": "STS"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Metric",
            "value": "STS",
            "index": 0
          },
          "predicate": "isUsedFor",
          "term2": {
            "class": "Model",
            "value": "клонированная e5 модель",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "клонированная e5 модель",
            "index": 0
          },
          "predicate": "isModificationOf",
          "term2": {
            "class": "Model",
            "value": "оригиналу",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Эти проблемы можно решить усовершенствовав пайплайн тренировки или некоторые гиперпараметры. Также можно заменить механизм экстраполяции, например, на xPos, Alibi или совсем недавний LongRoPE, а можно сделать спарсификацию аттеншена, как это предлагается в статье Blockwise Self-Attention for Long Document Understanding, ещё от 2019 года. Кроме того, в самой статье RoFormer упоминается, что авторы попробовали объединить RoPE с архитектурой Performer и работать посимвольно и линейно по сложности.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "e5"
        },
        {
          "index": 0,
          "class": "Task",
          "value": "STS"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "xPos"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "Alibi"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "LongRoPE"
        },
        {
          "index": 0,
          "class": "Date",
          "value": "2019"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "Performer"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "RoFormer"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "механизм экстраполяции"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "усовершенствовав пайплайн тренировки или некоторые гиперпараметры"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "спарсификацию аттеншена"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "объединить RoPE с архитектурой Performer"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "работать посимвольно и линейно по сложности"
        }
      ],
      "relations": []
    },
    {
      "text": "Удивляет, что существует огромное количество архитектур (от старого Reformer до новейших Biderectional SSM), которые могли бы эффективно использоваться уже долгое время в энкодерах, но стандартный вариант с квадратичным вниманием — всё ещё остаётся единственным путём, которым идёт большинство SOTA моделей, особенно на русском языке. В конечном итоге этот проект делает совсем небольшое усилие в сторону современных подходов в NLP и доказывает их относительную простоту реализации.",
      "terms": [
        {
          "index": 0,
          "class": "Model",
          "value": "архитектур"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "Reformer"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "Biderectional SSM"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "стандартный вариант с квадратичным вниманием"
        },
        {
          "index": 0,
          "class": "Lang",
          "value": "русском языке"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "большинство SOTA моделей"
        },
        {
          "index": 0,
          "class": "Science",
          "value": "NLP"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Model",
            "value": "Reformer",
            "index": 0
          },
          "predicate": "isExampleOf",
          "term2": {
            "class": "Model",
            "value": "архитектур",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "Biderectional SSM",
            "index": 0
          },
          "predicate": "isExampleOf",
          "term2": {
            "class": "Model",
            "value": "архитектур",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Method",
            "value": "стандартный вариант с квадратичным вниманием",
            "index": 0
          },
          "predicate": "isExampleOf",
          "term2": {
            "class": "Model",
            "value": "архитектур",
            "index": 0
          }
        }
      ]
    },
    {
      "text": "Веса всех моделей, их код и инструкции по запуску доступны в нашем аккаунте на HuggingFace (ссылки в начале статьи). В дальнейшем, у нас есть планы по выпуску large версий моделей и дотренировке с контекстом до 8k токенов, а также продолжение экспериментов с оптимизациями.",
      "terms": [
        {
          "index": 0,
          "class": "InfoResource",
          "value": "HuggingFace"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "large версий"
        },
        {
          "index": 0,
          "class": "Model",
          "value": "моделей"
        },
        {
          "index": 0,
          "class": "Method",
          "value": "дотренировке"
        },
        {
          "index": 0,
          "class": "Activity",
          "value": "экспериментов с оптимизациями"
        }
      ],
      "relations": [
        {
          "term1": {
            "class": "Method",
            "value": "дотренировке",
            "index": 0
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "large версий",
            "index": 0
          }
        },
        {
          "term1": {
            "class": "Model",
            "value": "large версий",
            "index": 0
          },
          "predicate": "isUsedForTraining",
          "term2": {
            "class": "Model",
            "value": "моделей",
            "index": 0
          }
        }
      ]
    }
  ]
}



