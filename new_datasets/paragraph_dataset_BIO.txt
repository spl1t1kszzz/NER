В	O
этой	O
статье	O
я	O
расскажу	O
все	O
,	O
что	O
вам	O
нужно	O
знать	O
про	O
ALBERT	B-Model
,	O
RoBERTa	B-Model
,	O
и	O
DistilBERT	B-Model
.	O
Если	O
непонятно	O
по	O
названию	O
,	O
эти	O
модели	O
—	O
модифицированные	O
версии	O
оригинального	O
современного	O
трансформера	O
BERT	B-Model
.	O
Эти	O
три	O
модели	O
из	O
библиотеки	O
Hugging	B-Library
Face	I-Library
—	O
самые	O
популярные	O
на	O
сегодняшний	O
день	O
.	O
Я	O
рассмотрю	O
их	O
сходства	O
и	O
различия	O
по	O
сравнению	O
друг	O
с	O
другом	O
и	O
добавлю	O
фрагменты	O
кода	O
.	O
Они	O
покажут	O
,	O
как	O
вы	O
можете	O
их	O
использовать	O
.	O
А	O
теперь	O
давайте	O
еще	O
вспомним	O
,	O
что	O
обкатанная	O
на	O
GPT	B-Model
-	I-Model
1	I-Model
технология	B-Method
Трансформеров	I-Method
оказалась	O
на	O
редкость	O
удачной	O
в	O
плане	O
масштабирования	O
:	O
она	O
умеет	O
работать	O
с	O
большими	O
объемами	O
данных	O
и	O
«	O
массивными	O
»	O
моделями	O
(	O
состоящими	O
из	O
огромного	O
числа	O
параметров	O
)	O
гораздо	O
эффективнее	O
своих	O
предшественников	O
.	O
Вы	O
думаете	O
о	O
том	O
же	O
,	O
о	O
чем	O
и	O
я	O
?	O
Ну	O
вот	O
и	O
ученые	O
из	O
OpenAI	B-Organization
в	O
2019	B-Date
году	O
сделали	O
такой	O
же	O
вывод	O
:	O
«	O
Пришло	O
время	O
пилить	O
здоровенные	O
языковые	O
модели	O
!	O
»	O
.	O
В	O
общем	O
,	O
было	O
решено	O
радикально	O
прокачать	O
GPT	B-Model
-	I-Model
2	I-Model
по	O
двум	O
ключевым	O
направлениям	O
:	O
набор	B-Dataset
тренировочных	I-Dataset
данных	I-Dataset
(	O
датасет	B-Dataset
)	O
и	O
размер	O
модели	O
(	O
количество	O
параметров	O
)	O
.	O
На	O
тот	O
момент	O
не	O
было	O
каких	O
‑	O
то	O
специальных	O
,	O
больших	O
и	O
качественных	O
,	O
публичных	O
наборов	O
текстовых	O
данных	O
для	O
тренировки	O
языковых	O
моделей	O
—	O
так	O
что	O
каждой	O
команде	O
специалистов	O
по	O
ИИ	O
приходилось	O
извращаться	O
согласно	O
их	O
собственной	O
степени	O
испорченности	O
.	O
Вот	O
ребята	O
из	O
OpenAI	B-Organization
и	O
решили	O
поступить	O
остроумно	O
:	O
они	O
пошли	O
на	O
самый	O
популярный	O
англоязычный	O
онлайн	O
‑	O
форум	O
Reddit	B-Application
и	O
тупо	O
выкачали	O
все	O
гиперссылки	O
из	O
всех	O
сообщений	O
,	O
имевших	O
более	O
трех	O
лайков	O
(	O
я	O
сейчас	O
не	O
шучу	O
—	O
научный	O
подход	O
,	O
ну	O
!	O
)	O
.	O
У	O
классических	O
transition	B-Application
-	I-Application
based	I-Application
parser	I-Application
возможны	O
три	O
операции	O
,	O
перечисленные	O
выше	O
:	O
стрелка	O
в	O
одну	O
сторону	O
,	O
стрелка	O
в	O
другую	O
сторону	O
и	O
шифт	O
.	O
Есть	O
еще	O
операция	O
Swap	B-Method
,	O
в	O
базовых	O
архитектурах	O
transition	B-Application
-	I-Application
based	I-Application
парсеров	I-Application
она	O
не	O
используется	O
,	O
но	O
в	O
UDPipe	B-Application
включена	O
.	O
Swap	B-Method
возвращает	O
второй	O
элемент	O
стека	O
в	O
буфер	O
,	O
чтобы	O
взять	O
потом	O
из	O
буфера	O
следующий	O
(	O
в	O
случае	O
если	O
они	O
разнесены	O
)	O
.	O
Это	O
помогает	O
пропустить	O
некоторое	O
количество	O
слов	O
и	O
восстановить	O
правильную	O
связь	O
.	O
Для	O
разработчиков	O
среднего	O
и	O
высокого	O
уровня	O
нейросети	B-Model
становятся	O
полноценными	O
помощниками	O
.	O
Есть	O
возможность	O
прокачать	O
и	O
оптимизировать	O
процессы	O
,	O
то	O
есть	O
сильно	O
повысить	O
свою	O
продуктивность	O
.	O
Например	O
,	O
у	O
компании	O
GitHub	B-Organization
есть	O
Copilot	B-Application
,	O
у	O
Amazon	B-Organization
—	O
CodeWhisperer	B-Application
.	O
Это	O
системы	O
,	O
которые	O
поддерживают	O
написание	B-Task
кода	I-Task
и	O
облегчают	O
работу	O
программиста	O
.	O
Davinchi	B-Model
–	O
самая	O
крупная	O
модель	O
OpenAI	B-Organization
(	O
для	O
понимания	O
:	O
при	O
обучении	O
модели	O
использовалось	O
более	O
150	O
миллиардов	O
обучающих	O
блоков	O
,	O
тогда	O
как	O
GPT	B-Model
-	I-Model
3	I-Model
,	I-Model
5	I-Model
и	O
GPT	B-Organization
-	I-Organization
4	I-Organization
обучались	O
всего	O
на	O
6	O
миллиардах	O
блоков	O
данных	O
)	O
.	O
Модель	O
успешно	O
решает	O
задачи	O
,	O
связанные	O
с	O
поиском	B-Task
причинно	I-Task
-	I-Task
следственных	I-Task
связей	I-Task
,	O
и	O
генерирует	B-Task
более	I-Task
качественный	I-Task
текст	I-Task
,	O
когда	O
речь	O
идет	O
о	O
сложных	O
задачах	O
.	O
При	O
этом	O
Davinchi	B-Model
потребляет	O
больше	O
ресурсов	O
и	O
времени	O
.	O
Интерактивное	O
взаимодействие	O
с	O
моделью	B-Model
происходит	O
за	O
счет	O
«	B-Object
Промтов	I-Object
»	I-Object
,	O
то	O
есть	O
запросов	O
,	O
которые	O
мы	O
отсылаем	O
в	O
нейросеть	B-Model
.	O
Поэтому	O
эффективность	O
применения	O
ChatGPT	B-Model
напрямую	O
зависит	O
от	O
такого	O
,	O
каким	O
образом	O
вы	O
конструируете	O
запрос	O
.	O
Модели	B-Model
OpenAI	B-Organization
,	O
доступные	O
к	O
взаимодействию	O
:	O
GPT	B-Model
-	I-Model
3	I-Model
,	I-Model
5	I-Model
и	O
GPT	B-Organization
-	I-Organization
4	I-Organization
–	O
это	O
основные	O
модели	O
,	O
с	O
которыми	O
мы	O
работаем	O
через	O
Telegram	B-Application
или	O
по	O
API	O
в	O
базовых	O
настройках	O
.	O
Это	O
немодифицированные	O
типовые	O
модели	O
,	O
которые	O
максимально	O
оптимизированы	O
для	O
универсальных	O
запросов	O
под	O
обычного	O
пользователя	O
.	O
Они	O
обучены	O
на	O
ограниченном	O
количестве	O
данных	O
,	O
но	O
зато	O
выдают	O
самый	O
быстрый	O
результат	O
из	O
всех	O
моделей	O
.	O
Ada	B-Model
-	O
самая	O
быстрая	O
из	O
всех	O
моделей	O
,	O
способна	O
выполнять	O
такие	O
задачи	O
,	O
как	O
синтаксический	B-Task
анализ	I-Task
текста	I-Task
,	O
исправление	B-Task
адреса	I-Task
и	O
менее	O
сложные	O
задачи	O
классификации	B-Task
.	O
Babage	B-Model
-	O
лучше	O
всего	O
подходит	O
для	O
простых	O
задач	O
классификации	B-Task
и	O
выполняет	O
SEO	B-Task
-	I-Task
анализ	I-Task
текста	I-Task
.	O
Curie	B-Model
-	O
подходит	O
для	O
задач	O
классификация	B-Task
и	O
анализа	B-Task
настроений	I-Task
.	O
Модель	O
также	O
выдает	B-Task
результаты	I-Task
на	I-Task
запросы	I-Task
,	O
отвечает	B-Task
на	I-Task
вопросы	I-Task
и	O
может	O
использоваться	B-Task
в	I-Task
качестве	I-Task
чат	I-Task
-	I-Task
бота	I-Task
общего	O
назначения	O
.	O
Сравнение	O
показывает	O
,	O
что	O
она	O
может	O
выполнять	O
многие	O
задачи	O
Davinci	B-Model
,	O
но	O
за	O
10	O
%	O
стоимости	O
.	O
В	O
целом	O
мы	O
видим	O
похожую	O
картину	O
,	O
хотя	O
точность	B-Metric
предсказаний	O
CatBoost	B-Library
получилась	O
выше	O
,	O
чем	O
нейронной	B-Model
сети	I-Model
.	O
В	O
CatBoost	B-Library
есть	O
функция	B-Method
для	I-Method
отрисовки	I-Method
деревьев	I-Method
,	O
воспользуемся	O
ей	O
,	O
чтобы	O
нарисовать	O
первое	O
дерево	O
:	O
model	O
.	O
plot_tree	O
(	O
0	O
)	O
.	O
Каждое	O
звено	O
дерева	O
содержит	O
разделяющее	O
правило	O
,	O
в	O
котором	O
проверяется	O
,	O
что	O
значение	O
указанного	O
признака	O
больше	O
указанного	O
порога	O
.	O
Каждый	O
лист	O
дерева	O
выдает	O
2	O
числа	O
:	O
само	O
значение	O
и	O
уверенность	O
.	O
Оба	O
эти	O
числа	O
суммируются	O
по	O
всем	O
деревьям	O
.	O
5	O
.	O
2	O
.	O
Регрессия	B-Method
с	O
константной	O
оценкой	O
дисперсии	O
*	O
Вспомним	O
вероятностную	B-Model
модель	I-Model
регрессии	I-Model
,	O
которую	O
мы	O
рассматривали	O
во	O
второй	O
части	O
(	O
2	O
)	O
.	O
В	O
ней	O
является	O
константой	O
.	O
Для	O
любого	O
значения	O
локальные	O
и	O
глобальные	O
минимумы	O
функции	O
потерь	O
одни	O
и	O
те	O
же	O
,	O
и	O
изменение	O
эквивалентно	O
масштабированию	O
функции	O
потерь	O
,	O
что	O
равносильно	O
изменению	O
learning	O
rate	O
.	O
Отсюда	O
получается	O
,	O
что	O
какое	O
бы	O
мы	O
не	O
брали	O
-	O
ничего	O
не	O
изменится	O
(	O
поскольку	O
learning	O
rate	O
выбирается	O
совсем	O
по	O
другим	O
соображениям	O
)	O
.	O
AI	B-Application
Text	I-Application
Classifier	I-Application
запустили	O
31	B-Date
января	I-Date
2023	I-Date
года	I-Date
.	O
Как	O
и	O
остальные	O
инструменты	O
OpenAI	B-Organization
,	O
он	O
бесплатный	O
.	O
Работает	O
так	O
:	O
анализирует	B-Task
образцы	I-Task
текста	I-Task
,	O
сравнивает	O
их	O
с	O
образцами	O
баз	O
данных	O
,	O
написанными	O
как	O
нейросетью	O
,	O
так	O
и	O
людьми	O
,	O
анализирует	O
множество	O
разных	O
стилей	O
написания	O
,	O
даже	O
такие	O
,	O
как	O
новостные	B-Object
статьи	I-Object
,	O
научные	B-Object
документы	I-Object
,	O
посты	B-Object
в	I-Object
соцсетях	I-Object
,	O
рекламные	B-Object
тексты	I-Object
.	O
Хоть	O
и	O
нельзя	O
со	O
стопроцентной	O
уверенностью	O
сказать	O
,	O
что	O
тот	O
или	O
иной	O
текст	O
написан	O
нейросетью	O
,	O
потенциал	O
у	O
AI	B-Application
Text	I-Application
Classifier	I-Application
есть	O
:	O
через	O
него	O
можно	O
обеспечивать	O
высокое	O
качество	O
текстов	O
в	O
сети	O
.	O
В	O
компании	O
OpenAI	B-Organization
утверждают	O
,	O
что	O
новый	O
инструмент	O
пресечёт	B-Task
такие	I-Task
попытки	I-Task
обмана	I-Task
,	O
как	O
запуск	O
автоматических	O
кампаний	O
по	O
дезинформации	O
,	O
мошенничество	B-Task
с	I-Task
задействованием	I-Task
ИИ	I-Task
в	I-Task
научной	I-Task
сфере	I-Task
,	O
имитацию	B-Task
переписки	I-Task
с	I-Task
живым	I-Task
человеком	I-Task
с	I-Task
помощью	I-Task
чат	I-Task
-	I-Task
ботов	I-Task
.	O
Чтобы	O
знать	O
,	O
как	O
работает	O
программа	O
,	O
нужно	O
понимать	O
особенности	O
текстов	O
,	O
которые	O
генерирует	O
ИИ	O
.	O
Такой	O
текст	O
чаще	O
всего	O
несогласованный	O
и	O
несвязный	O
:	O
модели	O
ИИ	O
в	O
большинстве	O
случаев	O
не	O
способны	O
создать	O
текст	O
,	O
основные	O
мысли	O
которого	O
логично	O
и	O
естественно	O
перетекают	O
из	O
одной	O
в	O
другую	O
.	O
GPT	B-Model
внедряют	O
в	O
большое	O
количество	O
проектов	O
.	O
Чаще	O
всего	O
это	O
,	O
конечно	O
,	O
анализ	B-Task
данных	I-Task
,	O
кластеризация	B-Task
,	O
группировка	B-Task
,	O
выявление	B-Task
закономерностей	I-Task
.	O
То	O
есть	O
то	O
,	O
для	O
чего	O
искусственные	B-Model
нейронные	I-Model
сети	O
использовались	O
и	O
раньше	O
.	O
В	O
новых	O
задачах	O
генеративного	O
плана	O
пока	O
что	O
на	O
практике	O
GPT	B-Model
используется	O
не	O
так	O
много	O
,	O
как	O
хотелось	O
бы	O
,	O
но	O
есть	O
замечательные	O
примеры	O
.	O
Первый	O
и	O
самый	O
активно	O
используемый	O
—	O
это	O
AI	B-Application
помощник	I-Application
,	O
встроенный	O
в	O
Notion	B-Application
,	O
—	O
он	O
может	O
как	O
помочь	O
улучшить	B-Task
уже	I-Task
написанный	I-Task
текст	I-Task
,	O
так	O
и	O
сгенерировать	B-Task
план	I-Task
или	O
даже	O
целиком	O
статью	O
под	O
ваш	O
запрос	O
.	O
Главное	O
—	O
не	O
нужно	O
взаимодействовать	O
с	O
каким	O
-	O
то	O
отдельным	O
сервисом	O
,	O
все	O
удобно	O
интегрировано	O
в	O
процесс	O
работы	O
над	O
контентом	O
.	O
Сейчас	O
такой	O
подход	O
с	O
интеллектуальными	O
помощниками	O
,	O
понимающими	O
ваш	O
запрос	O
,	O
—	O
самый	O
используемый	O
вариант	O
применения	O
в	O
большинстве	O
проектов	O
.	O
Получается	O
,	O
что	O
ChatGPT	B-Model
переводит	B-Task
запрос	I-Task
пользователя	I-Task
в	I-Task
понятный	I-Task
для	I-Task
системы	I-Task
вызов	I-Task
функции	I-Task
в	O
определённом	O
формате	O
и	O
с	O
указанными	O
пользователем	O
параметрами	O
,	O
на	O
этом	O
чаще	O
всего	O
магия	O
и	O
заканчивается	O
.	O
При	O
этом	O
даже	O
так	O
удобство	O
использования	O
значительно	O
улучшается	O
.	O
Что	O
мы	O
имеем	O
из	O
ванильных	O
моделей	O
:	O
GPT	B-Model
-	I-Model
1	I-Model
(	O
2018	B-Date
)	O
(	O
Context	O
:	O
512	O
)	O
-	O
Работала	O
не	O
очень	O
хорошо	O
(	O
длинные	O
тексты	O
генерировались	O
плохо	O
)	O
,	O
но	O
при	O
файнтюнинге	O
на	O
отдельных	O
задачах	O
эта	O
модель	O
могла	O
выполнять	O
несложные	O
задания	O
.	O
GPT	B-Model
-	I-Model
2	I-Model
(	O
2019	B-Date
)	O
(	O
Context	O
:	O
1024	O
)	O
-	O
стала	O
лучше	O
,	O
научилась	O
писать	B-Task
длинные	I-Task
связные	I-Task
тексты	I-Task
и	O
даже	O
решать	O
задачи	O
при	O
помощи	O
prompt	B-Method
engineering	I-Method
без	O
обучения	O
.	O
GPT	B-Model
-	I-Model
3	I-Model
(	O
2020	B-Date
)	O
(	O
Context	O
:	O
2048	O
)	O
-	O
Стала	O
в	O
10	O
(	O
!	O
)	O
раз	O
больше	O
,	O
и	O
настолько	O
крутой	O
,	O
что	O
даже	O
научилась	O
писать	B-Task
рабочий	I-Task
программный	I-Task
код	I-Task
.	O
RuGPT3	B-Model
,	O
RuDialoGPT3	B-Model
,	O
(	O
Context	O
:	O
512	O
!	O
)	O
-	O
и	O
множество	O
других	O
дообученных	O
версий	O
GPT3	B-Model
под	O
русский	B-Lang
язык	I-Lang
было	O
обучено	O
и	O
выложено	O
Сбером	B-Organization
в	O
открытый	O
доступ	O
(	O
В	O
свое	O
время	O
я	O
даже	O
эссе	O
по	O
истории	O
,	O
общаге	O
и	O
паре	O
спецов	O
написал	O
исключительно	O
ими	O
)	O
.	O
Но	O
был	O
у	O
них	O
один	O
небольшой	O
нюанс	O
.	O
Небольшой	O
он	O
настолько	O
же	O
,	O
как	O
их	O
контекст	O
,	O
длиной	O
512	O
токенов	O
.	O
(	O
это	O
как	O
у	O
самой	O
первой	O
GPT	O
,	O
да	O
)	O
.	O
Просто	O
выяснилось	O
,	O
что	O
если	O
тренировать	O
модель	O
,	O
рассчитанную	O
на	O
2048	O
контекст	O
кусками	O
текста	O
по	O
512	O
,	O
то	O
она	O
немного	O
отупеет	O
.	O
Свято	O
место	O
пусто	O
не	O
бывает	O
,	O
кто	O
-	O
то	O
должен	O
был	O
начать	O
это	O
монетизировать	O
.	O
Этим	O
занялись	O
сами	O
создатели	O
архитектуры	O
-	O
OpenAI	B-Organization
,	O
которые	O
решили	O
пойти	O
против	O
своего	O
названия	O
и	O
запустить	O
сайт	O
,	O
с	O
чат	O
интерфейсом	O
своей	O
новой	O
версии	O
GPT3	B-Model
,	O
дообученной	O
на	O
контексте	O
разметки	O
чата	O
-	O
ChatGPT	B-Model
.	O
В	O
первый	O
день	O
её	O
выхода	O
в	O
открытый	O
тест	O
я	O
зарегал	O
temp	O
phone	O
number	O
и	O
был	O
разочарован	O
.	O
Он	O
работал	O
ничуть	O
не	O
лучше	O
ванильной	O
GPT3	B-Model
на	O
английском	O
,	O
а	O
русский	B-Lang
язык	I-Lang
был	O
вообще	O
машинным	O
переводом	O
на	O
входе	O
и	O
выходе	O
.	O
После	O
выхода	O
ChatGPT3	B-Model
.	I-Model
5	I-Model
с	O
закрытыми	O
исходниками	O
долгое	O
время	O
была	O
видимость	O
того	O
,	O
что	O
развитие	O
отечественной	O
индустрии	O
энтузиастами	O
умрёт	O
вместе	O
с	O
рождением	O
ChatGPT4	B-Model
.	O