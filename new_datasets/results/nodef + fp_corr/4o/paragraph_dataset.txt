В O
этой O
статье O
я O
расскажу O
все O
, O
что O
вам O
нужно O
знать O
про O
ALBERT B-Model
, O
RoBERTa B-Model
, O
и O
DistilBERT B-Model
. O
Если O
непонятно O
по O
названию O
, O
эти O
модели O
— O
модифицированные O
версии O
оригинального O
современного O
трансформера O
BERT B-Model
. O
Эти O
три O
модели O
из O
библиотеки O
Hugging B-Organization
Face I-Organization
— O
самые O
популярные O
на O
сегодняшний O
день O
. O
Я O
рассмотрю O
их O
сходства O
и O
различия O
по O
сравнению O
друг O
с O
другом O
и O
добавлю O
фрагменты O
кода O
. O
Они O
покажут O
, O
как O
вы O
можете O
их O
использовать O
. O
А O
теперь O  
давайте O  
еще O  
вспомним O  
, O  
что O  
обкатанная O  
на O  
GPT B-Model
- I-Model
1 I-Model
технология B-InfoResource  
Трансформеров I-InfoResource  
оказалась O  
на O  
редкость O  
удачной O  
в O  
плане O  
масштабирования O  
: O  
она O  
умеет O  
работать O  
с O  
большими O  
объемами O  
данных O  
и O  
« O
массивными O
» O
моделями O  
( O  
состоящими O  
из O  
огромного O  
числа O  
параметров O  
) O  
гораздо O  
эффективнее O  
своих O  
предшественников O  
. O  
Вы O  
думаете O  
о O  
том O  
же O  
, O  
о O  
чем O  
и O  
я O  
? O  
Ну O  
вот O  
и O  
ученые B-Person  
из O  
OpenAI B-Organization  
в O  
2019 O  
году O  
сделали O  
такой O  
же O  
вывод O  
: O  
« O  
Пришло O  
время O  
пилить O  
здоровенные O  
языковые B-Lang  
модели I-Lang  
! O  
» O  
. O  
В O  
общем O  
, O  
было O  
решено O  
радикально O  
прокачать O  
GPT B-Model
- I-Model
2 I-Model
по O  
двум O  
ключевым O  
направлениям O  
: O  
набор B-Dataset  
тренировочных I-Dataset  
данных I-Dataset  
( O  
датасет O  
) O  
и O  
размер B-Metric  
модели I-Metric  
( O  
количество I-Metric  
параметров I-Metric  
) O  
. O  
На O  
тот O  
момент O  
не O  
было O  
каких O
‑ O
то O
специальных O  
, O  
больших O  
и O  
качественных O  
, O  
публичных O  
наборов B-Dataset  
текстовых I-Dataset  
данных I-Dataset  
для O  
тренировки O  
языковых B-Lang  
моделей I-Lang  
— O  
так O  
что O  
каждой O  
команде O  
специалистов O  
по O  
ИИ O  
приходилось O  
извращаться O  
согласно O  
их O  
собственной O  
степени O  
испорченности O  
. O  
Вот O  
ребята O  
из O  
OpenAI B-Organization  
и O  
решили O  
поступить O  
остроумно O  
: O  
они O  
пошли O  
на O  
самый O  
популярный O  
англоязычный B-Lang  
онлайн O
‑ O
форум O
Reddit B-InfoResource  
и O  
тупо O  
выкачали O  
все O  
гиперссылки O  
из O  
всех O  
сообщений O  
, O  
имевших O  
более O  
трех O  
лайков O  
( O  
я O  
сейчас O  
не O  
шучу O  
— O  
научный O  
подход O  
, O  
ну O  
! O  
) O  
. O
У O
классических O
transition B-Model
- I-Model
based I-Model
parser I-Model
возможны O
три O
операции O
, O
перечисленные O
выше O
: O
стрелка O
в O
одну O
сторону O
, O
стрелка O
в O
другую O
сторону O
и O
шифт O
. O
Есть O
еще O
операция O
Swap O
, O
в O
базовых O
архитектурах O
transition B-Model
- I-Model
based I-Model
парсеров I-Model
она O
не O
используется O
, O
но O
в O
UDPipe B-Model
включена O
. O
Swap O
возвращает O
второй O
элемент O
стека O
в O
буфер O
, O
чтобы O
взять O
потом O
из O
буфера O
следующий O
( O
в O
случае O
если O
они O
разнесены O
) O
. O
Это O
помогает O
пропустить O
некоторое O
количество O
слов O
и O
восстановить O
правильную O
связь O
. O
Для O
разработчиков O
среднего O
и O
высокого O
уровня O
нейросети O
становятся O
полноценными O
помощниками O
. O
Есть O
возможность O
прокачать O
и O
оптимизировать O
процессы O
, O
то O
есть O
сильно O
повысить O
свою O
продуктивность O
. O
Например O
, O
у O
компании B-Organization
GitHub I-Organization
есть O
Copilot B-Application
, O
у O
Amazon B-Organization
— O
CodeWhisperer B-Application
. O
Это O
системы O
, O
которые O
поддерживают O
написание O
кода O
и O
облегчают O
работу O
программиста O
. O
Davinchi B-Model
– O
самая O
крупная O
модель O
OpenAI B-Organization
( O
для O
понимания O
: O
при O
обучении B-Activity
модели O
использовалось O
более O
150 O
миллиардов O
обучающих I-Activity
блоков I-Activity
, O
тогда O
как O
GPT B-Model
- I-Model
3 I-Model
, I-Model
5 I-Model
и O
GPT B-Model
- I-Model
4 I-Model
обучались O
всего O
на O
6 O
миллиардах O
блоков O
данных O
) O
. O
Модель O
успешно O
решает O
задачи O
, O
связанные O
с O
поиском B-Task
причинно I-Task
- I-Task
следственных I-Task
связей I-Task
, O
и O
генерирует O
более O
качественный O
текст O
, O
когда O
речь O
идет O
о O
сложных O
задачах O
. O
При O
этом O
Davinchi B-Model
потребляет O
больше O
ресурсов O
и O
времени O
. O
Интерактивное O
взаимодействие O
с O
моделью O
происходит O
за O
счет O
« O
Промтов O
» O
, O
то O
есть O
запросов O
, O
которые O
мы O
отсылаем O
в O
нейросеть O
. O
Поэтому O
эффективность O
применения O
ChatGPT B-Application
напрямую O
зависит O
от O
такого O
, O
каким O
образом O
вы O
конструируете O
запрос O
. O
Модели B-Model
OpenAI B-Organization
, O
доступные O
к O
взаимодействию O
: O
GPT	B-Model
- I-Model
3 I-Model
, I-Model
5 I-Model
и O
GPT	B-Organization
-	I-Organization
4	I-Organization
–	O
это O
основные O
модели O
, O
с O
которыми O
мы O
работаем O
через O
Telegram B-Application
или O
по O
API B-Application
в O
базовых O
настройках O
. O
Это O
немодифицированные O
типовые O
модели O
, O
которые O
максимально O
оптимизированы O
для O
универсальных O
запросов O
под O
обычного O
пользователя O
. O
Они O
обучены O
на O
ограниченном O
количестве O
данных O
, O
но O
зато O
выдают O
самый O
быстрый O
результат O
из O
всех O
моделей O
. O
Ada B-Model
- O
самая O
быстрая O
из O
всех O
моделей O
, O
способна O
выполнять O
такие O
задачи O
, O
как O
синтаксический O
анализ O
текста O
, O
исправление O
адреса O
и O
менее O
сложные O
задачи O
классификации B-Task
. O
Babage B-Model
- O
лучше O
всего O
подходит O
для O
простых O
задач O
классификации B-Task
и O
выполняет O
SEO B-Task
- I-Task
анализ I-Task
текста O
. O
Curie B-Model
- O
подходит O
для O
задач O
классификация B-Task
и O
анализа O
настроений O
. O
Модель O
также O
выдает O
результаты O
на O
запросы O
, O
отвечает O
на O
вопросы O
и O
может O
использоваться O
в O
качестве O
чат O
- O
бота O
общего O
назначения O
. O
Сравнение O
показывает O
, O
что O
она O
может O
выполнять O
многие O
задачи O
Davinci B-Model
, O
но O
за O
10 O
% O
стоимости O
. O
В O
целом O
мы O  
видим O  
похожую O  
картину O  
, O  
хотя O  
точность O  
предсказаний O  
CatBoost B-Model
получилась O  
выше O  
, O  
чем O  
нейронной O  
сети O  
. O  
В O  
CatBoost B-Model
есть O  
функция O  
для O  
отрисовки O  
деревьев O  
, O  
воспользуемся O  
ей O  
, O  
чтобы O  
нарисовать O  
первое O  
дерево O  
: O  
model B-Application
. I-Application
plot_tree I-Application
( O  
0 O  
) O  
. O  
Каждое O  
звено O  
дерева O  
содержит O  
разделяющее O  
правило O  
, O  
в O  
котором O  
проверяется O  
, O  
что O  
значение O  
указанного O  
признака O  
больше O  
указанного O  
порога O  
. O  
Каждый O  
лист O  
дерева O  
выдает O  
2 O  
числа O  
: O  
само O  
значение O  
и O  
уверенность O  
. O  
Оба O  
эти O  
числа O  
суммируются O  
по O  
всем O  
дерьявм O  
. O  
5 O
. O
2 O
. O  
Регрессия B-Task
с O  
константной O  
оценкой O  
дисперсии O  
* O  
Вспомним O  
вероятностную O  
модель B-Model
регрессии B-Task
, O  
которую O  
мы O  
рассматривали O  
во O  
второй O  
части O  
( O  
2 O  
) O  
. O  
В O  
ней O  
является O  
константой O  
. O  
Для O  
любого O  
значения O  
локальные O  
и O  
глобальные O  
минимумы O  
функции B-Metric
потерь O  
одни O  
и O  
те O  
же O  
, O  
и O  
изменение O  
эквивалентно O  
масштабированию O  
B-Metric функции B-Metric  
потерь O  
, O  
что O  
равносильно O  
изменению O  
learning B-Method_ML
rate O  
. O  
Отсюда O  
получается O  
, O  
что O  
какое O  
бы O  
мы O  
не O  
брали O  
- O  
ничего O  
не O  
изменится O  
( O  
поскольку O  
B-Method_ML learning B-Method_ML  
rate O  
выбирается O  
совсем O  
по O  
другим O  
соображениям O  
) O  
. O
AI B-Application
Text I-Application  
Classifier I-Application  
запустили O  
31 O  
января O  
2023 O  
года O  
. O  
Как O  
и O  
остальные O  
инструменты O  
OpenAI B-Organization  
, O  
он O  
бесплатный O  
. O  
Работает O  
так O  
: O  
анализирует O  
образцы O  
текста O  
, O  
сравнивает O  
их O  
с O  
образцами O  
баз O  
данных O  
, O  
написанными O  
как O  
нейросетью O  
, O  
так O  
и O  
людьми O  
, O  
анализирует O  
множество O  
разных O  
стилей O  
написания O  
, O  
даже O  
такие O  
, O  
как O  
новостные O  
статьи O  
, O  
научные B-Science  
документы I-Science  
, O  
посты O  
в O  
соцсетях O  
, O  
рекламные O  
тексты O  
. O  
Хоть O  
и O  
нельзя O  
со O  
стопроцентной O  
уверенностью O  
сказать O  
, O  
что O  
тот O  
или O  
иной O  
текст O  
написан O  
нейросетью O  
, O  
потенциал O  
у O  
AI B-Application  
Text I-Application  
Classifier I-Application  
есть O  
: O  
через O  
него O  
можно O  
обеспечивать O  
высокое O  
качество O  
текстов O  
в O  
сети O  
. O  
В O  
компании O  
OpenAI B-Organization  
утверждают O  
, O  
что O  
новый O  
инструмент O  
пресечёт O  
такие O  
попытки O  
обмана O  
, O  
как O  
запуск O  
автоматических O  
кампаний O  
по O  
дезинформации O  
, O  
мошенничество O  
с O  
задействованием O  
ИИ O  
в O  
научной B-Science  
сфере I-Science  
, O  
имитацию O  
переписки O  
с O  
живым O  
человеком O  
с O  
помощью O  
чат O
- O
ботов O
. O  
Чтобы O  
знать O  
, O  
как O  
работает O  
программа O  
, O  
нужно O  
понимать O  
особенности O  
текстов O  
, O  
которые O  
генерирует O  
ИИ O  
. O  
Такой O  
текст O  
чаще O  
всего O  
несогласованный O  
и O  
несвязный O  
: O  
модели O  
ИИ O  
в O  
большинстве O  
случаев O  
не O  
способны O  
создать O  
текст O  
, O  
основные O  
мысли O  
которого O  
логично O  
и O  
естественно O  
перетекают O  
из O  
одной O  
в O  
другую O  
. O
GPT B-Model
внедряют O
в O
большое O
количество O
проектов O
. O
Чаще O
всего O
это O
, O
конечно O
, O
анализ B-Task
данных I-Task
, O
кластеризация B-Task
, O
группировка B-Task
, O
выявление B-Task
закономерностей I-Task
. O
То O
есть O
то O
, O
для O
чего O
искусственные O
нейронные O
сети O
использовались O
и O
раньше O
. O
В O
новых O
задачах O
генеративного O
плана O
пока O
что O
на O
практике O
GPT B-Model
используется O
не O
так O
много O
, O
как O
хотелось O
бы O
, O
но O
есть O
замечательные O
примеры O
. O
Первый O
и O
самый O
активно O
используемый O
— O
это O
AI O
помощник O
, O
встроенный O
в O
Notion B-Application
, O
— O
он O
может O
как O
помочь O
улучшить O
уже O
написанный O
текст O
, O
так O
и O
сгенерировать O
план O
или O
даже O
целиком O
статью O
под O
ваш O
запрос O
. O
Главное O
— O
не O
нужно O
взаимодействовать O
с O
каким O
- O
то O
отдельным O
сервисом O
, O
все O
удобно O
интегрировано O
в O
процесс O
работы O
над O
контентом O
. O
Сейчас O
такой O
подход O
с O
интеллектуальными O
помощниками O
, O
понимающими O
ваш O
запрос O
, O
— O
самый O
используемый O
вариант O
применения O
в O
большинстве O
проектов O
. O
Получается O
, O
что O
ChatGPT B-Model
переводит O
запрос O
пользователя O
в O
понятный O
для O
системы O
вызов O
функции O
в O
определённом O
формате O
и O
с O
указанными O
пользователем O
параметрами O
, O
на O
этом O
чаще O
всего O
магия O
и O
заканчивается O
. O
При O
этом O
даже O
так O
удобство O
использования O
значительно O
улучшается O
. O
Что O
мы O
имеем O
из O
ванильных O
моделей O
: O
GPT	B-Model
-	I-Model
1	I-Model
( O
2018 O
) O
( O
Context O
: O
512 O
) O
- O
Работала O
не O
очень O
хорошо O
( O
длинные O
тексты O
генерировались O
плохо O
) O
, O
но O
при O
файнтюнинге O
на O
отдельных O
задачах O
эта O
модель O
могла O
выполнять O
несложные O
задания O
. O
GPT	B-Model
-	I-Model
2	I-Model
( O
2019 O
) O
( O
Context O
: O
1024 O
) O
- O
стала O
лучше O
, O
научилась O
писать O
длинные O
связные O
тексты O
и O
даже O
решать O
задачи O
при O
помощи O
prompt	B-Method
engineering	I-Method
без O
обучения O
. O
GPT	B-Model
-	I-Model
3	I-Model
( O
2020 O
) O
( O
Context O
: O
2048 O
) O
- O
Стала O
в O
10 O
( O
! O
) O
раз O
больше O
, O
и O
настолько O
крутой O
, O
что O
даже O
научилась O
писать O
рабочий O
программный O
код O
. O
RuGPT3	B-Model
, O
RuDialoGPT3	B-Model
, O
( O
Context O
: O
512 O
! O
) O
- O
и O
множество O
других O
дообученных O
версий O
GPT3	B-Model
под O
русский	B-Lang
язык	I-Lang
было O
обучено O
и O
выложено O
Сбером	B-Organization
в O
открытый O
доступ O
( O
В O
свое O
время O
я O
даже O
эссе O
по O
истории O
, O
общаге O
и O
паре O
спецов O
написал O
исключительно O
ими O
) O
. O
Но O
был O
у O
них O
один O
небольшой O
нюанс O
. O
Небольшой O
он O
настолько O
же O
, O
как O
их O
контекст O
, O
длиной O
512 O
токенов O
. O
( O
это O
как O
у O
самой O
первой O
GPT	B-Model
, O
да O
) O
. O
Просто O
выяснилось O
, O
что O
если O
тренировать O
модель O
, O
рассчитанную O
на O
2048 O
контекст O
кусками O
текста O
по O
512 O
, O
то O
она O
немного O
отупеет O
. O
Свято O
место O
пусто O
не O
бывает O
, O
кто O
- O
то O
должен O
был O
начать O
это O
монетизировать O
. O
Этим O
занялись O
сами O
создатели O
архитектуры O
- O
OpenAI	B-Organization
, O
которые O
решили O
пойти O
против O
своего O
названия O
и O
запустить O
сайт O
, O
с O
чат O
интерфейсом O
своей O
новой O
версии O
GPT3	B-Model
, O
дообученной O
на O
контексте O
разметки O
чата O
- O
ChatGPT	B-Model
. O
В O
первый O
день O
её O
выхода O
в O
открытый O
тест O
я O
зарегал O
temp O
phone O
number O
и O
был O
разочарован O
. O
Он O
работал O
ничуть O
не O
лучше O
ванильной O
GPT3	B-Model
на	O
английском	B_Lang
,	O
а	O
русский	B-Lang
язык	I-Lang
был	O
вообще O
машинным O
переводом O
на O
входе O
и O
выходе O
. O
После O
выхода O
ChatGPT3	B-Model
.	I-Model
5	I-Model
с	O
закрытыми O
исходниками O
долгое O
время O
была O
видимость O
того O
, O
что O
развитие O
отечественной O
индустрии O
энтузиастами O
умрёт O
вместе O
с O
рождением O
ChatGPT4	B-Model
. O