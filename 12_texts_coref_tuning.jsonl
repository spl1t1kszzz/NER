{"messages": [{"role": "system","content": "Ты являешься экспертом в задаче решения референций из текстов на русском языке."}, {"role": "user", "content": "**Ты решаешь задачу разрешения кореференций (coreference resolution) в текстах на русском языке с помощью пошагового рассуждения (Chain-of-Thought).**\n\nСледуй указанным шагам:\n\n---\n\n### \uD83D\uDD39 Шаг 1 (Извлечение упоминаний):\n\n- Прочитай текст.\n- Выпиши все упоминания сущностей (людей, компаний, продуктов, сервисов и т.д.), а также местоимения и фразы, которые могут на них ссылаться.\n- Учитывай не только имена собственные, но и абстрактные понятия (например, \"база знаний\", \"информация\", \"уязвимости\"), если на них ссылаются в тексте.\n- Постарайся различать случаи, когда одно и то же слово может обозначать разные вещи в зависимости от контекста (например, \"ИИ\" может означать как технологию, так и конкретного помощника — Atlassian Intelligence).\n\n---\n\n### \uD83D\uDD39 Шаг 2 (Определение кореференций и группировка):\n\n- Определи, какие из упоминаний относятся к одной и той же сущности.\n- Учитывай грамматические признаки (род, число, падеж), контекст и смысл текста.\n- Если возникают сомнения, выбери наиболее вероятную связь. В сложных случаях можешь кратко пояснить причину (1–2 слова).\n- Сгруппируй упоминания по кластерам — каждому кластеру соответствует одна сущность.\n\n---\n\n### \uD83D\uDD39 Шаг 3 (Итоговый результат):\n\n- Верни только результат в виде массива массивов.\n- Каждый внутренний массив — отдельный кластер кореференций.\n- Не добавляй комментариев, пояснений или шагов рассуждения к итоговому выводу.\n\n---\n\n### \uD83D\uDCCC Пример:\n\n**Текст:**\n\n«Мария живет в Москве. Она любит этот город за его атмосферу.»\n\n**Ответ:**\n\n```json\n[\n  [\"Мария\", \"Она\"],\n  [\"Москве\", \"этот город\", \"его\"]\n]\n```\n\n---\n\nТеперь выполни аналогичный анализ для следующего текста, соблюдая описанные шаги рассуждения:\n\n`\"\n\nЗаметки об NLP (часть 3) / Habr\n\n\n              1  January  2010 at 18:26  Заметки об NLP (часть 3) Artificial Intelligence Natural Language Processing *      (Начало: 1, 2) Что ж, подходим к самому интересному — разбору предложений. Тема эта многогранна и многоуровнева, так что подступиться к ней не очень просто. Но ведь трудности лишь закаляют :) Да и выходные, текст пишется легко…\n\nНачнём с такого понятия, как синтаксический анализ предложений (по-английски parsing). Суть этого процесса состоит в построении графа, «каким-либо образом» отражающего структуру предложения.\nЯ говорю «каким-либо образом» потому, что на сегодня не существует единственно принятой системы принципов, на которых строится упомянутый граф. Даже в рамках одной концепции взгляды отдельных учёных на зависимости между словами могут различаться (это напоминает разногласия в трактовке морфологических явлений, о чём шла речь в предыдущей части).\n\nНаверно, прежде всего надо разделить способы построения графа (обычно — дерева) зависимостей на phrase structure-based parsing и dependency parsing.\n\nПредставители первой школы разделяют предложение на «составляющие», далее каждая составляющая разбивается на свои составляющие — и так до тех пор, пока не дойдём до слов. Эту идею хорошо иллюстрирует рисунок из Википедии:\n\n\nПредставители второй школы соединяют зависящие друг от друга слова между собой непосредственно, без каких-либо вспомогательных узлов:\n\n\nСразу скажу, что мои симпатии на стороне второго подхода (dependency parsing), но оба они заслуживают более детального обсуждения.\n\nШкола ХомскогоРазбор «по составляющим» явно вырос из грамматик Хомского. Если кто не знает, грамматика Хомского представляет собой способ задания правил, описывающих предложения языка. С помощью такой грамматики можно как генерировать фразы, так и анализировать. Например, следующая грамматика описывает «язык», состоящий из произвольного количества букв a, за которым следует произвольное количество букв b:\n\nS -> aS | bA | 'empty'\nA -> bA | 'empty'\n\nНачав с символа S, можно сгенерировать любую строку вида a...ab...b. Существует также универсальный алгоритм разбора такой грамматики. Скормив ему входную строку и набор правил грамматики, можно получить ответ — является ли строка корректной строкой в рамках данного языка или нет. Можно получить и дерево разбора, показывающее, каким образом строка выводится из начального символа S.\n\nДопустим, строке aabb соответствует вот такое дерево:\n\n\nЯвный плюс этого метода состоит в том, что грамматики Хомского — формализм давно известный. Существуют давно разработанные алгоритмы разбора, известны «формальные свойства» грамматик, т.е. их выразительные способности, сложность обработки и т.п. Кроме того, грамматики Хомского успешно применяются при компиляции языков программирования.\n\nСам Хомский прежде всего лингвист, и свои работы он примерял на естественный язык, английский, в первую очередь. Поэтому в англоязычной компьютерной лингвистике влияние его трудов достаточно велико. Хотя «в лоб» сейчас формализмы Хомского, насколько мне известно, при обработке текстов на естественном языке не применяют (они недостаточно развиты для этого), дух его школы живёт.\n\nХороший пример синтаксического анализатора, строящего подобные деревья — Stanford parser (есть онлайн-демо).\n\nМодель отношений между словами\nВообще этот подход тоже трудно назвать особо свежим. Все ссылаются на работы Люсьена Теньера (Lucien Tesniere) пятидесятых годов как на первоисточник. Упоминают и более ранние мысли (но из той же оперы, что называть отцом ООП Платона, т.к. он ввёл в оборот понятие «мира идей», то есть абстрактных классов). Однако в компьютерной лингвистике dependency parsing долгое время был на втором плане, в то время как грамматики Хомского активно применялись. Вероятно, ограничения подхода Хомского особенно больно ударили по языкам с более свободным (чем в английском) порядком слов, поэтому самые интересные работы в области dependency parsing до сих пор выполняются «снаружи» англоязычного мира.\n\nОсновная идея dependency parsing — соединять между собой зависимые слова. Центром практически любой фразы является глагол (явный или подразумеваемый). Далее от глагола (действия) можно задавать вопросы: кто делает, что делает, где делает и так далее. Для присоединённых сущностей тоже можно задать вопросы (в первую очередь, вопрос «какой»). Например, для приведённого выше дерева «я купил кофе в большом магазине» можно воспроизвести такую цепочку вопросов. Корень — купил (действие фразы). Кто купил? — Я. Что купил? — Кофе. Где купил? — В магазине. В каком магазине? — В большом.\n\nЗдесь тоже есть множество технических тонкостей и неоднозначностей. Можно по-разному обрабатывать отсутствие глагола. Обычно всё равно подразумевается глагол «to be»: «Я [есть] студент». В предикативных предложениях дело обстоит сложнее: На улице сыро. Не скажешь же, что на улице есть сыро :) Не всегда понятно, что от чего зависит, и как это трактовать. Например, «я не пойду сегодня на работу». Как соотносится с прочими словами частица «не»? Как вариант, можно считать, что здесь используется глагол «недеяния» «не_пойду» (пусть в русском такого нет, но по смыслу подходит). Не совсем понятно, как лепить однородные члены, соединённые союзом. «Я купил кофе и булочку». Например, можно лепить к «купил» слово «и», а к «и» присоединять уже «кофе» и «булочку». Но есть и другие подходы. Довольно тонкий момент возникает при взаимодействии слов, образующих некое единство: «я буду ходить на работу». Понятно, что «буду ходить» — это по сути один-единственный глагол (то есть действие) будущего времени, просто создан он двумя словами.\n\nЕсли хочется посмотреть на такой анализатор в действии — могу посоветовать сайт фирмы Connexor.\n\nЧем dependency parsing притягателен? Приводят разные аргументы. Например, говорится, что соединяя между собой слова, мы не создаём дополнительных сущностей, и, стало быть, упрощаем дальнейший анализ. В конце концов, синтаксический анализ — это лишь очередной этап обработки текста, и дальше надо представлять, что с полученным деревом делать. В каком-то смысле дерево зависимостей «чище», ибо показывает явные семантические связи между элементами предложения. Далее, нередко утверждают, что dependency parsing больше подходит для языков со свободным порядком слов. У Хомского все зависимые блоки так или иначе действительно оказываются рядом друг с другом. Здесь же в теории можно иметь связи между словами на разных концах предложения (хотя и здесь технически не так всё просто, но об этом позже). В принципе, уже этих аргументов для меня достаточно, чтобы примкнуть к лагерю Теньера :)\n\nНадо сказать, что существуют формальные доказательства близости получающихся деревьев. Где-то проскакивала теорема о том, что дерево одного вида можно сконвертировать в дерево другого вида и наоборот. Но на практике это не работает. По крайней мере, на моей памяти никто не пытался получить дерево зависимостей путём преобразования выходных данных Stanford parser'а. Видимо, не всё так просто, да и ошибки множатся… сначала стэнфордский парсер ошибётся, потом алгоритм конвертации ошибётся… и что получится в конце? Ошибка на ошибке.\n\n(UPD: упомянутые ребята из Стэнфорда всё же протестировали метод конвертации выходных данных своего парсера в dependency-структуры. Однако должен заметить, что при такой конвертации получаются только проективные деревья, речь о которых идёт в пятой части).\nНаверно, на сегодня хватит. Продолжим в следующей части.    Tags: NLPобработка текстовкомпьютерная лингвистика Hubs: Artificial IntelligenceNatural Language Processing"}, {"role": "assistant", "content": "[['NLP', 'NLP', 'Natural Language Processing', 'NLP', 'Natural Language Processing'], ['разбору предложений', 'Тема эта', 'ней', 'разбора'], ['синтаксический анализ предложений', 'parsing', 'этого процесса', 'синтаксический анализ'], ['графа', 'упомянутый граф', 'графа (обычно — дерева) зависимостей'], ['Я', 'мои', 'мне', 'Я', 'меня'], ['разногласия в трактовке морфологических явлений', 'чём'], ['способы построения графа', 'оба они'], ['дерева', 'дерево разбора', 'дерево', 'деревья', 'дерева', 'деревом', 'дерево', 'дерево', 'деревья'], ['phrase structure-based parsing', 'первой школы', 'Разбор «по составляющим»'], ['dependency parsing', 'второй школы', 'второго подхода', 'dependency parsing', 'этот подход', 'dependency parsing', 'dependency parsing', 'dependency parsing', 'dependency parsing', 'dependency parsing', 'Здесь', 'здесь'], ['Хомского', 'Хомского', 'Хомского', 'Хомского', 'Хомского', 'Хомский', 'свои', 'он', 'его', 'Хомского', 'его', 'Хомского', 'Хомского', 'Хомского'], ['грамматик Хомского', 'грамматики Хомского', 'грамматик', 'их', 'грамматики Хомского', 'грамматики Хомского'], ['грамматика Хомского', 'грамматики', 'такой грамматики', 'грамматики'], ['следующая грамматика'], ['S', 'символа S', 'начального символа S'], ['универсальный алгоритм разбора такой грамматики', 'ему'], ['входную строку', 'строка', 'строка'], ['естественный язык', 'естественном языке'], ['компьютерной лингвистике', 'компьютерной лингвистике'], ['формализмы Хомского', 'они'], ['Stanford parser', \"Stanford parser'а\", 'стэнфордский парсер', 'упомянутые ребята из Стэнфорда'], ['Люсьена Теньера', 'Lucien Tesniere', 'Теньера'], ['Платона', 'он'], ['глагол', 'глагола', 'глагола'], ['вопросы', 'вопросов'], ['я'], ['кофе', 'Кофе'], ['магазине', 'магазине', 'магазине'], ['кофе', 'кофе'], ['булочку', 'булочку'], ['слово «и»', '«и»'], ['«буду ходить»', 'один-единственный глагол', 'он'], ['разные аргументы', 'этих аргументов'], ['дерево зависимостей', 'дерево зависимостей'], ['не так всё просто', 'этом'], ['конвертации', 'такой конвертации']]"}]}
{"messages": [{"role": "system", "content": "Ты являешься экспертом в задаче решения референций из текстов на русском языке."}, {"role": "user", "content": "**Ты решаешь задачу разрешения кореференций (coreference resolution) в текстах на русском языке с помощью пошагового рассуждения (Chain-of-Thought).**\n\nСледуй указанным шагам:\n\n---\n\n### \uD83D\uDD39 Шаг 1 (Извлечение упоминаний):\n\n- Прочитай текст.\n- Выпиши все упоминания сущностей (людей, компаний, продуктов, сервисов и т.д.), а также местоимения и фразы, которые могут на них ссылаться.\n- Учитывай не только имена собственные, но и абстрактные понятия (например, \"база знаний\", \"информация\", \"уязвимости\"), если на них ссылаются в тексте.\n- Постарайся различать случаи, когда одно и то же слово может обозначать разные вещи в зависимости от контекста (например, \"ИИ\" может означать как технологию, так и конкретного помощника — Atlassian Intelligence).\n\n---\n\n### \uD83D\uDD39 Шаг 2 (Определение кореференций и группировка):\n\n- Определи, какие из упоминаний относятся к одной и той же сущности.\n- Учитывай грамматические признаки (род, число, падеж), контекст и смысл текста.\n- Если возникают сомнения, выбери наиболее вероятную связь. В сложных случаях можешь кратко пояснить причину (1–2 слова).\n- Сгруппируй упоминания по кластерам — каждому кластеру соответствует одна сущность.\n\n---\n\n### \uD83D\uDD39 Шаг 3 (Итоговый результат):\n\n- Верни только результат в виде массива массивов.\n- Каждый внутренний массив — отдельный кластер кореференций.\n- Не добавляй комментариев, пояснений или шагов рассуждения к итоговому выводу.\n\n---\n\n### \uD83D\uDCCC Пример:\n\n**Текст:**\n\n«Мария живет в Москве. Она любит этот город за его атмосферу.»\n\n**Ответ:**\n\n```json\n[\n  [\"Мария\", \"Она\"],\n  [\"Москве\", \"этот город\", \"его\"]\n]\n```\n\n---\n\nТеперь выполни аналогичный анализ для следующего текста, соблюдая описанные шаги рассуждения:\n\n`\"\n\nИзучаем синтаксические парсеры для русского языка / Habr\n\n\n               9  August  2018 at 09:00  Изучаем синтаксические парсеры для русского языка Сбер corporate blog Programming *Machine learning *Artificial Intelligence       Привет! Меня зовут Денис Кирьянов, я работаю в Сбербанке и занимаюсь проблемами обработки естественного языка (NLP). Однажды нам понадобилось выбрать синтаксический парсер для работы с русским языком. Для этого мы углубились в дебри морфологии и токенизации, протестировали разные варианты и оценили их применение. Делимся опытом в этом посте.\n\n\n\nПодготовка к отбору \n\nНачнём с основ: как все работает? Мы берем текст, проводим токенизацию и получаем некоторый массив псевдослов-токенов. Этапы дальнейшего анализа укладываются в пирамиду:\n\n\n\n\nНачинается все с морфологии — с анализа формы слова и его грамматических категорий (род, падеж и т.п.). На морфологии базируется синтаксис — взаимоотношения за рамками одного слова, между словами. Синтаксические парсеры, о которых пойдет речь, анализируют текст и выдают структуру зависимостей слов друг от друга.\n\nГрамматика зависимостей и грамматика непосредственных составляющих\n\nЕсть два основных подхода к синтаксическому анализу, которые в лингвистической теории существуют примерно на равных.\n\n\n\n\nВ первой строке предложение разобрано в рамках грамматики зависимостей. Этому подходу учат в школе. Каждое слово в предложении как-то связано с другими. «Мыла» — сказуемое, от которого зависит подлежащее «мама» (здесь грамматика зависимостей расходится со школьной, где сказуемое зависит от подлежащего). У подлежащего есть зависимое определение «моя». У сказуемого есть зависимое прямое дополнение «раму». А у прямого дополнения «раму» — определение «грязную».\n\n\nВо второй строке разбор идет в соответствии с грамматикой непосредственно составляющих. \n\nСогласно ей, предложение делится на группы слов (phrases). Слова внутри одной группы связаны теснее. Слова «моя» и «мама» связаны более тесно, «раму» и «грязную» — тоже. И есть еще отдельное «мыла».\n\n\nВторой подход для автоматического парсинга русского языка применим плохо, потому что в нем тесно связанные слова (члены одной группы) очень часто не стоят подряд. Нам пришлось бы объединять их странными скобками — через одно или два слова. Поэтому в автоматическом парсинге русского языка принято работать исходя из грамматики зависимостей. Это удобно еще и потому, что с таким «фреймворком» все знакомы по школе.\n\nДерево зависимостей\n\nНабор зависимостей мы можем перевести в древовидную структуру. Вершина — слово «мыла», некоторые слова напрямую зависят от него, некоторые зависят от его зависимых. Вот определение дерева зависимостей из учебника Мартина и Журафского:\n\nDependency tree is a directed graph that satisfies the following constraints:\n\n\nThere is a single designated root node that has no incoming arcs.\n\nWith the exception of the root node, each vertex has exactly one incoming arc.\n\nThere is a unique path from the root node to each vertex in V.\n\n\n\nЕсть верхнеуровневый узел — сказуемое. Из него можно дойти до любого слова. Каждое слово зависит от другого, но только от одного. Дерево зависимостей выглядит примерно так:\n\n\n\n\nВ этом дереве ребра подписаны некоторым особым типом синтаксического отношения. В грамматике зависимостей анализируют не только факт связи между словами, но и характер этой связи. Например, «is taken» — это почти одна глагольная форма, «inventory» — это подлежащее для «is taken». Соответственно, у нас от «is» есть ребро и в одну, и в другую сторону. Это не одинаковые связи, они носят разный характер, так что их надо различать.\n\n\nЗдесь и далее мы рассматриваем простые случаи, где члены предложения присутствуют, а не подразумеваются. Существуют структуры и отметки, позволяющие бороться с пропусками. В дереве появляется нечто, у чего нет поверхностного выражения — слова. Но это уже предмет другого исследования, а нам все-таки надо сосредоточиться на своем.\n\nПроект Universal Dependencies\n\nЧтобы облегчить себе выбор парсера, мы обратили свой взгляд на проект Universal Dependencies и недавно прошедшее в его рамках соревнование CoNLL Shared Task.\n\n\nUniversal Dependencies — это проект по унификации разметки синтаксических корпусов (трибанков) в рамках грамматики зависимостей. В русском языке количество типов синтаксических связей ограничено — подлежащее, сказуемое и т.д. В английском то же самое, но набор уже другой. Например, там появляется артикль, который тоже надо как-то маркировать. Если бы мы хотели написать волшебный парсер, который мог бы обрабатывать все языки, то довольно быстро уперлись бы в проблемы сопоставления разных грамматик. Героическим создателям Universal Dependencies удалось договориться между собой и разметить все корпусы, которые имелись в их распоряжении, в едином формате. Не очень важно, как именно они договорились, главное, что на выходе мы получили некий единообразный формат представления всей этой истории — более 100 трибанков для 60 языков.\n\n\nCoNLL Shared Task — это соревнование между разработчиками алгоритмов синтаксического парсинга, проводимое в рамках проекта Universal Dependencies. Организаторы берут некоторое количество трибанков и разбивают каждый из них на три части — обучающую, валидационную и тестовую. Первая часть предоставляется участникам соревнования, чтобы они обучили на ней свои модели. Вторая часть тоже используется участниками — чтобы после обучения оценить работу алгоритма. Обучение и оценку участники могут итеративно повторять. Потом они отдают свой лучший алгоритм организаторам, которые прогоняют его на тестовой части, закрытой для участников. Итоги работы моделей на тестовых частях трибанков — это и есть итоги соревнования.\n\nМетрики качества\n\nУ нас есть связи между словами и их типы. Мы можем оценивать, правильно ли нашли вершину слова — метрика UAS (Unlabeled attachment score). Или оценивать, правильно ли найдена как вершина, так и тип зависимости — метрика LAS (Labeled attachment score).\n\n\n\n\nКазалось бы, здесь напрашивается оценка точности (accuracy) — считаем, сколько раз мы попали из общего количества случаев. Если у нас есть 5 слов и для 4 мы правильно определили вершину, то получаем 80%.\n\n\nНо на самом деле оценить парсер в чистом виде проблематично. Разработчики, решающие задачи автоматического парсинга, часто берут на вход сырой текст, который в соответствии с пирамидой анализа проходит этапы токенизации и морфологического анализа. На качество работы парсера могут повлиять ошибки с этих более ранних этапов. В частности, это относится к процедуре токенизации — выделения слов. Если мы выделили неправильные слова-юниты, то уже не сможем корректно оценить синтаксические связи между ними — ведь в нашем исходном размеченном корпусе юниты были другие.\n\n\nПоэтому формулой оценки в данном случае является ф-мера, где точность (precision) — доля точных попаданий относительно общего числа предсказаний, а полнота — доля точных попаданий относительно числа связей в размеченных данных.\n\n\nКогда мы в дальнейшем будем приводить оценки, нужно помнить, что используемые метрики затрагивают не только синтаксис, но еще и качество разбиения на токены.\n\nРусский язык в Universal Dependencies\n\nДля того, чтобы парсер смог синтаксически размечать предложения, которых он еще не видел, ему для обучения нужно скормить размеченный корпус. Для русского языка есть несколько таких корпусов:\n\n\n\n\nВо втором столбце указано количество токенов — слов. Чем больше токенов, тем больше обучающий корпус и лучше итоговый алгоритм (если это хорошие данные). Очевидно, что все эксперименты проводятся на SynTagRus (разработка ИППИ РАН), в котором более миллиона токенов. На нем будут обучаться все алгоритмы, о которых пойдет речь дальше.\n\nПарсеры для русского языка в CoNLL Shared Task\n\nПо итогам соревнования прошлого года модели, которые обучались на одном и том же SynTagRus, достигли следующих показателей LAS:\n\n\n\n\nРезультаты парсеров для русского впечатляют — они лучше чем у парсеров для английского, французского и других более редких языков. Нам с вами очень повезло сразу по двум причинам. Во-первых, алгоритмы хорошо справляются с русским языком. Во-вторых, у нас есть SynTagRus — большой и размеченный корпус.\n\n\nКстати, уже прошло соревнование 2018 года, но свое исследование мы проводили весной этого года, так что мы опираемся на итоги дорожки прошлого года. Забегая вперед, заметим, что новая версия UDPipe (Future) оказалась еще выше в этом году.\n\n\nВ список не вошел Syntaxnet — парсер Google. Что с ним не так? Ответ прост: Syntaxnet начинался лишь с этапа морфологического анализа. Он брал готовую идеальную токенизацию, а уже поверх строил обработку. Поэтому оценивать его наравне с остальными нечестно — остальные делали разбиение на токены своими алгоритмами, и это могло ухудшить результаты на последующем этапе синтаксиса. У Syntaxnet образца 2017 года результат лучше, чем у всего списка выше, но проводить сравнение напрямую нечестно.\n\n\nВ таблицу попали две версии UDPipe, на 12 и 15 места. Разработкой этого парсера занимаются те же люди, которые принимали активное участие в самом проекте Universal Dependencies.\n\n\nПериодически появляются обновления UDPipe (несколько реже, кстати, обновляется и разметка корпусов). Так, уже после соревнования прошлого года UDPipe обновлялся (это были коммиты в еще не вышедшую версию 2.0; в дальнейшем для простоты мы будем грубо называть взятый нами коммит UDPipe 2.0, хотя строго говоря это не так); этих обновлений в таблице соревнования, разумеется, нет. Результат «нашего» коммита находится примерно в районе седьмого места.\n\n\n\n\nИтак, нам нужно выбрать парсер для русского языка. В качестве начальных данных у нас есть табличка выше с лидирующим Syntaxnet и с UDPipe 2.0 где-то на 7 месте.\n\nВыбираем модель\n\nДелаем просто: начинаем с парсера с самыми высокими показателями. Если с ним что-то не так, идем ниже. Что-то не так может быть по следующим критериям — может, они не идеальны, но нам подошли:\n\n\nСкорость работы. Наш парсер должен работать достаточно быстро. Синтаксис, разумеется, далеко не единственный модуль «под капотом» real-time системы, поэтому тратить на него больше десятка миллисекунд не стоит.\n\nКачество работы. Как минимум, самого парсера именно на данных русского языка. Требование очевидное. Для русского языка у нас есть достаточно хорошие морфологические анализаторы, которые могут встроиться в нашу пирамиду. Если мы сможем убедиться, что сам парсер без морфологии круто работает, то это нас устроит — морфологию подсунем потом.\n\nНаличие кода обучения и желательно модели в открытом доступе. При наличии кода обучения мы будем способны повторить результаты автора модели. Для этого они должны быть открыты. И, кроме того, нужно внимательно следить за условиями распространения корпусов и модели — придется ли нам, если мы будем их использовать в рамках своих алгоритмов, покупать лицензию на их использование?\n\nЗапуск без сверхусилий. Этот пункт очень субъективный, но важный. Что это значит? Это значит, что если мы три дня сидим и что-то запускаем, а оно не запускается, то выбрать этот парсер мы не сможем, даже если там будет идеальное качество.\n\n\n\nВсе, что в чарте парсеров было выше UDPipe 2.0, нам не подошло. У нас проект на Python, а некоторые парсеры из списка написаны не на Python. Чтобы имплементировать их в питонский проект, пришлось бы применить те самые сверхусилия. В других случаях мы сталкивались с закрытым исходным кодом, академическими, индустриальными разработками — в общем, не докопаешься.\n\n\nЗвездный Syntaxnet заслуживает отдельного рассказа про качество работы. Здесь же он нас не устроил по скорости работы. Время его ответа на какие-то простые, распространенные в чатах фразы составляет от 100 миллисекунд. Если мы столько будем тратить на синтаксис, нам не хватит времени ни на что другое. В то же самое время UDPipe 2.0 делает разбор предложения за ~3ms. В итоге выбор пал на UDPipe 2.0.\n\nUDPipe 2.0\n\nUDPipe — пайплайн, который обучается токенизации, лемматизации, морфологическому тэггингу и парсингу, основанному на грамматике зависимостей. Мы можем обучить его всему этому или чему-то отдельно. Например, сделать с ним еще один морфологический анализатор для русского языка. Или обучить и использовать UDPipe в качестве токенизатора.\n\n\nUDPipe 2.0 подробно задокументирован. Есть описание архитектуры, репозиторий с кодом обучения, мануал. Самое интересное — это готовые модели, в том числе и для русского языка. Качай и запускай. Также на этом ресурсе зарелизены подобранные для каждого языкового корпуса параметры обучения. Для каждой такой модели нужно порядка 60 параметров обучения, и с их помощью можно самостоятельно добиться таких же показателей качества, как в таблице. Они могут быть не оптимальны, но по крайней мере мы можем быть уверены, что пайплайн будет работать достаточно корректно. Кроме того, наличие такого референса позволяет нам спокойно поэкспериментировать с моделью самостоятельно.\n\nКак работает UDPipe 2.0\n\nСначала текст разделяется на предложения, а предложения — на слова. UDPipe делает все это сразу с помощью совместного модуля — нейронной сети (однослойной двухсторонней GRU), которая для каждого символа предсказывает, последний ли он в предложении или в слове.\n\n\nЗатем начинает работу теггер — штука, которая предсказывает морфологические свойства токена: в каком падеже слово стоит, в каком числе. По последним четырем символам каждого слова теггер генерирует гипотезы относительно части речи и морфологических тегов этого слова, а затем при помощи перцептрона отбирает лучший вариант.\n\n\nВ UDPipe есть еще лемматизатор, который подбирает для слов начальную форму. Он обучается примерно по тому же принципу, по которому не-носитель языка мог бы попытаться определить лемму незнакомого ему слова. Отрезаем приставку и конец слова, добавляем какой-нибудь «ть», который присутствует в начальной форме глагола и т.п. Так генерируются кандидаты, из которых наилучшего выбирает перцептрон.\n\n\nСхема морфологического тегирования (определение числа, падежа и всего остального) и предсказания лемм очень похожи. Их можно предсказывать вместе, но лучше раздельно — слишком уж богата морфология русского языка. Можно также подключить свой список лемм.\n\n\nПерейдем к самому интересному — к парсеру. Есть несколько архитектур dependency-парсеров. UDPipe — это transition-based архитектура: она работает быстро, за линейное время проходя по всем токенам один раз.\n\n\nСинтаксический парсинг в такой архитектуре начинается со стека (где в начале только root) и пустой конфигурации. Есть три дефолтных способа ее изменить:\n\n\nLeftArc — применим, если второй элемент стека не root. Сохраняет зависимость между токеном на верхушке стека и вторым токеном, а также выкидывает второй из стека.\n\nRightArc — то же самое, но зависимость строится в другую сторону, и отбрасывается верхушка.\n\nShift — переносит очередное слово из буфера в стек.\n\n\n\nНиже приведен пример работы парсера (источник). У нас есть фраза «book me the morning flight», и мы восстанавливаем связи в ней:\n\n\n\n\nВот что получается в итоге:\n\n\n\n\nУ классических transition-based parser возможны три операции, перечисленные выше: стрелка в одну сторону, стрелка в другую сторону и шифт. Есть еще операция Swap, в базовых архитектурах transition-based парсеров она не используется, но в UDPipe включена. Swap возвращает второй элемент стека в буфер, чтобы взять потом из буфера следующий (в случае если они разнесены). Это помогает пропустить некоторое количество слов и восстановить правильную связь.\n\n\nПо ссылке есть хорошая статья человека, который придумал операцию swap. Выделим из нее один момент: несмотря на то, что мы не один раз проходим по исходному буферу токенов (т.е. наше время уже не линейное), эти операции можно оптимизировать так, чтобы вернуть время очень близко к линейному. То есть перед нами не просто осмысленная с точки зрения языка операция, но еще и инструмент, не сильно замедляющий работу парсера.\n\n\nНа примере выше мы показали операции, в результате которых мы получаем некоторую конфигурацию — буфер токенов и связи между ними. Мы отдаем эту конфигурацию на текущем шаге transition-based парсеру, и с помощью нее он должен предсказать конфигурацию на следующем шаге. Сопоставляя входящие вектора и конфигурации на каждом шаге, модель обучается.\n\n\nИтак, мы отобрали парсер, который подходит под все наши критерии, и даже поняли, как он работает. Переходим к экспериментам.\n\nПроблемы UDPipe\n\nЗададим небольшое предложение: «Переведи маме сто рублей». Результат заставляет схватиться за голову.\n\n\n\n\n«Переведи» оказалось предлогом, но это вполне логично. Мы определяем грамматику словоформы по последним четырем символам. «Веди» — это что-то типа «посреди», так что выбор относительно логичен. С «мамой» поинтереснее: «мама» оказалась в предложном падеже и стала вершиной этого предложения.\n\n\nЕсли пытаться интерпретировать все, исходя из результатов парсинга, то мы получили бы что-то типа «посреди мамы (мамы кого? чья это мама?) сотни рублей». Не совсем то, что было в начале. Нужно как-то с этим бороться. И мы придумали, как.\n\n\nВ пирамиде анализа синтаксис строится поверх морфологии, на основании морфологических тегов. Вот хрестоматийный пример лингвиста Л.В. Щербы на этот счет:\n\n«Глокая куздра штеко будланула бокра и курдячит бокрёнка».\n\n\nАнализ этого предложения не вызывает проблем. Почему? Потому что мы, как теггер UDPipe, смотрим на конец слова и понимаем, к какой оно относится части речи и какая это форма. История с «переведи» в качестве предлога совершенно противоречит нашей интуиции, но оказывается логична в тот момент, когда мы пытаемся проделать то же самое с незнакомыми словами. Человек мог бы подумать точно так же.\n\n\nОценим теггер UDPipe отдельно. Если он нас не устроит, возьмем другой теггер — чтобы потом построить синтаксический парсинг поверх другой морфологической разметки.\n\nTagging from plain text (CoNLL17 F1 score)\n\n\ngold forms: 301639,\n\nupostag: 98.15%,\n\nxpostag: 99.89%,\n\nfeats: 93.97%,\n\nalltags: 93.44%,\n\nlemmas: 96.68%\n\n\n\nКачество морфологии UDPipe 2.0 неплохое. Но для русского языка достижимо лучше. Анализатор Mystem (разработка яндекса) в определении частей речи достигает лучших результатов, чем UDPipe. К тому же, остальные анализаторы сложнее имплементировать в python-проект, и они работают медленней при качестве, сопоставимом с Mystem. Кстати, сравнению морфологических анализаторов для русского языка посвящена пара интересных статей.\n\nМожно попробовать использовать его выходную морфологическую разметку в качестве входа для синтаксического парсера UDPipe. Но есть проблемы. Многие знают, что Mystem не полностью понимает морфологическую омонимию. Он знает, что в предложении «Мама мыла раму» слово «мыла» — от слова «мыть», а не от «мыло». Но нам этого мало. Еще нам нужно, чтобы в словах типа «директора», где лемма абсолютно очевидна (директор), мы понимали, какой это конкретно падеж. Это может быть:\n\n\n«нет директора» — родительный падеж единственного числа\n\n«я вижу директора» — т.е. винительный падеж единственного числа \n\n«это какие-то директора» — именительный падеж множественного числа (ударения-то у нас на письме нет)\n\n\n\nВ таких случаях Mystem честно отдает всю цепочку:\n\nm.analyze(\"нет директора\")\n\n[{'analysis': [{'lex': 'нет', 'gr': 'PART='}], 'text': 'нет'},\n\n{'text': ' '},\n\n{'analysis': [{'lex': 'директор', 'gr': 'S,муж,од=(вин,ед|род,ед|им,мн)'}],\n\n 'text': 'директора'},\n\n{'text': '\\n'}]\n\n\nНо мы не можем подать UDPipe на вход всю цепочку, а должны указать какой-то лучший тег. Как его выбрать? Если ничего не трогать, хочется взять первый, авось сработает. Но теги отсортированы по алфавиту в соответствии с английскими названиями, поэтому наш выбор будет близок к случайному, а некоторые разборы практически лишаются шансов стать первыми.\n\n\nЕсть анализатор, который умеет отдавать лучший вариант, — Pymorphy2. Но с анализом морфологии у него хуже. К тому же, он отдает лучшее слово без учета контекста. Pymorphy2 выдаст только один разбор для «нет директора», «вижу директора» и «директора». Он будет не случайным, а действительно лучшим по вероятностям, которые в pymorphy2 считались на отдельном корпусе текстов. Но некоторый процент неверных разборов боевых текстов будет гарантирован, просто потому что в них вполне могут быть фразы с разными реальным формами: как «вижу директора», так и «директора пришли на встречу», и «нет директора». Бесконтекстная вероятность разбора нам не подходит.\n\n\nКак получить контекстно лучший набор тегов? При помощи анализатора RNNMorph. Про него мало кто слышал, но в прошлом году он выиграл соревнование среди морфологических анализаторов, проводившееся в рамках конференции «Диалог».\n\n\nУ RNNMorph возникает своя проблема: у него нет токенизации. Если Mystem умеет токенизировать сырой текст, то RNNMorph требует на входе список токенов. Чтобы доехать до синтаксиса, придется сначала применить какой-то внешний токенизатор, потом отдать результат RNNMorph и только потом полученную морфологию скормить синтаксическому парсеру. \n\n\nВот какие варианты у нас есть. Не будем пока отказываться от бесконтекстного разбора в pymorphy2 поверх спорных случаев в Mystem — вдруг он от RNNMorph отстанет не сильно. Хотя если сравнивать их чисто на уровне качества морфологической разметки (данные с MorphoRuEval-2017), то проигрыш получается значительный — порядка 15%, если считать accuracy по словам.\n\nДальше нам нужно конвертировать выдачу Mystem в тот формат, который понимает UDPipe, — conllu. И это опять проблема, даже целых две. Чисто техническая — строки не совпадают. И концептуальная — не всегда до конца понятно, как их сопоставить. Сталкиваясь с двумя разными разметками языковых данных, вы почти наверняка упретесь в проблему соответствия тегов, см. примеры ниже. Ответы на вопрос «какой тег здесь правильный» могут быть разные, и, вероятно, правильный ответ зависит от задачи. Из-за такой непоследовательности сопоставление систем разметок — само по себе непростая задача.\n\n\nКак конвертировать? Есть russian_tagsets_package — пакет для Python, который умеет конвертировать разные форматы. Там нет перевода из формата выдачи Mystem в Conllu, который принят в Universal Dependencies, но зато есть перевод в conllu, например, из формата разметки национального корпуса русского языка (и обратно). Автор пакета (кстати, он же автор pymorphy2) прямо в документации написал замечательную вещь: «Если вы можете не пользоваться этим пакетом, не пользуйтесь им». Он сделал это не потому, что криворукий программист (он превосходный программист!), а потому что если вам надо конвертировать одно в другое, то вы рискуете получить проблемы из-за лингвистического несоответствия конвенций разметок.\n\n\nВот пример. В школе учили «категории состояния» (холодно, нужно). Одни говорят — это наречие, другие — прилагательное. Вам нужно это конвертировать, и вы дописываете какие-то правила, но все равно не добиваетесь однозначного соответствия между одним форматом и другим.\n\n\nДругой пример: залог (либо кто-то что-то делал, либо с кем-то что-то сделали). «Петя кого-то убил» или «Петя был убит». «Вася фотографирует» — «Вася фотографируется» (т.е. на самом деле «Васю фотографируют»). В SynTagRus есть еще медиальный залог — не будем даже углубляться в то, что это и почему. А в Mystem его нет. Если нужно как-то один формат привести к другому, это тупик.\n\n\nМы более-менее честно воспользовались советом автора пакета russian_tagsets — не использовали его разработку, потому что не нашли нужной пары в списке соответствий форматов. В итоге мы написали свой кастомный конвертер из Mystem в Conllu и поехали дальше. \n\nСоединяем сторонний теггер и парсер UDPipe\n\nПосле всех приключений мы взяли три алгоритма, про которые рассказывали выше:\n\n\nbaseline UDPipe\n\nMystem с дизамбигуацией тегов от pymorphy2\n\nRNNMorph\n\n\n\n\n\n\nМы потеряли в качестве по довольно понятной причине. Мы брали модель UDPipe, обученную на одной морфологии, но подсовывали на вход другую морфологию. Классическая проблема несоответствия данных трейна и теста — отсюда и падение качества.\n\n\nМы пытались наши автоматические инструменты морфологической разметки привести в соответствие с разметкой SynTagRus, который размечен вручную. У нас не получилось, поэтому в обучающем корпусе SynTagRus мы заменим всю ручную морфологическую разметку на полученную из Mystem и pymorphy2 в одном случае и из RNNMorph в другом. В размеченном руками валидированном корпусе мы вынуждены менять ручную разметку на автоматическую, потому что «в бою» мы никогда не получим ручную разметку.\n\n\nВ итоге мы обучили парсер UDPipe (только парсер) с такими же гиперпараметрами, как у baseline. То, что отвечало за синтаксис, — ID вершины, от кого зависит и тип связи — мы оставили, все остальное поменяли.\n\nРезультаты\n\nДальше буду сравнивать нас с Syntaxnet и остальными алгоритмами. Организаторы CoNLL Shared Task зарелизили разбиение SynTagRus (train/dev/test 80/10/10). Мы изначально взяли другое (train/test 70/30), поэтому у нас не всегда совпадают данные, хотя они и получены на том же корпусе. Кроме того, мы взяли последний (по состоянию на февраль-март) релиз из репозитория SynTagRus — эта версия немного отличается от той, что была на соревновании. Данные для того, что у нас не взлетело, приводятся по статьям, где сплит был такой же, как на соревновании — такие алгоритмы помечены в таблице звездочкой.\n\n\nВот итоговые результаты:\n\n\n\nRNNMorph действительно оказался лучше — не в абсолютном смысле, а в роли вспомогательного инструмента для получения общей метрики по итогам синтаксического анализа (по сравнению с Mystem + pymorphy2). То есть чем лучше морфология, тем лучше синтаксис, но при этом «синтаксический» отрыв значительно меньше морфологического. Заметим также, что мы не очень далеко уехали от baseline-модели, а значит, в морфологии на самом деле лежало не так много, как мы предполагали.\n\n\nИнтересно, много ли вообще лежит на морфологии? Можно ли добиться принципиального улучшения синтаксического парсера за счет идеальной морфологии? Чтобы ответить на этот вопрос, мы прогнали UDPipe 2.0 на идеально выверенной (на стандарте ручной разметки) токенизации и морфологии. Получился некоторый отрыв (см. в таблице строчку про Gold Morph; получается +1.54% от RNNMorph_reannotated_syntax) от того, что было у нас, в том числе, с точки зрения верного определения типа связи. Если кто-то когда-то напишет абсолютно идеальный морфологический анализатор русского языка, вероятно, результаты, которые мы получим, используя абстрактный синтаксический парсер, тоже вырастут. И мы примерно понимаем потолок (по крайней мере, потолок по той архитектуре и по тому сочетанию параметров, которое мы использовали для UDPipe, — он приведен в третьей строке таблицы выше).\n\n\nИнтересно, что мы почти дотянулись по метрике LAS до версии Syntaxnet. Понятно, что у нас немного разные данные, но в принципе все равно сопоставимо. У Syntaxnet токенизация «золотая», а у нас — от Mystem. Мы написали вышеупомянутую обертку к Mystem, но разбор все равно проходит автоматически; вероятно, Mystem тоже где-то ошибается. Из строчки таблицы «UDPipe 2.0 gold tok» видно, что если взять дефолтный UDPipe и золотую токенизацию, то он все равно немного проигрывает Syntaxnet-2017. Но зато работает намного быстрее.\n\n\nДо чего не дотянулся никто, так это до стенфордского парсера. Он устроен так же, как Syntaxnet, поэтому работает долго. В UDPipe мы идем просто по стеку. В архитектуре стенфордского парсера и Syntaxnet заложена другая концепия: сначала они генерируют полный ориентированный граф, и дальше работа алгоритма состоит в том, чтобы оставить тот скелет (минимальное остовное дерево), который будет наиболее вероятным. Для этого он перебирает комбинации, а этот перебор уже не линеен, потому что к одному слову вы будете обращаться не единожды. Несмотря на то, что это долго, с точки зрения чистой науки, по крайней мере для русского языка, — это более эффективная архитектура. Мы пытались поднять эту академическую разработку два дня — увы, не получилось. Но исходя из его архитектуры понятно, что работает он не быстро.\n\n\nЧто касается нашего подхода — хотя мы формально по метрикам почти не поднялись, зато у нас теперь с «мамой» все в порядке.\n\n\n\n\nВо фразе «переведи маме сто рублей» у нас «переведи» — действительно глагол в повелительном наклонении. «Мама» получила свой дательный падеж. И самое главное для нас — наша метка (iobj) косвенный объект (адресат). Хотя прирост по цифрам незначительный, мы хорошо справились с той проблемой, с которой задача начиналась.\n\nBonus track: пунктуация\n\nЕсли вернуться к реальным данным, то выясняется, что синтаксис зависит от пунктуации. Возьмем фразу «казнить нельзя помиловать». Что именно нельзя — «казнить» или «помиловать» — зависит от того, где стоит запятая. Даже если мы посадим лингвиста размечать данные, ему нужна будет пунктуация как некоторый вспомогательный инструмент. Без нее он не справится.\n\n\nВозьмем фразы «Петя привет» и «Петя, привет» и посмотрим на их разбор baseline-UDPipe моделью. Оставим за скобками проблемы того, что, если верить этой модели, то:\n\n1) «Петя» — это существительное женского рода;\n\n2) «Петя» — это (судя по набору тегов) начальная форма, но при этом лемма у него якобы не «Петя».\n\n\nВот как меняется результат из-за запятой, с ее помощью мы получаем нечто похожее на правду.\n\n\n\n\nВо втором случае «Петя» — это субъект, а «привет» — это глагол. Вернемся к предсказанию формы слова на основании последних четырех символов. В трактовке алгоритма это не «Петя привет», а «Петя привёт». Типа «Петя поет» или «Петя придет». Разбор довольно понятный: в русском языке запятой между подлежащим и сказуемым быть не может. Поэтому если запятая стоит, это слово «привет», а если запятой нет, это вполне может быть что-то вроде «Петя привёт».\n\n\nСталкиваться с этим на продакшене мы будем достаточно часто, потому что орфографию спеллчекеры исправят, а пунктуацию — нет. Что еще хуже, пользователь может неправильно ставить запятые, и наш алгоритм будет учитывать их в понимании естественного языка. Какие здесь есть возможные решения? Мы видим два варианта.\n\n\nПервый вариант — сделать так, как иногда делают при переводе речи в текст. Изначально в таком тексте нет никакой пунктуации, поэтому она восстанавливается через модель. На выходе получается относительно грамотный с точки зрения правил русского языка материал, который помогает корректно работать синтаксическому парсеру.\n\n\nВторая идея несколько более смелая и противоречащая школьным урокам русского языка. Она предполагает работу без пунктуации: если вдруг входные данные будут с пунктуацией, мы ее оттуда уберем. Из обучающего корпуса тоже удалим абсолютно всю пунктуацию. Будем считать, что русский язык существует без пунктуации. Только точки для разделения на предложения.\n\n\nТехнически это довольно просто, потому что мы никак не меняем в синтаксическом дереве неконечные узлы. У нас не может быть такого, что знак пунктуации — это вершина. Это всегда некоторый конечный узел, кроме знака %, который почему-то в SynTagRus является вершиной для предшествующего числительного (50% в SynTagRus размечено как % — вершина, а 50 — зависимое).\n\n\nПроведем тесты с использованием Mystem (+pymorphy 2) модели.\n\n\n\n\nНам критически важно не дать модели без пунктуации текст с пунктуацией. Но зато если мы всегда будем давать текст без пунктуации, то мы будем оказываться в рамках верхней строчки и получать как минимум приемлемые результаты. Если текст без пунктуации и модель будет работать непунктуационная, то относительно идеальной пунктуации и пунктуационной модели получится падение всего примерно в 3%.\n\n\nЧто с этим делать? Мы можем остановиться на этих цифрах — полученных с помощью беспунктуационной модели и чистки пунктуации. Или придумать какой-то классификатор, чтобы восстанавливать пунктуацию. Идеальных цифр (тех, что с пунктуацией на пунктуационной модели) мы уже не добьемся, потому что алгоритм восстановления пунктуации работает с некоторой погрешностью, а «идеальные» цифры были рассчитаны на абсолютно чистом SynTagRus. Но если мы будем писать модель, восстанавливающую пунктуацию, окупит ли прогресс наши затраты? Ответ пока неочевиден.\n\n\nМы можем долго размышлять про архитектуру парсера, но должны помнить, что на самом деле пока нет большого синтаксически размеченного корпуса веб-текстов. Его существование помогло бы лучше решать реальные задачи. Пока что мы учимся на корпусах абсолютно грамотных, отредактированных текстов — и теряем в качестве, получая на бою пользовательские тексты, которые зачастую написаны неграмотно.\n\nЗаключение\n\nМы рассмотрели использование различных алгоритмов синтаксического парсинга, основанного на грамматике зависимостей, применительно к русскому языку. Оказалось, что с точки зрения скорости, удобства и качества работы наилучшим инструментом оказался UDPipe. Его baseline-модель можно улучшить, если отдать этапы токенизации и морфологического анализа на откуп другим, сторонним, анализаторам: такой трюк даёт возможность исправлять некорректное поведение теггера и, как следствие, парсера в важных для анализа случаях.\n\n\nТакже мы проанализировали проблему взаимосвязи пунктуации и парсинга и пришли к выводу, что в нашем случае пунктуацию перед синтаксическим парсингом лучше удалять.\n\n\nНадеемся, что прикладные моменты, разобранные в нашей статье, помогут использовать синтаксический парсинг для решения ваших задач максимально эффективно.\n\nАвтор благодарит за помощь в подготовке статьи Никиту Кузнецова и Наталью Филиппову; за помощь в исследовании — Антона Алексеева, Никиту Кузнецова, Андрея Кутузова, Бориса Орехова и Михаила Попова.    Tags: парсерыкомпьютерная лингвистикасинтаксисUDPipedependency parsingnlp Hubs: Сбер corporate blogProgrammingMachine learningArtificial Intelligence"}, {"role": "assistant", "content": "[['синтаксические парсеры', 'синтаксические парсеры', 'синтаксический парсер', 'Синтаксические парсеры', 'парсера', 'парсер', 'который', 'парсер', 'парсера', 'парсер', 'он', 'ему', 'Парсеры', 'парсеров', 'парсеров', 'парсер', 'парсер', 'парсера', 'ним', 'парсер', 'самого парсера', 'сам парсер', 'этот парсер', 'парсеров', 'парсеры', 'парсеру', 'парсера', 'парсера', 'парсер', 'который', 'он', 'синтаксического парсера', 'синтаксическому парсеру', 'парсер', 'парсер', 'синтаксического парсера', 'синтаксический парсер', 'синтаксическому парсеру', 'парсера', 'парсера'], ['русского языка', 'русского языка', 'русским языком', 'русского языка', 'русского языка', 'русском языке', 'Русский язык', 'русского языка', 'для русского языка', 'русского', 'русским языком', 'русского языка', 'русского языка', 'русского языка', 'русского языка', 'русского языка', 'русского языка', 'русского языка', 'русского языка', 'русского языка', 'русского языка', 'русском языке', 'русского языка', 'русского языка', 'русский язык', 'русскому языку'], ['Меня', 'Денис Кирьянов', 'я', 'Автор'], ['нам', 'мы', 'Мы', 'Нам', 'мы', 'нас', 'мы', 'нам', 'своем', 'себе', 'мы', 'свой', 'мы', 'нас', 'Мы', 'мы', 'нас', 'мы', 'мы', 'нашем', 'мы', 'Нам', 'свое', 'мы', 'мы', 'мы', '«нашего»', 'нам', 'нас', 'нам', 'Наш', 'нас', 'мы', 'нас', 'мы', 'нам,', 'мы', 'своих', 'мы', 'мы', 'нам', 'нас', 'мы', 'нас', 'мы', 'нам', 'Мы', 'мы', 'нам', 'нас', 'мы', 'мы', 'наше', 'мы', 'Мы', 'мы', 'наши', 'Мы', 'мы', 'мы', 'мы', 'нашей', 'мы', 'нам', 'нам', 'мы', 'нас', 'мы', 'наш', 'нам', 'нас', 'нам', 'Мы', 'мы', 'мы', 'Мы', 'Мы', 'Мы', 'наши', 'нас', 'мы', 'мы', 'мы', 'мы', 'мы', 'нас', 'Мы', 'нас', 'мы', 'нас', 'мы', 'мы', 'мы', 'нас', 'мы', 'мы', 'мы', 'мы', 'нас', 'Мы', 'мы', 'Мы', 'нашего', 'мы', 'нас', 'нас', 'наша', 'мы', 'мы', 'мы', 'мы', 'наш', 'Мы', 'мы', 'мы', 'нас', 'Нам', 'мы', 'мы', 'Мы', 'мы', 'мы', 'наши', 'Мы', 'мы', 'Мы', 'мы'], ['морфологии', 'морфологии', 'морфологии', 'морфологии', 'морфологию', 'морфология', 'морфологии', 'морфологии', 'морфологию', 'морфологии', 'морфологию', 'морфология', 'морфологии', 'морфологии', 'морфологии', 'морфологии'], ['токенизации', 'токенизацию', 'токенизации', 'токенизации', 'разбиения на токены', 'токенизацию', 'разбиение на токены', 'токенизации', 'токенизации', 'токенизации', 'токенизация', 'токенизацию', 'токенизации'], ['разные варианты', 'их'], ['Грамматика зависимостей', 'грамматики зависимостей', 'Этому подходу', 'грамматика зависимостей', 'грамматики зависимостей', 'грамматике зависимостей', 'грамматики зависимостей', 'грамматике зависимостей', 'грамматике зависимостей'], ['грамматика непосредственных составляющих', 'грамматикой непосредственно составляющих', 'ей', 'Второй подход', 'нем'], ['предложение', 'предложении'], ['«Мыла»', '«мыла»', 'слово «мыла»', 'него', 'его'], ['сказуемое', 'сказуемое', 'сказуемого', 'сказуемое', 'сказуемое', 'сказуемым'], ['подлежащее', ' подлежащего', 'подлежащего', 'подлежащее', 'подлежащее', 'субъект', 'подлежащим'], ['«мама»', '«мама»', '«Мама»'], ['определение', 'определение'], ['«моя»', '«моя»'], ['прямое дополнение', 'прямого дополнения'], ['«раму»', '«раму»', '«раму»'], ['«грязную»', '«грязную»'], ['группы слов', 'группы', 'группы'], ['автоматического парсинга', 'автоматическом парсинге', 'синтаксического парсинга', 'автоматического парсинга', 'парсингу', 'Синтаксический парсинг', 'парсинга', 'синтаксический парсинг', 'синтаксического парсинга', 'парсинга', 'синтаксическим парсингом', 'синтаксический парсинг'], ['Дерево зависимостей', 'дерева зависимостей', 'Dependency tree', 'Дерево зависимостей', 'дереве', 'дереве', 'синтаксическом дереве'], ['root node', 'the root node', 'the root node', 'верхнеуровневый узел', 'него'], ['arcs', 'arc', 'ребра', 'ребро'], ['«is taken»', '«is taken»'], ['связи', 'они', 'их', 'синтаксические связи'], ['Проект Universal Dependencies', 'проект Universal Dependencies', 'его', 'Universal Dependencies', 'Universal Dependencies', 'проекта Universal Dependencies', 'Universal Dependencies', 'Universal Dependencies', 'Universal Dependencies'], ['соревнование CoNLL Shared Task', 'CoNLL Shared Task', 'соревнования', 'соревнования', 'CoNLL Shared Task', 'CoNLL Shared Task', 'соревновании', 'соревновании'], ['корпусов', 'корпусы', 'которые', 'корпуса', 'корпусах'], ['трибанков', 'трибанков', 'трибанков', 'трибанков'], ['английском', 'там'], ['артикль', 'который'], ['создателям Universal Dependencies', 'собой', 'их', 'они'], ['алгоритмов синтаксического парсинга', 'алгоритма', 'алгоритм', 'его'], ['Организаторы', 'организаторам', 'которые', 'Организаторы'], ['обучающую', 'Первая часть', 'ней'], ['валидационную', 'Вторая часть'], ['тестовую', 'тестовой части', 'тестовых частях'], ['участникам соревнования', 'они', 'свои', 'участниками', 'участники', 'они', 'участников'], ['модели', 'моделей'], ['обучения', 'Обучение', 'обучения'], ['Метрики', 'метрика', 'метрика', 'метрики'], ['вершину', 'вершина', 'вершину', 'вершина', 'вершиной'], ['метрика UAS (Unlabeled attachment score)'], ['метрика LAS (Labeled attachment score)', 'LAS', 'метрике LAS'], ['оценка точности (accuracy)', 'accuracy'], ['текст', 'который'], ['этапы токенизации и морфологического анализа', 'этих более ранних этапов'], ['морфологического анализа', 'морфологического анализа', 'морфологического анализа'], ['слова-юниты', 'ними', 'юниты'], ['размеченном корпусе', 'размеченный корпус', 'таких корпусов', 'корпус', 'размеченный корпус', 'корпусов', 'размеченного корпуса'], ['доля точных попаданий', 'доля точных попаданий'], ['синтаксис', 'синтаксиса', 'Синтаксис', 'него', 'он', 'синтаксис', 'синтаксис', 'синтаксиса', 'синтаксис', 'синтаксис', 'синтаксис'], ['токены', 'токенов', 'токенов', 'токенов', 'токены', 'токена', 'токенам', 'токеном', 'токеном', 'токенов', 'токенов', 'ними', 'токенов'], ['предложения', 'которых'], ['SynTagRus', 'котором', 'нем', 'SynTagRus', 'SynTagRus', 'SynTagRus', 'SynTagRus', 'который', 'SynTagRus', 'SynTagRus', 'том же корпусе', 'SynTagRus', 'SynTagRus', 'SynTagRus', 'SynTagRus'], ['алгоритмы', 'которых'], ['соревнования прошлого года', 'соревнования прошлого года'], ['модели', 'которые'], ['Результаты парсеров для русского', 'они'], ['вами', 'вы', 'вы', 'вам', 'вы', 'Вам', 'вы', 'вы', 'ваших'], ['UDPipe', 'UDPipe', 'этого парсера', 'UDPipe', 'UDPipe', 'UDPipe', 'его', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'парсер UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'UDPipe', 'Его'], ['Syntaxnet', 'ним', 'Syntaxnet', 'Он', 'его', 'Syntaxnet', 'Syntaxnet', 'Syntaxnet', 'его', 'Syntaxnet', 'Syntaxnet', 'Syntaxnet', 'Syntaxnet', 'Syntaxnet', 'Syntaxnet', 'они'], ['UDPipe 2.0', '«нашего» коммита', 'UDPipe 2.0', 'UDPipe 2.0', 'UDPipe 2.0', 'UDPipe 2.0.', 'UDPipe 2.0', 'UDPipe 2.0', 'этом ресурсе', 'UDPipe 2.0', 'UDPipe 2.0', 'UDPipe 2.0', 'UDPipe 2.0', 'он'], ['таблице соревнования', 'табличка выше', 'чарте парсеров', 'списка'], ['следующим критериям', 'они'], ['морфологические анализаторы', 'которые', 'морфологический анализатор', 'морфологических анализаторов', 'морфологический анализатор'], ['кода обучения', 'кода обучения'], ['модели', 'модели', 'модели', 'их', 'их'], ['результаты автора модели', 'они'], ['сверхусилий', 'те самые сверхусилия'], ['что-то', 'оно'], ['Python', 'Python', 'python', 'Python'], ['пайплайн', 'который', 'пайплайн'], ['готовые модели', 'такой модели'], ['60 параметров обучения', 'их', 'Они'], ['нейронной сети (однослойной двухсторонней GRU)', 'которая'], ['символа', 'он'], ['теггер', 'теггер', 'теггер', 'теггер', 'он', 'теггер', 'теггер', 'теггера'], ['штука', 'которая'], ['слова', 'этого слова'], ['перцептрона', 'перцептрон'], ['лемматизатор', 'который', 'Он'], ['начальную форму', 'начальной форме'], ['принципу', 'которому'], ['не-носитель языка', 'ему'], ['лемму', 'лемм', 'лемм', 'лемма', 'лемма'], ['«ть»', 'который'], ['кандидаты', 'которых'], ['Схема морфологического тегирования (определение числа, падежа и всего остального) и предсказания лемм', 'Их'], ['dependency-парсеров'], ['transition-based архитектура', 'она', 'такой архитектуре', 'transition-based', 'transition-based', 'transition-based'], ['стека', 'стека', 'стека', 'стека', 'стек', 'стека', 'стеку'], ['root', 'root'], ['пустой конфигурации', 'ее'], ['зависимость', 'зависимость'], ['верхушке стека', 'верхушка'], ['вторым токеном', 'второй'], ['буфера', 'буфер', 'буфера', 'буферу', 'буфер'], ['фраза «book me the morning flight»', 'ней'], ['transition-based parser', 'transition-based парсеров', 'transition-based парсеру', 'он'], ['операция Swap', 'она', 'Swap', 'операцию swap'], ['следующий', 'они'], ['хорошая статья человека', 'нее'], ['человека', 'который'], ['операции', 'которых'], ['некоторую конфигурацию', 'эту конфигурацию', 'нее'], ['«Переведи маме сто рублей»', 'этого предложения', '«переведи маме сто рублей»'], ['«Переведи»', '«переведи»'], ['предлогом', 'предлога'], ['«Глокая куздра штеко будланула бокра и курдячит бокрёнка»', 'этого предложения'], ['слова', 'оно'], ['Анализатор Mystem', 'Mystem', 'его', 'Mystem', 'Он', 'Mystem', 'Mystem', 'Mystem', 'Mystem', 'Mystem', 'Mystem', 'Mystem', 'Mystem', 'Mystem', 'Mystem', 'Mystem', 'Mystem', 'Mystem', 'Mystem'], ['остальные анализаторы', 'они'], ['«нет директора»', '«нет директора»', '«нет директора»'], ['падеж', 'падеж', 'падеж', 'падеж'], ['числа', 'числа', 'числа'], ['всю цепочку', 'всю цепочку'], ['лучший тег', 'его'], ['разборы', 'разбор', 'Он', 'разборов', 'разбора', 'разбор', 'разбор', 'Разбор'], ['Pymorphy2', 'он', 'Pymorphy2', 'pymorphy2', 'pymorphy2', 'pymorphy2', 'pymorphy2', 'pymorphy2', 'pymorphy 2'], ['«вижу директора»', '«вижу директора»'], ['текстов', 'них'], ['анализатора RNNMorph', 'него', 'он', 'RNNMorph', 'своя', 'него', 'RNNMorph', 'RNNMorph', 'pymorphy2', 'RNNMorph', 'их', 'RNNMorph', 'RNNMorph', 'RNNMorph'], ['бесконтекстного разбора', 'он'], ['conllu', 'Conllu', 'который', 'conllu', 'Conllu'], ['russian_tagsets_package', 'Там', 'пакета', 'этим пакетом', 'russian_tagsets'], ['пакет для Python', 'который'], [' Автор пакета', 'он', 'автор pymorphy2', 'Он', 'он', 'автора пакета', 'его'], ['медиальный залог', 'его'], ['три алгоритма', 'которые'], ['baseline', 'baseline', 'baseline-модели', 'baseline', 'baseline-модель'], ['морфологической разметки', 'морфологическую разметку', 'разметку', 'разметку'], ['данные', 'они'], ['сочетанию параметров', 'которое'], ['стенфордского парсера', 'Он', 'стенфордского парсера', 'эту академическую разработку', 'его', 'он'], ['алгоритма', 'он'], ['скелет', 'который'], ['той проблемой', 'которой'], ['пунктуация', 'пунктуации', 'пунктуация', 'нее', 'пунктуацию', 'пунктуации', 'она', 'пунктуации', 'пунктуацией', 'ее', 'пунктуацию', 'пунктуации', 'пунктуации', 'пунктуации', 'пунктуацией', 'пунктуации', 'пунктуации', 'пунктуации', 'пунктуации', 'пунктуацию', 'пунктуацией', 'пунктуации', 'пунктуацию', 'пунктуации', 'пунктуацию'], ['запятая', 'запятой', 'ее', 'запятой', 'запятая', 'запятой', 'запятые', 'их'], ['лингвиста', 'ему', 'он'], ['фразы «Петя привет» и «Петя, привет»', 'их'], ['Петя', 'Петя', '«Петя»', '«Петя»', 'него', '«Петя»', '«Петя»', 'Петя', 'Петя', 'Петя', 'Петя', 'Петя'], ['материал', 'который'], ['Вторая идея', 'Она'], ['знака %', 'который'], ['%'], ['вершина'], ['модели', 'модель', 'модели', 'модели', 'модели', 'модель'], ['Идеальных цифр', '«идеальные» цифры'], ['большого синтаксически размеченного корпуса веб-текстов', 'Его'], ['пользовательские тексты', 'которые'], ['Никиту Кузнецова', 'Никиту Кузнецова']]"}]}
{"messages": [{"role": "system", "content": "Ты являешься экспертом в задаче решения референций из текстов на русском языке."}, {"role": "user", "content": "**Ты решаешь задачу разрешения кореференций (coreference resolution) в текстах на русском языке с помощью пошагового рассуждения (Chain-of-Thought).**\n\nСледуй указанным шагам:\n\n---\n\n### \uD83D\uDD39 Шаг 1 (Извлечение упоминаний):\n\n- Прочитай текст.\n- Выпиши все упоминания сущностей (людей, компаний, продуктов, сервисов и т.д.), а также местоимения и фразы, которые могут на них ссылаться.\n- Учитывай не только имена собственные, но и абстрактные понятия (например, \"база знаний\", \"информация\", \"уязвимости\"), если на них ссылаются в тексте.\n- Постарайся различать случаи, когда одно и то же слово может обозначать разные вещи в зависимости от контекста (например, \"ИИ\" может означать как технологию, так и конкретного помощника — Atlassian Intelligence).\n\n---\n\n### \uD83D\uDD39 Шаг 2 (Определение кореференций и группировка):\n\n- Определи, какие из упоминаний относятся к одной и той же сущности.\n- Учитывай грамматические признаки (род, число, падеж), контекст и смысл текста.\n- Если возникают сомнения, выбери наиболее вероятную связь. В сложных случаях можешь кратко пояснить причину (1–2 слова).\n- Сгруппируй упоминания по кластерам — каждому кластеру соответствует одна сущность.\n\n---\n\n### \uD83D\uDD39 Шаг 3 (Итоговый результат):\n\n- Верни только результат в виде массива массивов.\n- Каждый внутренний массив — отдельный кластер кореференций.\n- Не добавляй комментариев, пояснений или шагов рассуждения к итоговому выводу.\n\n---\n\n### \uD83D\uDCCC Пример:\n\n**Текст:**\n\n«Мария живет в Москве. Она любит этот город за его атмосферу.»\n\n**Ответ:**\n\n```json\n[\n  [\"Мария\", \"Она\"],\n  [\"Москве\", \"этот город\", \"его\"]\n]\n```\n\n---\n\nТеперь выполни аналогичный анализ для следующего текста, соблюдая описанные шаги рассуждения:\n\n`\"\n\nОбзор современных инструментов дата-аналитика / Habr\n\n\n              16  February  2021 at 18:16  Обзор современных инструментов дата-аналитика Data Mining *Data visualization *Machine learning *Artificial Intelligence       \n\n\nСразу уточню, что видов аналитиков очень много, так как анализировать можно все что угодно. Это и веб-аналитики, и классические data scientists, и бизнес-аналитики, и финансовые аналитики, а также продуктовые, системные и UX аналитики. Причина такого разнообразия, по-видимому, в том, что в ряде крупных компаний над созданием одной платформы или продукта одновременно могут работать десятки, а то и сотни программистов и аналитиков. В таких условиях происходит сильное сужение специализации.\n\n\nВсе перечисленные виды аналитиков используют свои специфические наборы инструментов. Поэтому сосредоточусь только непосредственно на сфере анализа данных вне контекста происхождения этих самых данных. Таким образом мы исключаем из обзора системы веб-аналитики, CRM, ERP, системы складского учета, управления логистикой и документооборотом.\n\n1. Языки программирования\n\nНе будем касаться исключительных, уникальных или редких случаев. Рассмотрим все только самое популярное. И конечно же, в первую очередь, это язык python.\n\nPython\n\n\nPython служит главным инструментом в руках data scientists, не имеет строгой типизации и предназначен для быстрой разработки прототипов или написания коротких сценариев или скриптов. Люди разбирающиеся в программировании и computer science его часто критикуют за то, что алгоритмы написанные на чистом python оказываются не оптимальными в отношении своей производительности и требованиям к памяти.\n\n\nНо тем не менее у данного языка программирования есть много плюсов. Среди них я бы отметил то, что python преподают уже практически везде, в связи с чем сравнительно легко найти аналитика знающего python. Второе преимущество — это библиотеки для работы с данными и машинного обучения, имеющие удобный интерфейс. Например, на основе библиотеки sklearn легко собирать конвейеры предварительной обработки данных и построения моделей. Все алгоритмы и настройки машинного обучения инкапсулированы внутри классов и объектов, что делает код очень простым.\n\nR\n\n\nДо недавнего времени основным конкурентом python был язык R. Пожелания к знанию R и сейчас изредка встречаются в описаниях вакансий по крайней мере в разделе «преимущества». До середины 2018-го года я и сам программировал на R. И при попытке автоматизировать часть своей работы по машинному обучению чуть не изобрел велосипед, пытаясь на R создать конвейеры подготовки данных и обучения моделей. Чуть позже узнал, что такие конвейеры уже давно существуют в библиотеке sklearn и называются pipeline.\n\nC++, C#\n\n\nЕсли существующих библиотек на python недостаточно и требуется реализовать новый алгоритм с высокой производительностью, к вашим услугам компилируемый и статически типизированный язык C++ или похожий на него язык C#.\n\nMatLab\n\n\nЯзык MatLab встроен в одноименный пакет программ и интерактивную среду инженерных расчетов. Правда предназначен данный язык в большей степени для решения технических задач, а не для выполнения финансового или бизнес-анализа. Например, мне посчастливилось применять MatLab дважды: в процессе исследования сигналов акустической эмиссии в конструкциях, а также при обработке человеческой речи.\n\n\nСуществует ряд библиотек машинного обучения с API для других языков программирования, таких как Java, JavaScript, Scala и т.д. Но останавливаться на них не буду поскольку цель статьи немного иная.\n\n\nПрошу немного потерпеть. Обо всем вы узнаете в следующих разделах.\n\n2. AutoML и визуальные конструкторы\n\nAutoML согласно своей основной идее резко упрощает задачу исследователя и сводит несколько шагов по изучению и подготовке данных, конструированию признаков, выбору и сравнению алгоритма машинного обучения и настройке гиперпараметров к одному единственному шагу. И этот шаг заключается в выборе и настройке одного большого ящика под названием AutoML. Результатом запуска алгоритма AutoML является сконструированный и соответствующим образом настроенный и обученный pipeline. Остается только брать «сырые» данные, подсовывать их в pipeline и ждать на выходе результат в виде прогнозов.\n\n\nЯщик под названием «AutoML» выглядит либо как библиотека машинного обучения, либо как веб-сервис куда заливаются данные.\n\n\nЕсли это библиотека, то она отличается от sklearn тем, что наш привычный код в 20-30 строк сжимается до 5 строк. Известный пример такой библиотеки H2O.\n\n\nДругой пример — библиотека MLBox. Про нее в интернете можно найти истории, о том как применение MLBox позволило попасть в топовые 5% на соревнованиях kaggle.\n\n\nТеперь несколько слов об облачных сервисах AutoML. Во первых, свои технические решения спешат представить все основные цифровые гиганты. Вот некоторые из них: Google AutoML Tables, Azure Machine Learning (Microsoft), SageMaker Autopilot (Amazon). Перечисленные сервисы должны быть интересны в первую очередь тем компаниям, которые разрабатывают аналитические системы на облачных платформах. Очень удобно, когда и инфраструктуру данных, и вычислительные ресурсы, и готовые алгоритмы машинного обучения предоставляет один и тот же провайдер. Интеграция получается поистине бесшовной.\n\n\nПомимо цифровых гигантов на рынке AutoML появляются и игроки поменьше. Например, непосредственно в настоящий момент в компании Bell Integrator идет активная работа над платформой neuton.ai.\n\n\nВ этом же разделе стоит вспомнить про системы машинного обучения, занимающие промежуточные позиции между непосредственным программированием на R и Python и полностью упакованным в коробку AutoML. Это так называемые конструкторы workflow. Два типичных примера: конструктор машинного обучения Azure от Microsoft и платформа SberDS Сбербанка.\n\n\nКонструктор представляет собой набор кубиков, из которых можно собрать весь конвейер машинного обучения, включая финальную проверку работоспособности модели. Это несомненно красивое решение для людей с визуальным типом мышления, которым удобно представлять процесс машинного обучения и тестирования моделей в виде схем.\n\n3. Инструменты BI\n\nЗдесь бы я хотел рассмотреть несколько BI решений в области аналитики: Power BI, Tableau, Qlik Sense, QlikView и Excel.\n\nPower BI\n\n\nPower BI — это набор аналитических инструментов от Microsoft, которые доступны в виде десктопных приложений и облачных сервисов. Существуют корпоративные решения, работающие на закрытой it-инфраструктуре компании. Работа в Power BI Desktop или Power BI Services не требует навыков программирования. Предусмотрена возможность онлайн-интеграции с внешними источниками данных, а также загрузка данных в формате csv.\n\n\nPower BI способен решать задачи машинного обучения посредством AutoML, то есть для построения модели классификации или регрессии писать программный код как на питоне не придется. Кроме стандартных задач анализа табличных данных в функционал встроены технологии анализа тональности, извлечения ключевых фраз, распознавания языка и добавления тегов к изображению.\n\nTableau\n\n\nTableau также представляет собой целое семейство онлайн и десктопных приложений, как и Power BI. Данные приложения имеют простой визуальный интерфейс и позволяют работать методом перетаскивания drag-and-drop. Красивые графики строятся буквально за несколько кликов. Также данные можно анализировать в табличном виде и применять к ним различные фильтры.\n\n\nTableau позволяет решать и задачи машинного обучения, такие как регрессия, прогнозирование временных рядов, кластерный анализ. А главное, Tableau способен интегрироваться с внешними скриптами на R и Python. Получается легко расширяемый инструмент.\n\nQlik Sence и QlikView\n\n\nQlik Sence и QlikView по позиционированию и интерфейсу отличаются между собой, но по сути и по алгоритмам решения задач построены на одном движке. QlikView — корпоративная платформа, которой управляют it-специалисты, Qlik Sence — инструмент для личного использования без необходимости обращаться за помощью в тех. поддержку.\n\n\nПри первом же знакомстве бросается в глаза «красота» и легкость визуализации. Это тот самый инструмент, если надо построить приятный глазу дашборд для руководства. С моей точки зрения особенно зрелищным выглядит возможность менять масштаб при анализе географических карт и кластеров на двухмерных графиках. Вспоминаются кадры из фильмов, где на фото со спутников пытаются разглядеть номер автомобиля или выделить человека из толпы на площади.\n\n\nЕще одна интересная опция — наличие мобильного приложения для выполнения анализа со смартфона. Так и представляется топ-менеджер сети ритейла, спешащий на очередной рейс в аэропорту и получивший неожиданное сообщение в мессенджере со ссылкой на дашборд.\n\nQlik Sence интегрируется с Python, а следовательно и с машинным обучением.\n\nExcel\n\n\nВы меня простите, но я не мог пройти мимо Excel. Сколько не смейся, но любой инструмент по своему хорош. Например, в Excel прекрасно строятся сводные таблицы и графики, буквально в несколько кликов. В сочетании с удобным табличным процессором и работой с форматом csv вполне себе хороший инструмент.\n\n4. Изюминка на торте. Автоматическая генерация кода на основе AI\n\nКак-то раз при знакомстве в сети мне задали вопрос «ты программируешь на python?». И когда я ответил «Да», продолжение было совершенно неожиданным.\n\n\n«А ты знаешь про это… » и далее шла ссылка на ролик в Youtube \nhttps://www.youtube.com/watch?v=fZSFNUT6iY8&t=4s&ab_channel=FazilBabu.\n\n\nРечь идет о генеративной текстовой модели от OpenAI, обученной на репозитории GitHub. На конкретных примерах показана способность модели генерировать код на Python на основании заголовка функции и ее краткого описания.\n\n\nА что будет, если такую модель удастся хорошо обучить на скриптах data scientists? Это вопрос для размышлений…    Tags: аналитикdata sciencebusiness intelligencedata miningmachine learningмашинное обучениедата-аналитиканализ данных Hubs: Data MiningData visualizationMachine learningArtificial Intelligence"}, {"role": "assistant", "content": "[['дата-аналитика', 'дата-аналитика', 'дата-аналитика'], ['Machine learning', 'машинного обучения', 'машинного обучения', 'машинному обучению', 'машинного обучения', 'машинного обучения', 'машинного обучения', 'машинного обучения', 'машинного обучения', 'машинного обучения', 'машинного обучения', 'машинного обучения', 'машинного обучения', 'машинного обучения', 'машинным обучением', 'machine learning', 'машинное обучение', 'Machine learning'], ['аналитиков', 'аналитиков', 'аналитиков', 'аналитика'], ['data scientists', 'data scientists', 'data scientists'], ['данных', 'самых данных', 'данными', 'данных', 'данных', 'данные', 'их', 'данные', 'данных', 'данных', 'данных', 'данных', 'данные', 'ним', 'данных'], ['мы', 'наш', 'мне'], ['Языки программирования', 'языка программирования', 'язык', 'язык', 'язык', 'Язык', 'языков программирования'], ['программирования', 'программировании', 'программирования', 'программирования', 'программированием', 'программирования'], ['python', 'Python', 'Python', 'его', 'python', 'данного языка программирования', 'python', 'python', 'python', 'python', 'Python', 'питоне', 'Python', 'Python', 'python', 'Python'], ['производительности', 'производительностью'], ['много плюсов', 'них'], ['я', 'я', 'своей', 'мне', 'я', 'моей', 'меня', 'я', 'я', 'ты'], ['библиотеки', 'библиотеки', 'библиотеке', 'библиотек', 'библиотек', 'библиотека', 'библиотека', 'она', 'библиотеки', 'библиотека'], ['sklearn', 'sklearn', 'sklearn'], ['моделей', 'моделей', 'модели', 'моделей', 'модели', 'модели'], ['код', 'код', 'кода', 'код'], ['R', 'R', 'R', 'R', 'R', 'R', 'R'], ['конвейеры подготовки данных и обучения моделей', 'такие конвейеры'], ['pipeline', 'pipeline', 'pipeline'], ['C++', 'C++'], ['C#', 'C#'], ['вашим', 'Вы'], ['MatLab', 'MatLab', 'данный язык', 'MatLab'], ['Java, JavaScript, Scala и т.д.', 'них'], ['AutoML', 'AutoML', 'своей', 'AutoML', 'AutoML', '«AutoML»', 'AutoML', 'AutoML', 'В этом же разделе', 'AutoML', 'AutoML'], ['одному единственному шагу', 'этот шаг'], ['H2O'], ['MLBox', 'нее', 'MLBox'], ['облачных сервисах', 'облачных сервисов'], ['основные цифровые гиганты', 'них', 'Google AutoML Tables, Azure Machine Learning (Microsoft), SageMaker Autopilot (Amazon)', 'Перечисленные сервисы', 'цифровых гигантов'], ['Microsoft', 'Microsoft', 'Microsoft', 'компании'], ['компаниям', 'которые'], ['конструкторы workflow', 'конструктор', 'Конструктор'], ['кубиков', 'которых'], ['людей с визуальным типом мышления', 'которым'], ['BI', 'BI'], ['Power BI', 'Power BI', 'Power BI', 'Power BI', 'Power BI', 'Power BI', 'Power BI'], ['Tableau', 'Tableau', 'Tableau', 'Tableau', 'Tableau'], ['Qlik Sense', 'Qlik Sence', 'Qlik Sence', 'Qlik Sence', 'Qlik Sence'], ['QlikView', 'QlikView', 'QlikView', 'QlikView'], ['Excel', 'Excel', 'Excel', 'Excel'], ['аналитических инструментов', 'которые'], ['csv', 'csv'], ['регрессии', 'регрессия'], ['целое семейство онлайн и десктопных приложений', 'Данные приложения'], ['корпоративная платформа', 'которой'], ['дашборд', 'дашборд'], ['генеративной текстовой модели от OpenAI', 'модели', 'такую модель'], ['функции', 'ее']]"}]}
{"messages": [{"role": "system", "content": "Ты являешься экспертом в задаче решения референций из текстов на русском языке."}, {"role": "user", "content": "**Ты решаешь задачу разрешения кореференций (coreference resolution) в текстах на русском языке с помощью пошагового рассуждения (Chain-of-Thought).**\n\nСледуй указанным шагам:\n\n---\n\n### \uD83D\uDD39 Шаг 1 (Извлечение упоминаний):\n\n- Прочитай текст.\n- Выпиши все упоминания сущностей (людей, компаний, продуктов, сервисов и т.д.), а также местоимения и фразы, которые могут на них ссылаться.\n- Учитывай не только имена собственные, но и абстрактные понятия (например, \"база знаний\", \"информация\", \"уязвимости\"), если на них ссылаются в тексте.\n- Постарайся различать случаи, когда одно и то же слово может обозначать разные вещи в зависимости от контекста (например, \"ИИ\" может означать как технологию, так и конкретного помощника — Atlassian Intelligence).\n\n---\n\n### \uD83D\uDD39 Шаг 2 (Определение кореференций и группировка):\n\n- Определи, какие из упоминаний относятся к одной и той же сущности.\n- Учитывай грамматические признаки (род, число, падеж), контекст и смысл текста.\n- Если возникают сомнения, выбери наиболее вероятную связь. В сложных случаях можешь кратко пояснить причину (1–2 слова).\n- Сгруппируй упоминания по кластерам — каждому кластеру соответствует одна сущность.\n\n---\n\n### \uD83D\uDD39 Шаг 3 (Итоговый результат):\n\n- Верни только результат в виде массива массивов.\n- Каждый внутренний массив — отдельный кластер кореференций.\n- Не добавляй комментариев, пояснений или шагов рассуждения к итоговому выводу.\n\n---\n\n### \uD83D\uDCCC Пример:\n\n**Текст:**\n\n«Мария живет в Москве. Она любит этот город за его атмосферу.»\n\n**Ответ:**\n\n```json\n[\n  [\"Мария\", \"Она\"],\n  [\"Москве\", \"этот город\", \"его\"]\n]\n```\n\n---\n\nТеперь выполни аналогичный анализ для следующего текста, соблюдая описанные шаги рассуждения:\n\n`\"\n\nТеория вероятностей в машинном обучении. Часть 1: модель регрессии / Habr\n\n\n               Теория вероятностей в машинном обучении. Часть 1: модель регрессии  Reading time  \n    28 min\n   Views  14K Open Data Science corporate blog Mathematics *Machine learning *Statistics in IT Artificial Intelligence       В данной статье мы подробно рассмотрим вероятностную постановку задачи машинного обучения: что такое распределение данных, дискриминативная модель, i.i.d.-гипотеза и метод максимизации правдоподобия, что такое регрессия Пуассона и регрессия с оценкой уверенности, и как нормальное распределение связано с минимизацией среднеквадратичного отклонения.В следующей части рассмотрим метод максимизации правдоподобия в классификации: в чем роль кроссэнтропии, функций сигмоиды и softmax и как кроссэнтропия связана с \"расстоянием\" между распределениями вероятностей и почему модель регрессии тоже обучается через минимизацию кроссэнтропии.В третьей части (статья планируется) перейдем от метода максимизации правдоподобия к байесовскому выводу и его различным приближениям, таким как метод апостериорного максимума, методы Монте-Карло и вариационный вывод. Рассмотрим, как применение этих методов порождает типичные для машинного обучения понятия, такие как стохастический градиентный спуск, регуляризация, ансамблирование, подбор архитектуры и гиперпараметров. Также поговорим о роли априорных гипотез в машинном обучении.Данная серия статей не является введением в машинное обучение и предполагает знакомство читателя с основными понятиями. Задача статей - рассмотреть машинное обучение с точки зрения теории вероятностей, что позволит по новому взглянуть на проблему, понять связь машинного обучения со статистикой и лучше понимать формулы из научных статей. Также на описанном материале строятся более сложные темы, такие как вариационные автокодировщики (Kingma and Welling, 2013), нейробайесовские методы (Müller et al., 2021) и даже некоторые теории сознания (Friston et al., 2022).Бывало ли у вас такое, что разбираясь в некой сложной области вам сначала не удается систематизировать в голове всю имеющуюся информацию, а затем вы что-то узнаете или догадываетесь, и пазл внезапно складывается в стройную и непротиворечивую картину? Именно такую роль может сыграть понимание байесовского вывода в машинном обучении.Как писал Пьер-Симон Лаплас в начале XIX века, \"теория вероятностей - это здравый смысл, выраженный в вычислениях\". Поэтому разрабатывать алгоритмы машинного обучения можно и не опираясь на теорию вероятностей и байесовский вывод. Но с изучением этих областей то, что раньше казалось просто здравым смыслом, приобретает большую строгость и обоснования.Впрочем, в машинном обучении любые теории строятся на очень зыбкой почве, поскольку машинное обучение - это не чистая математика, а наука о применении алгоритмов обучения на данных к реальному миру. Любая математическая теория основана на аксиомах и условиях (например, в статистике таким условием часто является  \"независимость и одинаковая распределенность обучающих примеров\"). В реальности эти условия могут не выполняться, и иногда даже не иметь четкого смысла. Об этой теме мы тоже поговорим подробнее.Возможно, изложение в статье покажется слишком подробным и затянутым, но эти вещи невозможно объяснить в двух словах. Большое количество деталей и пояснений позволяет надеяться, что ничего важного не будет упущено, и в понимании не останется пробелов.Содержание текущей частиВ первом разделе мы рассмотрим связь между машинным обучением и статистическим выводом.Во втором разделе поговорим о моделях: рассмотрим вероятностную модель регрессии и ее обучение методом максимизации правдоподобия.В третьем разделе поговорим о данных: рассмотрим понятие вероятностного распределения данных, задачу максимизации метрики на распределении и i.i.d.-гипотезу.В четвертом разделе снова вернемся к методу максимизации правдоподобия, используя материал из третьего раздела, и введем понятие статистической модели.В пятом разделе рассмотрим модель регрессии с оценкой уверенности в виде формул и программного кода.Звездочкой* отмечены дополнительные разделы, которые не повлияют на понимание дальнейшего материала.1. Машинное обучение и статистический вывод2. Вероятностная модель регрессии        2.1. Регрессия, классификация и промежуточные варианты        2.2. Вероятностное моделирование        2.3. Модель регрессии        2.4. Обсуждение        2.5. Функция потерь Хьюбера*3. Вероятностное распределение данных        3.1. Понятие распределения данных        3.2. Дискриминативные модели        3.3. Генеративные модели*        3.4. i.i.d.-гипотеза и качество обобщения        3.5. Проблемы i.i.d.-гипотезы*4. Статистические модели        4.1. Простые статистические модели        4.2. Статистические модели в машинном обучении5. Регрессия с оценкой уверенности*        5.1. Моделирование дисперсии в модели регрессии*        5.2. Регрессия с константной оценкой дисперсии*1. Машинное обучение и статистический выводСтатистический вывод (оценка параметров и проверка гипотез) часто включается в курсы машинного обучения. Но многими он воспринимается лишь как досадная заноза, которая только отнимает время и далее нигде не используется. Однако машинное обучение и статистический вывод тесно связаны и решают почти одну и ту же задачу.Машинное обучение заключается в написании и применении алгоритмов, которые обучаются на данных, автоматически выводя общие закономерности из частных примеров.Статистический вывод заключается в оценке параметров распределений и проверке гипотез на основе наблюдений, то есть опять-таки в получении общих выводов из частных примеров.Процесс выведения общих правил из частных примеров называется обобщением (generalization), или индуктивным выводом (inductive inference). Чем же тогда отличаются эти два раздела науки? Граница между ними довольно расплывчата. Вообще говоря, большую часть машинного обучения можно считать статистическим выводом, что мы более формально рассмотрим в дальнейшем.Иногда говорят, что в статистике целью обычно является вывод (inference) о том, верна ли гипотеза или как связаны между собой переменные, а в машинном обучении целью обычно является предсказание (prediction) или генерация чего-либо (Bzdok et al., 2018), хотя эти две цели часто близки. В машинном обучении, как правило, используются более сложные модели, тогда как в традиционной статистике модели обычно проще, но за счет этого они более интерпретируемы и  больше внимания уделяется оценке уверенности в сделанных выводах (Breiman, 2001).2. Вероятностная модель регрессии2.1. Регрессия, классификация и промежуточные вариантыЗадачи классификации и регрессии отличаются типом целевого признака: в регрессии целевой признак количественный (иногда его называют \"числовой\"), в классификации - категориальный. Отличие категориального от количественного признака заключается не в его типе (int или float), а скорее в предполагаемой метрике сходства на множестве его значений:В количественном признаке чем больше модуль разности между двумя значениями, тем сильнее они непохожи друг на друга. Например, предсказать значение 2 вместо 1 будет в меньшей степени ошибкой, чем предсказать 10 вместо 1.В категориальном признаке все значения, называемые классами, одинаково непохожи друг на друга. Например, предсказать 2-й класс вместо 1-го будет в той же степени ошибкой, что и предсказать 10-й класс вместо 1-го. При этом множество значений дискретно и, как правило, конечно.Все остальные отличия в алгоритмах (в формате выходных данных, функции потерь и метрике качества) обусловлены описанной выше разницей между этими типами. Вообще, при желании задачу классификации технически можно решать как регрессию, то есть напрямую предсказывать номер класса и округлять его до целого числа. Регрессию, наоборот, можно свести к классификации, выполнив дискретизацию множества значений целевого признака. Такие модели кое-как обучатся, но чаще всего их качество на валидации будет существенно ниже, впрочем бывают и исключения (Müller et al., 2021).В случае иерархической классификации (Silla and Freitas, 2011) целевой признак на первый взгляд является категориальным, но на деле некоторые классы могут быть ближе по смыслу и положению в иерархии друг к другу, чем другие. Например, в классификации животных предсказать овчарку вместо лабрадора является в меньшей степени ошибкой, чем предсказать бегемота вместо лабрадора. Это означает, что на множестве значений целевого признака есть какая-то нетривиальная метрика сходства, и ее желательно учесть в алгоритме обучения.Будет ли это по прежнему задачей классификации? Сложно сказать. Базовые понятия \"классификации\" и \"регрессии\" не покрывают все разнообразие промежуточных вариантов (так же как есть промежуточные варианты между молотком, топором и другими инструментами) (Bernholdt et al., 2019, Twomey et al., 2019). Важно лишь то, что при выборе формата выходных данных и функции потерь нужно учитывать метрику сходства на множестве значений целевого признака. Это один из способов внесения в модель априорной (известной или предполагаемой заранее) информации, к чему мы будем еще не раз возвращаться.2.2. Вероятностное моделированиеВ задачах классификации и регрессии (и многих других задачах) требуется найти зависимость между исходными данными  и целевыми данными  в виде функции . Обычно модель имеет параметры, которые подбираются в ходе обучения, поэтому модель можно записывать как функцию от входных данных  и параметров . Поскольку параметров обычно много, то  - это некий массив чисел. Предсказанное моделью значение  (в отличие от истинного значения) обычно обозначается крышечкой (циркумфлексом): .Общая идея вероятностного моделирования заключается в том, что вместо одного числа модель должна предсказывать распределение вероятностей на множестве  при заданном значении . В теории вероятностей это называется условным распределением и записывается как  или просто . Поскольку модель имеет параметры, то вероятностную модель записывают как  - эта запись читается как \"вероятность  при  и \" (позже рассмотрим конкретные примеры).Таким образом мы позволим модели \"сомневаться\" в предсказании. Выполнив такой переход, мы ничего не теряем: распределение вероятностей  несет больше информации, чем точечная оценка . Зато мы получаем важное преимущество. Величина  определена для всех , поэтому мы можем количественно оценить ошибку модели: чем меньшую вероятность модель назначает верному , тем сильнее ошибка. Так мы естественным образом можем задать функцию потерь.2.3. Модель регрессииПусть мы имеем нейронную сеть с одним скрытым слоем шириной в  нейронов и хотим применить ее для решения задачи регрессии с = 10 входными признаками. Такая сеть имеет 2 матрицы весов и 2 вектора bias'ов: . Нетрудно посчитать, что, например, матрица  содержит = 10000 весов, а всего количество весов равно 12001. Пусть в качестве функции активации используется . Преобразование входных данных в выходные осуществляется по следующей формуле (всю сеть обозначим как функцию ):Примечание. Умножение вектора на матрицу может выполняться слева () или справа (), в зависимости от того, работаем мы с векторами-строками или векторами-столбцами. Например, в TensorFlow используется умножение справа, и матрица весов имеет размер (in, out), в PyTorch - умножение слева, и матрица весов имеет размер (out, in).Меняя параметры , можно получить совершенно разные функции . Обучение сети заключается в том, что мы подбираем такие , чтобы минимизировать ошибку предсказания на обучающей выборке. Но способ расчета ошибки предсказания можно выбрать по-разному:Алгоритмический подход. Мы выбираем способ расчета ошибки произвольным образом на основании здравого смысла, руководствуясь любыми соображениями. Например можем взять в качестве ошибки среднеквадратичное отклонение  или среднее абсолютное отклонение . Нетрудно видеть, что . Поэтому MSE, в сравнении с MAE, не так сильно штрафует несущественные ошибки, но сильнее штрафует большие ошибки. Таким образом, MAE более устойчив к выбросам (подробнее см., например, здесь).Определившись с тем, как сильно нам нужно штрафовать большие ошибки в сравнении с маленькими, мы можем выбрать функцию потерь. Далее, поскольку в обучающей выборке много примеров, сложим величину ошибки на всех примерах и будем минимизировать полученную сумму.Вероятностный подход. Будем рассматривать значение  как мат. ожидание нормального распределения с некой фиксированной дисперсией . Так мы получим распределение вероятностей для  при данном :Иногда то же самое записывают другим способом:Формула плотности вероятности нормального распределения:Эта формула выглядит несколько громоздкой, но на самом деле она несложная. Если мы обозначим , то основу формулы (4) составляет выражение . График этой функции выглядит как характерный симметричный \"колокол\" с центром в . Коэффициент  определяет \"сжатие\" колокола по горизонтальной оси, коэффициент  определяет \"сжатие\" по вертикальной оси. Эти коэффициенты выбраны так, чтобы интеграл функции от  до  был равен единице (что требуется для всех распределений), и чтобы распределение имело дисперсию .Итак, мы считаем, что для каждого  величина  распределена нормально, и мат. ожидание распределения является функцией от , которую требуется найти. В отличие от предыдущего подхода, теперь мы можем рассчитать вероятность для любого значения  при  и .Для каждого обучающего примера: чем меньше вероятность истинного значения  при заданных  и , тем сильнее ошибка предсказания на данном примере. Отсюда естественным образом вытекает функция потерь: нам нужно максимизировать . Формула (2) задает вид этого распределения, а формула (4) помогает подсчитать конкретное численное значение  для каждого обучающего примера.Поиск значений параметров , при которых вероятность (правдоподобие) наблюдаемых данных  максимальна, называется методом максимизации правдоподобия (maximum likelihood estimation, MLE). Параметры, максимизирующие правдоподобие, часто обозначают как , мы будем обозначать их как .Пока что мы рассмотрели только один пример, но в обучающей выборке много примеров. Будем искать такие параметры , чтобы максимизировать произведение вероятностей для всех примеров (позже рассмотрим, почему именно произведение, а не сумму). Максимизация некой величины эквивалентна максимизации ее логарифма, а логарифм произведения равен сумме логарифмов множителей:Формула (5) говорит, что нам нужно минимизировать функцию потерь, равную сумме  по всем обучающим примерам , где  мы моделируем нормальным распределением (2). Подставив выражение для нормального распределения (4) получим:Первые два слагаемых в (6) являются константами и поэтому не влияют на положение максимума по , значит их можно удалить. В третьем слагаемом знаменатель является константой, поэтому он тоже не влияет на положение максимума по , его можно заменить на единицу. После этих действий подставим (6) в (5) и получим:Ранее мы вводили обозначение . Согласно формуле (7) нам надо минимизировать сумму среднеквадратичных отклонений  по всем обучающим примерам. Вспомним, что изначально в модели регрессии мы считали  константой, для которой выбрано произвольное значение. Теперь выясняется, что константа  не влияет на оптимальные параметры , поэтому в задаче поиска оптимальных параметров ее значение не играет роли.Примечание. Значение влияет на результат в байесовском выводе, который мы будем рассматривать в следующих частях.Резюме. В алгоритмическом подходе модель машинного обучения - это некая параметризованная функция . Например, это может быть линейная регрессия, нейронная сеть, ансамбль решающих деревьев или машина опорных векторов (хотя две последние модели имеют переменное число параметров, но это не принципиально). Такая модель напрямую предсказывает значение . В вероятностном подходе модель по-прежнему предсказывает число , но теперь это число считается не окончательным предсказанием , а мат. ожиданием нормального распределения с некой фиксированной дисперсией . Таким образом мы моделируем условное распределение .Важно понять, что между этими двумя подходами нет принципиальной разницы. Если вероятностная модель предсказывает, что \" находится где-то вокруг точки  со среднеквадратичным отклонением \", то при точечной оценке модель предсказывает, что \" равно \", но при этом мы понимаем, что модель обычно не выдает идеально точных предсказаний, и интерпретируем ее предсказание как \" находится где-то вокруг точки \".Таким образом, вероятностная модель просто формализует то, что при точечной оценке мы предполагали неформально. Вспомним цитату Лапласа о том, что теория вероятностей - лишь здравый смысл, выраженный в вычислениях.2.4. ОбсуждениеНа примере регрессии мы увидели, что вероятностный подход позволяет вывести выражение для функции потерь, которое в алгоритмическом подходе мы выбирали произвольно. Однако нормальное распределение в (2) мы выбрали произвольно. Вместо него мы могли бы взять, например, распределение Пуассона  или Лапласа. В случае распределения Лапласа мы пришли бы к тому, что надо минимизировать среднее абсолютное отклонение . Отсюда получается, что выбор распределения  в вероятностном подходе эквивалентен выбору функции ошибки  в алгоритмическом подходе.Какой же подход лучше - вероятностный или алгоритмический? Вероятностный подход удобно применять в тех случаях, когда есть объективные причины предполагать, что  имеет тот или иной вид:Пример 1. Иногда мы знаем, что  - это количество неких событий, которые произошли в условиях  - например, количество посетителей тренажерного зала в зависимости от погоды. Тогда распределение  скорее всего похоже на распределение Пуассона, параметр которого является функцией от . Мы можем расписать аналог формулы (2) для распределения Пуассона и из него вывести функцию потерь, которую следует применять. В результате мы получаем регрессию Пуассона. Кроме того, преимущество в том, что мы получаем не точечную оценку, а распределение вероятностей на .Пример 2. Иногда мы знаем, что распределение  гетероскедастично, то есть дисперсия  непостоянна и зависит от . Например, пусть  - доходы, а  - расходы человека на питание. Понятно что в среднем  растет с ростом , но дисперсия  также растет: более богатый человек может тратить на еду много, а может тратить мало, в зависимости от предпочтений, тогда как более бедный человек скорее всего тратит мало. Поэтому в формуле (2) мы можем сделать дисперсию тоже функцией от . Например, если мы используем нейронную сеть, то пусть она имеет 2 выходных нейрона: один нейрон предсказывает мат. ожидание нормального распределения для , а другой нейрон предсказывает дисперсию. Такой способ называется регрессией с оценкой (не)уверенности, в последнем разделе мы рассмотрим его подробнее. Пока что вы можете попробовать сами вывести требуемые формулы.2.5. Функция потерь Хьюбера*Часто мы используем нормальное распределение для . Обоснован ли этот выбор? Иногда да. По центральной предельной теореме, если некая случайная величина  является суммой множества независимых случайных факторов, и каждый фактор вносит исчезающе малый вклад в сумму, то величина  распределена приблизительно нормально, что говорит о комбинаторной природе нормального распределения. Например, сумма большого числа независимых величин, каждая из которых с вероятностью 0.5 принимает значение +1 или -1, распределена приблизительно нормально, что замечательно демонстрирует игрушка Galton Board:В реальности многие величины распределены приблизительно нормально. Однако распределения, которые встречаются в реальном мире, часто имеют более тяжелые хвосты, то есть большую вероятность встретить крайние значения. Скажем, рост или вес человека в популяции распределен приблизительно нормально, но рекордсмены по росту или весу (в ту или иную сторону) встречаются намного чаще, чем если бы распределение было строго нормальным.Поэтому разумно было бы моделировать  распределением, похожим на нормальное, но с более тяжелыми хвостами. Это снизило бы влияние выбросов. Например, распределение Лапласа имеет более тяжелые хвосты, чем нормальное распределение. Мы можем \"склеить\" эти два распределения, взяв центральную часть от нормального распределения и хвосты от распределения Лапласа. Если далее вывести функцию потерь по формуле (6) как минус логарифм плотности вероятности, то получим часто используемую функцию потерь Хьюбера. Эта функция потерь имеет гиперпараметр . Она не так сильно \"штрафует\" большие выбросы, как среднеквадратичная ошибка.В целом, из формулы (5) видно, что функция потерь равна минус логарифму плотности вероятности для выбранного распределения. На самом деле графики плотности для многих распределений удобнее смотреть в логарифмическом масштабе по вертикальной оси. Например, нормальное распределение в логарифмическом масштабе выглядит как парабола с направленными вниз ветвями, а распределение Лапласа в логарифмическом масштабе выглядит как функция . Зеркально отразив эти графики по оси , мы увидим график функции потерь. Скомбинировав нормальное распределение с хвостами от распределения Лапласа, получаем функцию потерь Хьюбера.Интересно, что функцию потерь Хьюбера можно рассматривать как среднеквадратичную ошибку + gradient clipping, применяемый к градиенту функции потерь по . Gradient clipping означает, что если при обратном проходе  по модулю больше некоего порога , то он обрезается до этого порога:Gradient clipping также применяется при обучении нейронных сетей, но в этом случае он действует не на градиент по , а на градиент по весам.3. Вероятностное распределение данных3.1. Понятие распределения данныхКак правило считается, что обучающие и тестовые данные берутся из одного и того же совместного распределения , называемого распределением данных или генеральной совокупностью. Говоря о распределении  или , мы условно предполагаем наличие некоего \"бесконечного генератора пар \", из которого взяты обучающая и тестовая выборка.Конечно, генеральная совокупность данных - это условность, и вопрос ее близости к истине довольно философский. Обычно мы имеем лишь конечную выборку данных, добытую тем или иным способом, но не имеем строгого определения для . Но в целом мы считаем, что  наиболее велико для \"типичных\" пар , и равно нулю для невозможных пар (в которых либо , либо ).Например, пусть мы имеем датасет из объявлений о продаже автомобилей. Для нашего датасета верно, например, следующее:Количество авто \"Lada Granta\" превосходит количество авто \"Москвич-412\"Количество авто \"Победа\" с двигателем мощностью 500 л. с. равно нулюТогда мы можем считать датасет выборкой из распределения, в котором для  верно следующее:Если распределение  невырождено (то есть не назначает всю вероятность одной точке), то это значит, что  не может быть однозначно определен из , но может быть определен приблизительно.3.2. Дискриминативные моделиЗапишем одну из базовых формул теории вероятностей:Модели, которые моделируют , то есть ищут некое приближение для истинного , называют дискриминативными моделями. Иногда используют другую терминологию: Murphy, 2023 называет модели  предиктивными моделями, которые делятся на дискриминативные (классификация) и модели регрессии.Важная особенность всех таких моделей в том, что они не моделируют вероятность входных данных . Это означает, что дискриминативная модель не может оценить то, насколько реалистичны входные данные. Например, мы могли обучать нейронную сеть распознавать животных на фотографиях. Получив на вход изображение с \"белым шумом\", сеть тоже выдаст какой-то ответ (скорее всего, с высокой уверенностью предскажет одно из животных) и никак не сможет сказать вам, что фотография вообще не является животным.3.3. Генеративные модели*Кроме дискриминативных, существуют еще генеративные модели - в статистике так называются модели, которые моделируют распределение для всех используемых переменных, то есть , либо , если  отсутствует. Иногда \"генеративными моделями\" также называют модели, способные генерировать что-то сложное, вроде картинок или текста - это близко к предыдущему определению, но не всегда одно и то же, так что здесь есть неоднозначность в терминологии. В целом, генеративные модели - это тема для отдельной статьи. Подробно о генеративных моделях можно почитать в книге Murphy, 2023, раздел 20 и далее. Дискриминативные модели рассматриваются там же в разделе 14 и далее.С одной стороны кажется, что в задаче предсказания  по  \"информативность\" модели растет в ряду: , поскольку имея  мы можем рассчитать , а имея   мы можем получить точечную оценку , но не наоборот.Но здесь не учитываются некоторые тонкости. Моделировать  можно по-разному, например, одна модель может оценивать для любой пары  плотность вероятности , а другая модель не имеет такой возможности, но позволяет вычислительно эффективно семплировать пары  из совместного распределения. Если бы мы имели бесконечные вычислительные ресурсы, то эти случаи были бы эквивалентны (то есть умея оценивать плотность вероятности мы могли бы семплировать, и наоборот), но на деле вычислительные ресурсы ограничены, и эти случаи различаются. Если наша модель  может только семплировать пары , то задача оценки  может стать невыполнимой.3.4. i.i.d.-гипотеза и качество обобщенияЧасто выборку данных рассматривают как выборку из независимых и одинаково распределенных величин (independent and identically distributed, i.i.d.). Это означает, что каждый пример  из обучающей и тестовой выборки является случайной величиной, взятой из некоего общего распределения . Отсюда автоматически следует, что все примеры независимы друг от друга.По моему мнению, данная гипотеза сочетает в себе \"блеск и нищету\" машинного обучения. С одной стороны, она позволяет формализовать задачу машинного обучения как максимизация ожидаемой метрики качества  на всем распределении данных (подробнее можно почитать, например, здесь):i.i.d.-гипотеза дает очень простой способ оценки качества модели: достаточно разделить данные на train и test, и поскольку примеры в test взяты из того же распределения, что и train, то оценка качества модели на test является оценкой Монте-Карло для ожидаемой метрики на всем распределении данных.При большом размере тестовой выборки  эта оценка будет достаточно точной. Правда, бывают исключения, когда даже при большом размере тестовой выборки оценка качества будет иметь большую погрешность. Это может происходить в том случае, когда влияние отдельных примеров на метрику качества сопоставимо с суммой влияния всех остальных примеров: например в случае несбалансированных классов или сильных выбросов.В случае регрессии формула (12) обычно подразумевает точечную оценку для , а не распределение вероятностей. Если же модель выдает распределение вероятностей , то мы легко можем преобразовать его в точечную оценку. Например, если целевая метрика  - среднеквадратичное отклонение, то нужно взять мат. ожидание , если же целевая метрика - среднее абсолютное отклонение, то нужно взять медиану (подробнее см., например, здесь).3.5. Проблемы i.i.d.-гипотезы*В простоте оценки качества заключается внешний \"блеск\" i.i.d.-гипотезы, за которым часто скрывается \"нищета\". Дело в том, что она часто не выполняется: обучающая выборка и та выборка, на которой модели предстоит работать по назначению, часто оказываются распределены по-разному.Например, это практически неизбежно в кредитном скоринге: нам нужно предсказать вероятность того, что заемщик не вернет кредит, то обучающие данные собираются только по тем заемщикам, которым ранее выдали кредит. Если раньше в банке не выдавали кредиты людям моложе 20 лет, то такие люди не попадут в обучающее распределение данных. Модель может не научиться корректно работать на таких примерах. Если тестовая выборка была случайно отделена от обучающей, то на ней эта проблема никак не будет заметна, но при работе модели по назначению такие люди, конечно, будут часто встречаться. О способах решения данной проблемы в кредитном скоринге можно почитать, например, в Ehrhardt et al., 2021.Часто бывают ситуации, когда обучающее распределение  менее разнообразно (чем то, на котором модели предстоит работать), содержит паразитные корреляции, и избавиться от этого иногда не представляется возможным. Отсюда возникают проблемы сдвига и утечки данных. Подробно и с большим количеством примеров я рассказывал об этом в статье \"Проблемы современного машинного обучения\".В целом понятие \"обучения\" является более широким, чем задача максимизации метрики (11), особенно когда дело касается обучения агентов (роботов, ботов в компьютерных играх и т. д). Во многом обучение связано с умением выявлять причинно-следственные связи (Pearl, 2009), что помогает функционировать в изменяющейся среде и не обращать внимание на нерелевантные детали окружающей среды. Например, робот, который метко бросает мяч в корзину в лаборатории, но впадает в ступор на уличной площадке из-за того, что корзина стала другого цвета, вряд ли будет полезен. Но способность выявлять причинно-следственные связи никак не связана с максимизацией метрики (11), да и само понятие \"распределения данных\" часто неоднозначно, поскольку обычно источник данных не является генератором независимых и одинаково распределенных примеров.Таким образом, i.i.d.-обучение, то есть любые методы, целью которых является максимизация метрики (11), имеет свои границы применимости. Во многих случаях, когда очевидно что i.i.d.-гипотеза неверна, применяют другие подходы (например, марковские цепи, авторегрессионные модели для прогнозирования временных рядов). Но существуют и пограничные случаи, когда непонятно можно ли применять i.i.d.-обучение. На мой взгляд, в табличных задачах слишком часто полагаются на i.i.d.-гипотезу. Возможно, что когда-нибудь в табличном ML в обиход войдут новые (или \"хорошо забытые старые\") подходы, которые могут учесть зависимость примеров друг от друга, например, с помощью латентных переменных или теории информации.4. Статистические моделиВ первой части мы рассмотрели метод максимизации правдоподобия, но за кадром остался вопрос о том, почему мы максимизируем именно произведение вероятностей, а не их сумму. В этой части мы еще вернемся к этому методу, используя материал из второй части, и соединим все элементы пазла в единую картину, введя понятие статистической модели и вероятности обучающей выборки.4.1. Простые статистические моделиСтатистический вывод (statistical inference) занимается оценкой неизвестных параметров распределений. Пусть у нас есть данные, которые являются выборкой из неизвестного распределения. Нам нужно ответить на вопрос - из какого распределения они получены? Мы вводим набор \"распределений-кандидатов\", который называется статистической моделью. Рассмотрим несколько примеров:Пример 1. У нас есть набор чисел . Мы знаем, что эти числа являются i.i.d.-выборкой из нормального распределения, его дисперсия равна 1, а среднее неизвестно (его нужно оценить из данных). Тогда наша статистическая модель  - это множество всех одномерных нормальных распределений с дисперсией 1 и всеми возможными значениями среднего:Возьмем любое распределение из множества . Поскольку нам известно, что все примеры в выборке независимы, то плотность вероятности всей выборки  равна произведению плотностей вероятностей для всех примеров:Отсюда, зная , мы можем рассчитать . Это значит, что для любого распределения из множества  мы можем рассчитать . Теперь просто выберем из множества  то распределение, для которого  максимально. Для этого нужно просто подставить формулу нормального распределения в (15) и отыскать максимум полученного выражения по  (формулы подробно расписывать здесь не будем). Это и есть метод максимизации правдоподобия.Пример 2. Аналогично предыдущему примеру, но мы не знаем ни среднее, ни дисперсию. Тогда наша статистическая модель - это множество всех одномерных нормальных распределений:Здесь метод максимизации правдоподобия можно выполнить полностью аналогично, только формулы получатся сложнее.Таким образом, статистическая модель ничего не говорит об оптимальных значениях параметров, а лишь описывает среди какого множества распределений  мы выполняем поиск. Сам поиск может выполняться методом максимизации правдоподобия, как в предыдущих примерах. Однако такой подход не всегда хорошо работает, есть и другие методы (байесовский вывод и его аппроксимации), их мы рассмотрим в следующих частях.На какие \"подводные камни\" мы можем натолкнуться, используя метод максимизации правдоподобия? Их несколько.Проблема 1. На самом деле распределение, из которого пришла выборка  может не быть нормальным. Вообще, практически ни одно из распределений реального мира не является строго нормальным (кроме, наверное, квантовой механики). Значит ли это что наша модель неверна? Строго говоря да. Но если искомое распределение хорошо аппроксимируется нормальным, то выполнив аппроксимацию мы практически ничего не потеряем. То есть, наша модель  может не содержать искомого распределения, но если элементами из  можно его хорошо аппроксимировать - этого уже достаточно (если же нельзя - говорят о \"model misspecification\"). Вообще, модель - это некое приближение реальности. Она может быть не идеально точна, но все равно полезна: вся наука и инженерия основаны на моделях как упрощениях наблюдаемых явлений. Как говорил один из статистиков, \"all models are wrong, but some are useful\".Проблема 2. Выборка может не быть независимой и одинаково распределенной (i.i.d.). Например, она может быть результатом двухступенчатного семплирования или даже временным рядом, то есть выборкой из , где  - время, разное для разных семплов. Здесь опять получается, что наша модель  неверна, но она все равно может быть полезна, если зависимостью от  можно пренебречь. Если же ей принебречь нельзя, то нужно использовать другие методы, такие как прогнозирование временных рядов. Они тоже могут быть основаны на теории вероятностей, но i.i.d.-гипотеза в них не используется.Проблема 3. Мы можем заранее иметь предположения о том, какие значения параметров более вероятны, а какие менее вероятны. В этом случае метод максимизации правдоподобия применять не стоит, поскольку он не учитывает эту информацию. В следующей части мы рассмотрим байесовский вывод, который позволяет ее учесть.Проблема 4. Если мы будем искать не 1-2 параметра (, ), а, скажем, 100 параметров, а в нашей выборке всего 10 семплов, то наверное этих семплов недостаточно. Мы легко сможем подогнать параметры под данные, и мы даже сможем найти много разных значений параметров, одинаково хорошо подогнанных под данные. Но некоторые из таких значений параметров будут очень плохо соответствовать всему распределению данных, при хорошем соответствии выборке. Таким образом, в методе максимизации правдоподобия возникает проблема переобучения (1, 2, 3).В третьей части мы рассмотрим байесовский подход, с точки зрения которого метод максимума правдоподобия является лишь аппроксимацией полного байесовского вывода. При этом чем сложнее модель и меньше данных, тем менее точной получается аппроксимация, что и является причиной переобучения. Полный байесовский вывод часто невозможно выполнить ввиду огромной вычислительной сложности, но существуют его более точные аппроксимации: ансамблирование, вариационный вывод и другие методы.4.2. Статистические модели в машинном обученииВ предыдущем разделе мы рассмотрели статистическую модель как параметризованное семейство распределений. Тот же подход можно применить в машинном обучении.Модель машинного обучения (такую как линейная регрессия, случайный лес, нейронная сеть) можно рассмотреть как параметризованную функцию. С помощью этой функции мы оцениваем параметры распределения . Например, пусть мы предполагаем, что условное распределение  является нормальным распределением с   и мат. ожиданием, зависящим от . За  обозначим множество всех возможных значений параметров. Наша статистическая модель будет иметь следующий вид:У нас есть набор пар , и мы предполагаем, что они являются i.i.d.-выборкой из некоего распределения . Это предположение мы подробно рассматривали во второй части. Все, то нам осталось сделать - найти такие параметры , чтобы вероятность обучающей выборки была максимальна: Поскольку мы предполагаем, что все примеры независимы друг от друга (i.i.d.), то по базовым законам теории вероятностей вероятность выборки равна произведению вероятностей всех примеров, так же как в формуле (5). Именно поэтому в разделе 2 мы максимизировали произведение вероятностей (а не, например, сумму или минимум).Все остальное ними уже было рассмотрено ранее: осталось сформулировать задачу оптимизации (5-7) и итеративно решить каким-либо способом (для нейронных сетей - градиентным спуском, для случайного леса - построением новых деревьев и т. д.), найдя оптимальные значения параметров , максимизирующие вероятность выборки.Как видим, здесь есть полная аналогия с простыми статистическими моделями, которые мы рассмотрели в предыдущем разделе. Вместе с этим мы получаем те же самые три подводных камня, основным из которых часто является переобучение: существует множество значений параметров, дающих очень малую ошибку на train и высокую ошибку на test, и процесс оптимизации часто сходится именно к таким решениям.Зачем нам нужны были сложности с логарифмом, и почему нельзя напрямую максимизировать вероятность ? Пожалуй, мы могли бы так сделать. Но во-первых, в случае нормального распределения взятие логарифма упрощает формулы (5-7). Во-вторых, в этом случае мы должны максимизировать уже не сумму, а произведение величин  по всем примерам (5). Оптимизация произведения сотен и тысяч множителей может быть численно нестабильной и вызывать переполнение float. В-третьих, для оптимизации суммы большого количества слагаемых существуют методы стохастической оптимизации с доказанной эффективностью, тогда как для оптимизации произведения большого количества множителей аналогичных методов может не быть.Казалось бы, мы уже рассмотрели много формул и терминов, но от проблемы переобучения никуда не делись, и  вообще переход к вероятностной постановке задачи не принес осоой пользы (кроме регрессии Пуассона). Пока что это действительно так. Фундамент дома сам по себе не приносит пользы: жить в нем ничуть не более удобно, чем в вагончике на стройке. В следующих разделах мы рассмотрим как вероятностная постановка задач классификации и регрессии помогает задавать функцию потерь в сложных случаях, когда данные размечены несколькими аннотаторами по-разному, либо когда нам нужно оценивать уверенность в задаче регрессии. В следующей части рассмотрим байесовский вывод, который основывается на материале данной части.5. Регрессия с оценкой уверенности*5.1. Моделирование дисперсии в модели регрессии*В разделе части 1 мы рассмотрели вариант модели регрессии, в котором  моделируется нормальным распределением, для которого мат. ожидание предсказывается с помощью , а дисперсия является константой. Но нам ничего не мешает предсказывать и дисперсию. Для этого нужно, чтобы модель регрессии выдавала не одно, а два числа:  и . В случае нейронной сети мы можем взять 2 выходных нейрона. В случае градиентного бустинга над решающими деревьями каждый лист каждого дерева может выдавать 2 числа, которые суммируются по всем деревьям.Но дисперсия не может быть меньше нуля, а модель может выдавать любое число. Поэтому удобнее, чтобы модель выдавала логарифм дисперсии. Ее дополнительно можно умножить на константу , которая будет гиперпараметром функции потерь. Тогда мат. ожидание целевой переменной  и ее дисперсия  выразим через  и  следующим образом:Так же как и в первой части, мы минимизируем минус логарифм вероятности для каждого примера. Подставив в (6) выражения для  и , получим выражение для функции потерь:Если функцию потерь умножить на константу или сложить с константой, то положение ее локальных и глобальных минимумов не изменится. Поэтому в формуле (19) удалим второе и четвертое слагаемое, которые являются константами. Затем первое и третье слагаемое умножим на 2. Получим упрощенное выражение для функции потерь:Мы снова видим среднеквадратичное отклонение, но теперь к нему добавляется дополнительный множитель и дополнительное слагаемое. Высокое значение предсказанной неуверенности  повышает значение функции потерь за счет второго слагаемого, но при этом уменьшает вклад первого слагаемого, которое штрафует ошибку предсказания . Это дает модели некий компромисс: модель может предсказывать на каких-то примерах высокую неуверенность, уменьшая штраф от неточного предсказания самого значения  на них.Проверим этот метод на практике, обучив модель на табличном датасете California Housing, в котором нужно предсказывать цену недвижимости в разных районах Калифорнии, имея 8 исходных признаков. В данном случае хорошо работает значение .Кодfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import pearsonr\nimport tensorflow as tf, numpy as np, matplotlib.pyplot as plt\nfrom tensorflow.keras import Sequential, layers, optimizers, callbacks\nfrom tensorflow.keras.optimizers.schedules import InverseTimeDecay\nfrom tqdm.notebook import tqdm\ndef custom_loss(y_true, y_pred, C=0.2):\n  y_true = tf.squeeze(y_true)\n  mu, log_sigma = y_pred[:, 0], y_pred[:, 1]\n  loss = (y_true - mu)**2 / tf.math.exp(log_sigma) / C + log_sigma\n  return tf.math.reduce_mean(loss)\ndef custom_mse(y_true, y_pred):\n  y_true = tf.squeeze(y_true)\n  mu, log_sigma = y_pred[:, 0], y_pred[:, 1]\n  metrics = (y_true - mu)**2\n  return tf.math.reduce_mean(metrics)\n# downloading and preprocessing dataset\nX, y = fetch_california_housing(return_X_y=True)\ny = np.log(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\ny_train = scaler.fit_transform(y_train[:, None])[:, 0]\ny_test = scaler.transform(y_test[:, None])[:, 0]\n#training models\ndef train_simple_model(epochs=50, dropout=0.5):\n  model = Sequential([\n      layers.Dense(1000, 'relu'),\n      layers.Dropout(dropout),\n      layers.Dense(1)\n  ])\n  model.compile(loss='mse', optimizer=\n                optimizers.Adam(InverseTimeDecay(1e-3, 1000, 2)))\n  model.fit(X_train, y_train, validation_data=(X_test, y_test),\n            batch_size=128, epochs=epochs, verbose=0)\n  return model, model.history.history['val_loss']\nn_tests = 10\nsimple = [train_simple_model() for i in tqdm(range(n_tests))]\nuncertainty = [train_model_with_uncertainty() for i in tqdm(range(n_tests))]\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 3))\nfrom matplotlib.lines import Line2D\nfor model, history in simple:\n  ax1.plot(history, color='C0', label='simple models with MSE loss')\nfor model, history in uncertainty:\n  ax1.plot(history, color='C1', label='models with uncertainty')\nax1.set_ylim(0.2, 0.4)\nax1.legend([Line2D([0], [0], color='C0', lw=4),\n                Line2D([0], [0], color='C1', lw=4)],\n           ['simple models with MSE loss', 'models with uncertainty'])\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Validation MSE')\nmodel, _ = uncertainty[0]\npreds_test = model.predict(X_test)\nax2.hexbin(np.abs(preds_test[:, 0] - y_test),\n           preds_test[:, 1], bins='log', extent=(0, 3, -3, 3), gridsize=50)\nax2.set_xlabel('Absolute error')\nax2.set_ylabel('Predicted log sigma')\ncorr = pearsonr(np.abs(preds_test[:, 0] - y_test), preds_test[:, 1])[0]\nax2.set_title(f'Correlation: {corr:g}')\nplt.show()Результаты эксперимента. Мы обучили 10 моделей с простой функцией потерь MSE и еще 10 моделей с оценкой неуверенности. На рисунке слева - динамика изменения MSE на валидации для всех моделей, в зависимости от эпохи обучения. Видно, что наши модели превысили по качеству обычные модели, обучаемые с функцией потерь MSE. Возможно это связано с тем, что снижается переобучение модели на выбросах, так как в областях с частыми выбросами предсказывается высокое значение неуверенности, что смягчает ошибку предсказания мат. ожидания в этих областях, уменьшая множитель при первом слагаемом.На втором графике взяли одну из моделей с оценкой неуверенности и рассмотрели корреляцию фактической ошибки предсказания с предсказанной неуверенностью (на валидационной выборке). Положительная корреляция говорит о том, что модель в какой-то степени справляется с задачей оценки собственной уверенности в предсказании.Описанный в этом разделе подход давно применяется в разных задачах (см. например Nix and Weigend, 1994). Недавно аналогичная функция потерь была реализована в библиотеке градиентного бустинга CatBoost под названием RMSEWithUncertainty. Посмотрим как она работает на практике, используя CatBoost версии 1.1.1:Кодfrom catboost import CatBoostRegressor\nmodel = CatBoostRegressor(depth=3, iterations=1000,\n                          loss_function='RMSEWithUncertainty')\nmodel.fit(X_train, y_train, verbose=0)\nstaged_preds = list(model.staged_predict(X_test, eval_period=1))\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 3))\nfrom sklearn.metrics import mean_squared_error\nmetrics = [mean_squared_error(y_test, p[:, 0]) for p in staged_preds]\nax1.plot(metrics, label='catboost')\nax1.set_xlabel('N trees')\nax1.set_ylabel('validation MSE')\nax1.legend()\nlast_preds = staged_preds[-1]\nax2.hexbin(np.abs(last_preds[:, 0] - y_test),\n           last_preds[:, 1], bins='log', extent=(0, 3, -3, 3), gridsize=50)\nax2.set_xlabel('Absolute error')\nax2.set_ylabel('Predicted log sigma')\ncorr = pearsonr(np.abs(last_preds[:, 0] - y_test), last_preds[:, 1])[0]\nax2.set_title(f'Correlation: {corr:g}')\nplt.show()В целом мы видим похожую картину, хотя точность предсказаний CatBoost получилась выше, чем нейронной сети. В CatBoost есть функция для отрисовки деревьев, воспользуемся ей, чтобы нарисовать первое дерево:model.plot_tree(0)Каждое звено дерева содержит разделяющее правило, в котором проверяется, что значение указанного признака больше указанного порога. Каждый лист дерева выдает 2 числа: само значение  и уверенность. Оба эти числа суммируются по всем деревьям.5.2. Регрессия с константной оценкой дисперсии*Вспомним вероятностную модель регрессии, которую мы рассматривали во второй части (2). В ней   является константой. Для любого значения  локальные и глобальные минимумы функции потерь одни и те же, и изменение  эквивалентно масштабированию функции потерь, что равносильно изменению learning rate. Отсюда получается, что какое бы  мы не брали - ничего не изменится (поскольку learning rate выбирается совсем по другим соображениям). Но что если мы хотим знать реальное ?Для этого мы могли бы сделать  обучаемым параметром, не зависящим от . Заменим в формуле (28) выражение  на обучаемый параметр . Видно, что при стремлении  к  или  все выражение стремится к   , поэтому функция потерь никогда не стремится к  и для параметров () существует глобальный оптимум. Например, в нейронной сети мы можем сделать веса, идущие к нейрону, который выдает уверенность, нулевыми и необучаемыми. Тогда уверенность будет равна bias'у этого нейрона, который является обучаемым параметром.Имеет ли это смысл? Возможно в каких-то случаях да. Но если дать модели выучивать параметр , то он будет рассчитываться на обучающей выборке и поэтому может быть переобученным (если модель хорошо запомнила все обучающие примеры, то и  будет низким). Кроме того, при обучении нейронных сетей часто применяют dropout, и значение  будет рассчитано в условиях наличия dropout (хотя на инференсе он обычно не используется). Поэтому обычно нам гораздо удобнее оценивать дисперсию предсказаний на валидационной или тестовой выборке.Существует еще один похожий подход, называемый inductive conformal prediction (Vovk et al., 2005, Sousa, 2022). В нем на отдельной выборке рассчитывается вероятность модели допустить ошибку выше заданного порога, что близко по смыслу к оценке дисперсии.Конец части 1. Ссылка на вторую часть. Спасибо@ivankomarovи @yorkoза ценные комментарии, которые были учтены при подготовке статьи.      Tags: статистическая модельметод максимального правдоподобиярегрессиянормальное распределениестатистикатеория вероятностейсреднеквадратичное отклонение  Hubs: Open Data Science corporate blogMathematicsMachine learningStatistics in ITArtificial Intelligence"}, {"role": "assistant", "content": "[['Теория вероятностей', 'Теория вероятностей', 'теории вероятностей', 'теория вероятностей', 'теорию вероятностей', 'теории вероятностей', 'теория вероятностей', 'теории вероятностей', 'теории вероятностей', 'теория вероятностей'], ['машинном обучении', 'машинном обучении', 'Machine learning', 'машинного обучения', 'машинного обучения', 'машинном обучении', 'машинное обучение', 'машинное обучение', 'машинного обучения', 'машинном обучении', 'машинного обучения', 'машинном обучении', 'машинное обучение', 'машинным обучением', 'Машинное обучение', 'машинном обучении', 'Машинное обучение', 'машинного обучения', 'машинное обучение', 'Машинное обучение', 'машинного обучения', 'машинном обучении', 'машинном обучении', 'машинного обучения', 'машинного обучения', 'машинного обучения', 'машинного обучения', 'ML', 'машинном обучении', 'машинном обучении', 'машинного обучения'], ['модель регрессии', 'модель регрессии', 'модель регрессии', 'модель регрессии', 'ее', 'модель регрессии', 'модель регрессии', 'Модель регрессии', 'модели регрессии', 'модель регрессии', 'Модель регрессии', 'модели регрессии', 'модели регрессии', 'модели регрессии', 'модели регрессии', 'модель регрессии', 'модель', 'модель', 'модели', 'модель', 'модель'], ['мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'Мы', 'нам', 'мы', 'мы', 'мы', 'мы', 'мы', 'нам', 'мы', 'мы', 'нам', 'мы', 'мы', 'нам', 'мы', 'мы', 'мы', 'мы', 'м', 'мы', 'мы', 'мы', 'мы', 'мы', 'Мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'Мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'нашего', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'наша', 'мы', ' нам', 'мы', 'мы', 'мы', 'Нам', 'Мы', 'нас', 'Мы', 'наша', 'нам', 'мы', 'мы', 'мы', 'наша', 'мы', 'мы', 'наша', 'мы', 'наша', 'наша', 'Мы', 'мы', 'мы', 'Мы', 'мы', 'мы', 'мы', 'мы', 'Наша', 'нас', 'мы', 'мы', 'нам', 'мы', 'мы', 'мы', 'мы', 'нам', 'мы', 'мы', 'мы', 'мы', 'нам', 'мы', 'нам', 'мы', 'мы', 'Мы', 'Мы', 'наши', 'мы', 'мы', 'мы', 'мы', 'мы', 'нам'], ['распределение данных', 'распределения данных', 'распределение данных', 'распределения данных', 'распределений', 'распределение данных', 'распределения данных', 'распределением данных', 'генеральной совокупностью', 'генеральная совокупность данных', 'распределение', 'распределении данных', 'распределении данных', 'распределение данных', '\"распределения данных\"', 'распределению данных'], ['данных', 'данных', 'данных', 'данных', 'данных', 'данные', 'данных', 'данные', 'которые', 'они', 'данных', 'данные', 'данные', 'данных', 'данные'], ['дискриминативная модель', 'Дискриминативные модели', 'Дискриминативные модели', 'дискриминативными моделями', 'дискриминативные', 'дискриминативная модель', 'дискриминативных', 'Дискриминативные модели'], ['i.i.d.-гипотеза', 'i.i.d.-гипотезу', 'i.i.d.-гипотеза', 'i.i.d.-гипотезы', 'i.i.d.-гипотеза', 'данная гипотеза', 'i.i.d.-гипотеза', 'i.i.d.-гипотезы', 'i.i.d.-гипотезы', 'она', 'i.i.d.-гипотеза', 'i.i.d.-гипотезу', 'i.i.d.-гипотеза'], ['метод максимизации правдоподобия', 'метод максимизации правдоподобия', 'метода максимизации правдоподобия', 'методом максимизации правдоподобия', 'методу максимизации правдоподобия', 'методом максимизации правдоподобия (maximum likelihood estimation, MLE)', 'метод максимизации правдоподобия', 'этому методу', 'метод максимизации правдоподобия', 'метод максимизации правдоподобия', 'методом максимизации правдоподобия', 'метод максимизации правдоподобия', 'метод максимизации правдоподобия', 'он', 'параметров', 'методе максимизации правдоподобия', 'метод максимума правдоподобия', 'метод максимального правдоподобия'], ['регрессия Пуассона', 'регрессию Пуассона', 'регрессии Пуассона'], ['регрессия с оценкой уверенности', 'регрессии с оценкой уверенности', 'Регрессия с оценкой уверенности', 'регрессией с оценкой (не)уверенности', 'его', 'Регрессия с оценкой уверенности'], ['нормальное распределение', 'нормального распределения', 'нормального распределения', 'нормальным распределением', 'нормального распределения', 'нормального распределения', 'нормальное распределение', 'него', 'нормального распределения', 'нормальное распределение', 'нормального распределения', 'нормальным', 'нормальное', 'нормальное распределение', 'нормального распределения', 'нормальное распределение', 'нормальное распределение', 'нормального распределения', 'его', 'нормальных распределений', 'нормального распределения', 'нормальных распределений', 'нормальным', 'нормальным', 'нормальным', 'нормальным распределением', 'нормального распределения', 'нормальным распределением', 'которого', 'нормальное распределение'], ['минимизацией', 'минимизацию'], ['среднеквадратичного отклонения', 'среднеквадратичное отклонение', 'MSE', 'среднеквадратичных отклонений', 'среднеквадратичным отклонением', 'среднеквадратичное отклонение', 'среднеквадратичное отклонение', 'нему', 'MSE', 'MSE', 'MSE', 'среднеквадратичное отклонение'], ['классификации', 'классификация', 'классификация', 'классификации', 'классификации', 'классификации', 'классификации', 'классификации', 'классификации', 'классификации', 'классификации', 'классификация', 'классификации'], ['кроссэнтропии', 'кроссэнтропия', 'кроссэнтропии'], ['сигмоиды'], ['softmax'], ['распределениями вероятностей', 'распределение вероятностей', ' распределение вероятностей', 'распределение вероятностей', 'распределений', 'распределение', 'распределения', 'распределения', 'распределения', 'распределение', 'распределение вероятностей', 'распределение', 'распределения', 'которые', 'распределение', 'распределением', 'распределения', 'распределений', 'распределение вероятностей', 'распределение вероятностей'], ['байесовскому выводу', 'его', 'байесовского вывода', 'байесовский вывод', 'этих областей', 'байесовском выводе', 'который', 'она', 'байесовский вывод', 'его', 'байесовский вывод', 'который', 'байесовского вывода', 'байесовский вывод', 'который'], ['метод апостериорного максимума'], ['метод апостериорного максимума, методы Монте-Карло и вариационный вывод', 'этих методов'], ['методы Монте-Карло'], ['вариационный вывод', 'вариационный вывод'], ['стохастический градиентный спуск'], ['регуляризация'], ['ансамблирование', 'ансамблирование'], ['архитектуры'], ['гиперпараметров', 'гиперпараметр', 'гиперпараметром'], ['априорных гипотез'], ['Данная серия статей', 'статей'], ['читателя', 'вас', 'вам', 'вы', 'вы', 'сами', 'вам'], ['статистикой', 'статистике', 'статистике', 'статистике', 'статистике', 'статистика'], ['вариационные автокодировщики'], ['нейробайесовские методы'], ['теории сознания'], ['Пьер-Симон Лаплас', 'Лапласа', 'Лапласа', 'Лапласа', 'Лапласа', 'Лапласа', 'Лапласа'], ['\"теория вероятностей - это здравый смысл, выраженный в вычислениях\"', 'цитату Лапласа'], ['математика'], ['условиях', 'эти условия'], ['независимость'], ['распределенность'], ['статистическим выводом', 'статистический вывод', 'статистический вывод', 'Статистический вывод', 'он', 'статистический вывод', 'Статистический вывод', 'статистическим выводом', 'выводах', 'Статистический вывод (statistical inference)'], ['моделях', 'модели', 'модели', 'они', 'модель', 'модель', 'модель', 'модель', 'модель', 'модели', 'модель', 'модель', 'Такая модель', 'модель', 'модель', 'модель', 'ее', 'Модели', 'которые', 'модели', 'модели', 'которые', 'модели', 'модели', 'модель', 'модель', 'модель', 'модель', 'модели', 'Модель', 'модели', 'модели', 'модель', 'модель', 'модель', 'Она', 'моделях', 'модель', 'она', 'модель', 'Модель', 'моделей', 'моделей', 'моделей', 'модели', 'модели', 'модели', 'моделей', 'модель', 'модели', 'модель', 'модели'], ['третьем разделе', 'третьего раздела'], ['задачу максимизации метрики на распределении'], ['статистической модели', 'Статистические модели', 'статистические модели', 'Статистические модели', 'Статистические модели', 'статистические модели', 'статистической моделью', 'статистическая модель', 'статистическая модель', 'статистическая модель', 'Статистические модели', 'статистическую модель', 'статистическая модель', 'статистическими моделями', 'которые', 'статистическая модель'], ['дополнительные разделы', 'которые'], ['Регрессия', 'Регрессия', 'регрессии', 'регрессии', 'регрессию', 'Регрессию', 'регрессии', 'регрессии', 'регрессии', 'регрессии', 'регрессии', 'регрессии', 'регрессии', 'регрессия'], ['промежуточные варианты', 'промежуточные варианты', 'промежуточных вариантов', 'промежуточные варианты'], ['моделирование', 'Моделирование', 'моделирование', 'моделирования', 'Моделирование'], ['Функция потерь Хьюбера', 'Функция потерь Хьюбера', 'функцию потерь Хьюбера', 'Эта функция потерь', 'Она', 'функцию потерь Хьюбера', 'функцию потерь Хьюбера'], ['Генеративные модели', 'Генеративные модели', 'генеративные модели', '\"генеративными моделями\"', 'генеративные модели', 'генеративных моделях'], ['качество обобщения', 'качество обобщения'], ['дисперсии', 'дисперсией', 'дисперсию', 'дисперсией', 'дисперсия', 'дисперсия', 'дисперсию', 'дисперсию', 'дисперсия', 'дисперсией', 'дисперсию', 'дисперсии', 'дисперсия', 'дисперсию', 'дисперсия', 'дисперсии', 'Ее', 'дисперсия', 'дисперсию', 'дисперсии'], ['Регрессия с константной оценкой дисперсии', 'Регрессия с константной оценкой дисперсии'], ['параметров', 'параметров', 'параметры', 'которые', 'параметров', 'параметров', 'параметры', 'параметры', 'параметров', 'Параметры', 'их', 'параметры', 'параметров', 'параметр', 'параметров', 'параметра', 'параметры', 'параметров', 'параметров', 'параметры', 'параметров', 'параметров', 'параметров', 'параметров', 'параметр', 'он'], ['гипотез', 'гипотез', 'гипотеза'], ['заноза', 'которая'], ['алгоритмов', 'которые'], ['обобщением (generalization)'], ['индуктивным выводом (inductive inference)'], ['эти два раздела науки', 'ними'], ['вывод (inference)'], ['предсказание (prediction)', 'предсказании', 'предсказания', 'предсказания', 'предсказания', 'предсказанием', 'предсказаний', 'предсказание', 'предсказания', 'предсказания', 'предсказания', 'предсказания', 'предсказания', 'предсказании', 'предсказаний', 'предсказаний'], ['генерация', 'эти две цели'], ['оценке уверенности'], ['целевого признака', 'целевой признак', 'его', 'его', 'его', 'целевого признака', 'целевой признак', 'целевого признака', 'целевого признака'], ['количественный', 'количественного признака', 'количественном признаке'], ['категориальный', 'категориального', 'категориальном признаке', 'этими типами', 'категориальным'], ['метрике сходства', 'метрика сходства', 'ее', 'метрику сходства'], ['модуль разности'], ['двумя значениями', 'они'], ['ошибкой', 'ошибкой', 'ошибкой', 'ошибки', 'ошибки', 'ошибки'], ['классами'], ['множество значений', 'множества значений', 'множестве значений', 'множестве значений'], ['формате выходных данных', 'формата выходных данных'], ['функции потерь', 'функции потерь', 'функцию потерь', 'функцию потерь', 'функция потерь', 'функцию потерь', 'функции потерь', 'функцию потерь', 'которую', 'функцию потерь', 'функция потерь', 'функции потерь', 'функции потерь', 'функцию потерь', 'функции потерь', 'функции потерь', 'функцию потерь', 'ее', 'функции потерь', 'функции потерь', 'функцией потерь', 'функцией потерь', 'функция потерь', 'функции потерь', 'функции потерь', 'функция потерь'], ['метрике качества', 'метрики качества', 'качества модели', 'метрику качества'], ['номер класса', 'его'], ['дискретизацию'], ['Такие модели', 'их'], ['валидации', 'валидации'], ['иерархической классификации'], ['лабрадора', 'лабрадора'], ['априорной (известной или предполагаемой заранее) информации', 'чему'], ['исходными данным', 'исходных признаков'], ['целевыми данными'], ['входных данных', 'входных данных', 'входных данных', 'входные данные'], ['Предсказанное моделью значение'], ['истинного значения', 'истинного значения'], ['условным распределением', 'условное распределение', 'условное распределение'], ['вероятностную модель', 'вероятностная модель', 'вероятностная модель', 'вероятностную модель регрессии', 'которую', 'ней'], ['вероятность', 'вероятность', 'вероятность', 'вероятность', 'вероятность', 'вероятностей', 'вероятность', 'вероятность', 'вероятность', 'вероятностей', 'вероятность', 'вероятность', 'вероятностей', 'вероятностей', 'вероятность', 'вероятность', 'вероятности', 'вероятность'], ['Величина'], ['ошибку модели', 'ошибка', 'ошибку', 'ошибки', 'ошибки', 'ошибки', 'ошибка', 'ошибки', 'ошибку', 'ошибку', 'ошибку', 'ошибку', 'ошибки', 'ошибку'], ['нейронную сеть', 'ее', 'Такая сеть', 'сети', 'нейронная сеть', 'нейронную сеть', 'она', 'нейронных сетей', 'нейронную сеть', 'сеть', 'нейронная сеть', 'нейронных сетей', 'нейронной сети', 'нейронной сети', 'нейронной сети', 'нейронных сетей'], ['скрытым слоем'], ['нейронов', 'нейрона', 'нейрон', 'нейрон', 'нейрону', 'который', 'этого нейрона'], ['входными признаками'], ['матрицы весов', 'матрица', 'матрица весов', 'матрица весов'], ['весов', 'весов', 'весов', 'весам', 'веса'], [\"вектора bias'ов\"], [\"bias'ов\", \"bias'у\", 'который'], ['функции активации'], ['выходные'], ['вектора', 'векторами', 'векторами'], ['матрицу'], ['TensorFlow'], ['PyTorch'], ['обучающей выборке', 'обучающей выборке', 'обучающей выборке', 'обучающая', 'обучающей', 'обучающая выборка', 'обучающей', 'обучающей выборки', 'обучающей выборке'], ['способ расчета ошибки предсказания', 'способ расчета ошибки'], ['Алгоритмический подход', 'предыдущего подхода', 'алгоритмическом подходе', 'алгоритмическом подходе', 'алгоритмическом подходе', 'алгоритмический'], ['среднее абсолютное отклонение', 'MAE', 'MAE', 'среднее абсолютное отклонение', 'среднее абсолютное отклонение'], ['сумму', 'сумму', 'сумме', 'сумме', 'сумму', 'суммой', 'сумму', 'сумма', 'суммой', 'сумму', 'сумму', 'сумму,', 'суммы'], ['Вероятностный подход', 'вероятностном подходе', 'этими двумя подходами', 'вероятностный подход', 'вероятностном подходе', 'вероятностный', 'Вероятностный подход'], ['мат. ожидание', 'мат. ожидание', 'мат. ожиданием', 'мат. ожидание', 'мат. ожидание', 'среднее', 'его', 'среднего', 'среднее', 'мат. ожиданием', 'мат. ожидание', 'мат. ожидание', 'мат. ожидания'], ['Формула плотности вероятности нормального распределения', 'Эта формула', 'она', 'формулы'], ['плотности вероятности', 'плотности вероятности', 'плотности вероятности', 'плотности', 'плотность вероятности', 'плотность вероятности', 'плотность вероятности', 'плотностей вероятностей'], ['этой функции', 'функции'], ['\"колокол\"', 'колокола'], ['Коэффициент', 'Эти коэффициенты'], ['горизонтальной оси'], ['коэффициент'], ['вертикальной оси', 'вертикальной оси'], ['единице', 'единицу'], ['величина', 'величин'], ['функцией', 'которую'], ['обучающего примера', 'обучающего примера', 'обучающим примерам', 'обучающим примерам', 'обучающие примеры'], ['Формула (2)', '(2)', '(2)', 'формулы (2)', 'формуле (2)'], ['значений параметров', 'которых'], ['правдоподобие', 'правдоподобие'], ['произведение', 'произведение', 'произведения', 'произведение', 'произведению', 'произведению', 'произведение', 'произведение', 'произведения', 'произведения'], ['Максимизация', 'максимизации', 'максимизация', 'максимизации', 'максимизацией', 'максимизация'], ['некой величины', 'ее'], ['логарифма', 'логарифм', 'логарифмов', 'логарифм', 'логарифму', 'логарифмом', 'логарифма', 'логарифм', 'логарифм'], ['Формула (5)', '(5)', 'формулы (5)', 'формуле (5)'], ['Первые два слагаемых', 'их'], ['(6)', '(6)', 'формуле (6)'], ['константами', 'константой', 'константой', 'которой', 'константа', 'ее', 'константой', 'константу', 'которая', 'константу', 'константой', 'константами', 'константой'], ['максимума', 'максимума', 'максимум'], ['третьем слагаемом'], ['знаменатель', 'он', 'его'], ['оптимальные параметры', 'оптимальных параметров'], ['линейная регрессия', 'линейная регрессия'], ['ансамбль решающих деревьев'], ['машина опорных векторов', 'две последние модели'], ['число', 'это число'], ['точки', 'точки'], ['точечной оценке', 'точечной оценке', 'точечную оценку', 'точечную оценку', 'точечную оценку', 'точечную оценку'], ['выражение для функции потерь', 'которое'], ['распределение Пуассона', 'распределение Пуассона', 'которого', 'распределения Пуассона'], ['Пуассона', 'Пуассона', 'Пуассона', 'Пуассона'], ['Лапласа', 'распределения Лапласа', 'распределение Лапласа', 'распределения Лапласа', 'распределение Лапласа', 'распределения Лапласа'], ['мы'], ['функции ошибки'], ['событий', 'которые'], ['аналог формулы (2)', 'него'], ['центральной предельной теореме'], ['случайная величина', 'величина', 'величин', 'случайной величиной'], ['независимых случайных факторов', 'фактор', 'независимых величин', 'каждая из которых', 'независимых'], ['тяжелые хвосты'], ['рост', 'росту'], ['вес', 'весу'], ['тяжелыми хвостами', 'тяжелые хвосты'], ['эти два распределения'], ['минус', 'минус', 'минус'], ['среднеквадратичная ошибка', 'среднеквадратичную ошибку'], ['графики', 'график'], ['логарифмическом масштабе', 'логарифмическом масштабе', 'логарифмическом масштабе'], ['парабола'], ['gradient clipping', 'Gradient clipping', 'Gradient clipping', 'он'], ['градиенту', 'градиент', 'градиент'], ['модулю'], ['некоего порога', 'этого порога'], ['он'], ['обучающие', 'обучающие данные'], ['тестовые данные'], ['совместного распределения', 'совместного распределения'], ['распределения', 'распределении', 'распределения', 'котором', 'распределение', 'распределения', 'распределения', 'распределение', 'распределений', 'распределения', 'распределение', 'распределения', 'распределение', 'которого', 'распределений', 'распределение', 'которого', 'распределений', 'распределение', 'распределения', 'распределений', 'распределения'], ['бесконечного генератора пар', 'которого'], ['пар', 'пар', 'пар', 'пары', 'пары', 'пары', 'пар', 'они'], ['тестовая выборка', 'тестовой выборки', 'тестовой выборки', 'тестовой выборки', 'тестовая выборка', 'ней', 'тестовой выборке'], ['условность', 'ее'], ['выборку данных', 'выборкой', 'выборку данных', 'выборку'], ['нулю', 'нулю'], ['невозможных пар', 'которых'], ['датасет', 'датасета', 'датасет'], ['автомобилей', 'авто', 'авто', 'авто'], ['приближение'], ['предиктивными моделями', 'которые', 'таких моделей', 'они'], ['животных', 'животных', 'животным'], ['фотографиях', 'фотография'], ['книге Murphy, 2023', 'там же'], ['вычислительные ресурсы', 'вычислительные ресурсы'], ['эти случаи', 'эти случаи'], ['независимых и одинаково распределенных величин (independent and identically distributed, i.i.d.)', 'независимых и одинаково распределенных примеров', 'независимой и одинаково распределенной (i.i.d.)'], ['моему', 'я', 'мой'], ['train', 'train', 'train'], ['test', 'test', 'test', 'test'], ['оценка качества модели', 'эта оценка', 'оценка качества', 'оценки качества'], ['оценкой Монте-Карло'], ['метрики'], ['погрешность'], ['несбалансированных классов'], ['сильных выбросов'], ['целевая метрика', 'целевая метрика'], ['медиану'], ['внешний \"блеск\"', 'которым'], ['выборка', 'которой', 'выборкой', 'выборке', 'выборки', 'выборка', 'Выборка', 'она', 'выборкой', 'выборке', 'выборке', 'выборки', 'выборки'], ['кредитном скоринге', 'кредитном скоринге'], ['заемщик', 'заемщикам'], ['кредит', 'кредит', 'кредиты'], ['которым'], ['людям моложе 20 лет', 'такие люди', 'такие люди'], ['обучающее распределение'], ['то', 'котором'], ['паразитные корреляции', 'этого'], ['корреляции', 'корреляцию', 'корреляция'], ['проблемы сдвига'], ['утечки данных'], ['\"обучения\"', 'обучения', 'обучение'], ['метрики (11)', 'метрики (11)', 'метрики (11)'], ['агентов'], ['причинно-следственные связи', 'причинно-следственные связи'], ['робот', 'который'], ['корзину', 'корзина'], ['источник данных'], ['генератором'], [' i.i.d.-обучение', 'i.i.d.-обучение'], ['методы', 'которых'], ['марковские цепи'], ['авторегрессионные модели для прогнозирования временных рядов'], ['подходы', 'которые'], ['латентных переменных'], ['теории информации'], ['статистической модели'], ['вероятности обучающей выборки'], ['оценкой неизвестных параметров', 'неизвестных параметров'], ['нас'], ['неизвестного распределения', 'некоего распределения'], ['набор \"распределений-кандидатов\"', 'который'], ['Пример 1', 'предыдущему примеру'], ['набор чисел', 'эти числа'], ['i.i.d.-выборкой', 'i.i.d.-выборкой'], ['множество', 'множества', 'множества', 'множества', 'множество', 'множества'], ['все примеры', 'всех примеров'], ['Пример 2'], ['оптимальных значениях параметров'], ['мы'], ['поиск', 'поиск'], ['другие методы', 'их'], ['\"подводные камни\"', 'Их', 'три подводных камня', 'которых'], ['искомое распределение', 'искомого распределения', 'его'], ['аппроксимацию', 'аппроксимацией', 'аппроксимация', 'аппроксимации'], ['\"model misspecification\"'], ['двухступенчатного семплирования'], ['временным рядом'], ['время'], ['семплов', 'семплов', 'семплов'], ['зависимостью', 'ей'], ['другие методы', 'Они', 'них'], ['прогнозирование временных рядов'], ['эту информацию', 'ее'], ['переобучения', 'переобучения', 'переобучение', 'переобучения', 'переобучение'], ['мы'], ['байесовский подход', 'которого'], ['полного байесовского вывода', 'Полный байесовский вывод', 'его'], ['вычислительной сложности'], ['семейство распределений'], ['случайный лес', 'случайного леса'], ['параметризованную функцию', 'этой функции'], ['множество всех возможных значений параметров', 'параметры'], ['второй части', 'разделе 2'], ['все примеры', 'всех примеров'], ['минимум'], ['ними'], ['оптимизации', 'оптимизации', 'Оптимизация', 'оптимизации', 'оптимизации', 'оптимизации'], ['градиентным спуском'], ['построением новых деревьев'], ['оптимальные значения параметров'], ['множителей', 'множителей'], ['методы стохастической оптимизации'], ['эффективностью'], ['вероятностной постановке задачи', 'вероятностная постановка задач'], ['Фундамент дома', 'сам по себе', 'нем'], ['аннотаторами'], ['уверенность', 'уверенности', 'уверенность', 'уверенность', 'уверенность'], ['вариант модели регрессии', 'котором'], ['выходных нейрона'], ['градиентного бустинга', 'градиентного бустинга'], ['решающими деревьями', 'дерева', 'деревьям'], ['лист', 'лист'], ['2 числа', 'которые'], ['целевой переменной', 'ее'], ['локальных и глобальных минимумов', 'локальные и глобальные минимумы'], ['второе и четвертое слагаемое', 'которые'], ['дополнительный множитель'], ['дополнительное слагаемое'], ['неуверенности', 'неуверенность', 'неуверенности', 'неуверенности', 'неуверенности', 'неуверенностью'], ['второго слагаемого'], ['первого слагаемого', 'которое', 'первом слагаемом'], ['каких-то примерах', 'них'], ['штраф'], ['табличном датасете California Housing', 'котором'], ['эпохи обучения'], ['выбросах', 'выбросами'], ['областях с частыми выбросами', 'этих областях'], ['валидационной выборке', 'валидационной'], ['Положительная корреляция'], ['библиотеке градиентного бустинга CatBoost', 'CatBoost', 'CatBoost', 'CatBoost'], ['RMSEWithUncertainty', 'она'], ['точность'], ['функция для отрисовки деревьев', 'ей'], ['деревьев', 'дерево', 'дерева', 'дерева', 'деревьям'], ['звено'], ['разделяющее правило', 'котором'], ['признака'], ['порога', 'порога'], ['2 числа', 'Оба эти числа'], ['само значение'], ['learning rate', 'learning rate'], ['мы'], ['обучаемым параметром', 'обучаемый параметр', 'обучаемым параметром'], ['стремлении'], ['глобальный оптимум'], ['dropout', 'dropout', 'он'], ['inductive conformal prediction'], ['нем']]"}]}
{"messages": [{"role": "system", "content": "Ты являешься экспертом в задаче решения референций из текстов на русском языке."}, {"role": "user", "content": "**Ты решаешь задачу разрешения кореференций (coreference resolution) в текстах на русском языке с помощью пошагового рассуждения (Chain-of-Thought).**\n\nСледуй указанным шагам:\n\n---\n\n### \uD83D\uDD39 Шаг 1 (Извлечение упоминаний):\n\n- Прочитай текст.\n- Выпиши все упоминания сущностей (людей, компаний, продуктов, сервисов и т.д.), а также местоимения и фразы, которые могут на них ссылаться.\n- Учитывай не только имена собственные, но и абстрактные понятия (например, \"база знаний\", \"информация\", \"уязвимости\"), если на них ссылаются в тексте.\n- Постарайся различать случаи, когда одно и то же слово может обозначать разные вещи в зависимости от контекста (например, \"ИИ\" может означать как технологию, так и конкретного помощника — Atlassian Intelligence).\n\n---\n\n### \uD83D\uDD39 Шаг 2 (Определение кореференций и группировка):\n\n- Определи, какие из упоминаний относятся к одной и той же сущности.\n- Учитывай грамматические признаки (род, число, падеж), контекст и смысл текста.\n- Если возникают сомнения, выбери наиболее вероятную связь. В сложных случаях можешь кратко пояснить причину (1–2 слова).\n- Сгруппируй упоминания по кластерам — каждому кластеру соответствует одна сущность.\n\n---\n\n### \uD83D\uDD39 Шаг 3 (Итоговый результат):\n\n- Верни только результат в виде массива массивов.\n- Каждый внутренний массив — отдельный кластер кореференций.\n- Не добавляй комментариев, пояснений или шагов рассуждения к итоговому выводу.\n\n---\n\n### \uD83D\uDCCC Пример:\n\n**Текст:**\n\n«Мария живет в Москве. Она любит этот город за его атмосферу.»\n\n**Ответ:**\n\n```json\n[\n  [\"Мария\", \"Она\"],\n  [\"Москве\", \"этот город\", \"его\"]\n]\n```\n\n---\n\nТеперь выполни аналогичный анализ для следующего текста, соблюдая описанные шаги рассуждения:\n\n`\"\n\nКак работает ChatGPT: объясняем на простом русском эволюцию языковых моделей с T9 до чуда / Habr\n\n\n               Как работает ChatGPT: объясняем на простом русском эволюцию языковых моделей с T9 до чуда Level of difficulty  \n    Easy\n   Reading time  \n    30 min\n   Views  235K Open Data Science corporate blog Machine learning *Artificial Intelligence The future is here Natural Language Processing * \n    Review\n        В последнее время нам почти каждый день рассказывают в новостях, какие очередные вершины покорили языковые нейросетки, и почему они уже через месяц совершенно точно оставят лично вас без работы. При этом мало кто понимает — а как вообще нейросети вроде ChatGPT работают внутри? Так вот, устраивайтесь поудобнее: в этой статье мы наконец объясним всё так, чтобы понял даже шестилетний гуманитарий!OpenAI (компанию, сделавшую ChatGPT) основали в 2015 году именно вот эти двое парнишек – кто бы тогда знал, во что это в итоге выльется...На всякий случай сразу оговоримся: у этой статьи два автора. За всю техническую часть (и за всё хорошее в статье) отвечал Игорь Котенков — широко известный чувак в узких кругах русскоязычной тусовки специалистов по искусственному интеллекту, а также автор канала Сиолошная про машинное обучение, космос и технологии. За мольбы «вот тут непонятно, давай как‑нибудь попроще!» и за добавление кринжовых неуместных мемов был ответственен Павел Комаровский — автор канала RationalAnswer про рациональный подход к жизни и финансам.Собственно, статья так и родилась: Павел пришел к Игорю и возмутился — дескать, «почему никто еще не написал на русском нормальную статью про ChatGPT, объясняющую понятно даже для моей бабушки, как всё вот это нейроколдунство работает?». Так что заранее приносим свои извинения всем хардкорным технарям: при подготовке этого текста мы стремились к максимальному упрощению. Нашей задачей было — дать читателям общее понимание принципов работы языковых нейросетей на уровне концепций и аналогий, а не разобрать до последнего винтика все глубокие технические нюансы процесса.В общем, наливайте себе кружечку горячего чая и устраивайтесь поудобнее — сейчас мы вам расскажем всё про то, что там крутится под капотом у языковых моделей, каким образом эти покемоны эволюционировали до текущих (местами поразительных) способностей, и почему взрывная популярность чат‑бота ChatGPT стала полным сюрпризом даже для его создателей. Поехали! (Кстати, этот материал есть еще и в формате видео.)НавигаторT9: сеанс языковой магии с разоблачениемОткуда нейросети берут вероятности слов?Парадокс Барака, или зачем языковым моделям уметь в творчество2018: GPT-1 трансформирует языковые модели2019: GPT-2, или как запихнуть в языковую модель семь тысяч ШекспировПочему в мире языковых моделей больше ценятся именно модели «Plus Size»2020: GPT-3, или как сделать из модели Невероятного ХалкаПромпты, или как правильно уламывать модельЯнварь 2022: InstructGPT, или как научить робота не зиговатьНоябрь 2022: ChatGPT, или маленькие секреты большого хайпаПодведем итогиT9: сеанс языковой магии с разоблачениемНачнем с простого. Чтобы разобраться в том, что такое ChatGPT с технической точки зрения, надо сначала понять, чем он точно не является. Это не «Бог из машины», не разумное существо, не аналог школьника (по уровню интеллекта и умению решать задачи), не джинн, и даже не обретший дар речи Тамагочи. Приготовьтесь услышать страшную правду: на самом деле, ChatGPT — это Т9 из вашего телефона, но на бычьих стероидах! Да, это так: ученые называют обе этих технологии «языковыми моделями» (Language Models); а всё, что они по сути делают, — это угадывают, какое следующее слово должно идти за уже имеющимся текстом.Ну, точнее, в совсем олдовых телефонах из конца 90-х (вроде культовой неубиваемой Nokia 3210) оригинальная технология Т9 лишь ускоряла набор на кнопочных телефонах за счет угадывания текущего вводимого, а не следующего слова. Но технология развивалась, и к эпохе смартфонов начала 2010-х она уже могла учитывать контекст (предыдущее слово), ставить пунктуацию и предлагать на выбор слова, которые могли бы идти следующими. Вот именно об аналогии с такой «продвинутой» версией T9/автозамены и идет речь.Кого ни разу не подставляла автозамена на телефоне – пусть первый бросит в меня каменьИтак, и Т9 на клавиатуре смартфона, и ChatGPT обучены решать до безумия простую задачу: предсказание единственного следующего слова. Это и есть языковое моделирование – когда по некоторому уже имеющемуся тексту делается вывод о том, что должно быть написано дальше. Чтобы иметь возможность делать такие предсказания, языковым моделям под капотом приходится оперировать вероятностями возникновения тех или иных слов для продолжения. Ведь, скорее всего, вы были бы недовольны, если бы автозаполнение в телефоне просто подкидывало вам абсолютно случайные слова с одинаковой вероятностью.Представим для наглядности, что вам прилетает сообщение от приятеля: «Чё, го седня куда нить?». Вы начинаете печатать в ответ: «Да не, у меня уже дела(( я иду в...», и вот тут подключается Т9. Если он предложит вам закончить предложение полностью рандомным словом, типа «я иду в капибару» — то для такой белиберды, если честно, никакая хитрая языковая модель особо и не нужна. Реальные же модели автозаполнения в смартфонах подсказывают гораздо более уместные слова (можете сами проверить прямо сейчас).Мой Samsung Galaxy предлагает такие варианты. Сразу видно типичного айтишника: получил зарплату, прокутил – и сразу в аптеку, лечиться!Так, а как конкретно Т9 понимает, какие слова будут следовать за уже набранным текстом с большей вероятностью, а какие предлагать точно не стоит? Для ответа на этот вопрос нам придется погрузиться в базовые принципы работы самых простейших нейросеток.Откуда нейросети берут вероятности слов?Давайте начнем с еще более простого вопроса: а как вообще предсказывать зависимости одних вещей от других? Предположим, мы хотим научить компьютер предсказывать вес человека в зависимости от его роста — как подойти к этой задаче?Здравый смысл подсказывает, что надо сначала собрать данные, на которых мы будем искать интересующие нас зависимости (для простоты ограничимся одним полом — возьмем статистику по росту/весу для нескольких тысяч мужчин), а потом попробуем «натренировать» некую математическую модель на поиск закономерности внутри этих данных.Для наглядности сначала нарисуем весь наш массив данных на графике: по горизонтальной оси  будем откладывать рост в сантиметрах, а по вертикальной оси — вес.Судя по нашим прикидкам, мужики в выборке попались в среднем ну такие – довольно упитанные (или сплошь качки на массе, тут сразу не разберешь)  Даже невооруженным взглядом видна определенная зависимость: высокие мужики, как правило, больше весят (спасибо, кэп!). И эту зависимость довольно просто выразить в виде обычного линейного уравнения , знакомого нам всем с пятого класса школы. На картинке нужная нам линия уже проведена с помощью модели линейной регрессии — по сути, она позволяет подобрать коэффициенты уравнения  и  таким образом, чтобы получившаяся линия оптимально описывала ключевую зависимость в нашем наборе данных (можете для интереса подставить свой рост в сантиметрах вместо  в уравнение на картинке и проверить, насколько точно наша модель угадает ваш вес).Вы тут уже наверняка хотите воскликнуть: «Окей, с ростом/весом и так интуитивно всё было понятно, только причем тут вообще языковые нейросети?» А притом, что нейросети — это и есть набор примерно тех же самых уравнений, только куда более сложных и использующих матрицы (но не будем сейчас об этом).Можно упрощенно сказать, что те же самые T9 или ChatGPT — это всего лишь хитрым образом подобранные уравнения, которые пытаются предсказать следующее слово (игрек) в зависимости от набора подаваемых на вход модели предыдущих слов (иксов). Основная задача при тренировке языковой модели на наборе данных — подобрать такие коэффициенты при этих иксах, чтобы они действительно отражали какую‑то зависимость (как в нашем примере с ростом/весом). А под большими моделями мы далее будем понимать такие, которые имеют очень большое количество параметров. В области ИИ их прямо так и называют — LLM, Large Language Models. Как мы увидим чуть дальше, «жирная» модель с множеством параметров — это залог успеха для генерации крутых текстов!Кстати, если вы в этом месте уже недоумеваете, почему мы всё время говорим о «предсказании одного следующего слова», тогда как тот же ChatGPT бодро отвечает целыми портянками текста – то не ломайте зря голову. Языковые модели без всякого труда генерируют длинные тексты, но делают они это по принципу «слово за словом». По сути, после генерации каждого нового слова, модель просто заново прогоняет через себя весь предыдущий текст вместе с только что написанным дополнением – и выплевывает последующее слово уже с учетом него. В результате получается связный текст.Парадокс Барака, или зачем языковым моделям уметь в творчествоНа самом деле, в наших уравнениях в качестве «игрека» языковые модели пытаются предсказать не столько конкретное следующее слово, сколько вероятности разных слов, которыми можно продолжить заданный текст. Зачем это нужно, почему нельзя всегда искать единственное, «самое правильное» слово для продолжения? Давайте разберем на примере небольшой игры.Правила такие: вы притворяетесь языковой моделью, а я вам предлагаю продолжить текст «44-й президент США (и первый афроамериканец на этой должности) — это Барак...». Подставьте слово, которое должно стоять вместо многоточия, и оцените вероятность, что оно там действительно окажется.Ваш ход, маэстро!Если вы сейчас сказали, что следующим словом должно идти «Обама» с вероятностью 100%, то поздравляю — вы ошиблись! И дело тут не в том, что существует какой‑то другой мифический Барак: просто в официальных документах имя президента часто пишется в полной форме, с указанием его второго имени (middle name) — Хуссейн. Так что правильно натренированная языковая модель должна, по‑хорошему, предсказать, что в нашем предложении «Обама» будет следующим словом только с вероятностью условно в 90%, а оставшиеся 10% выделить на случай продолжения текста «Хуссейном» (после которого последует Обама уже с вероятностью, близкой к 100%).И тут мы с вами подходим к очень интересному аспекту языковых моделей: оказывается, им не чужда творческая жилка! По сути, при генерации каждого следующего слова, такие модели выбирают его «случайным» образом, как бы кидая кубик. Но не абы как — а так, чтобы вероятности «выпадения» разных слов примерно соответствовали тем вероятностям, которые подсказывают модели зашитые внутрь нее уравнения (выведенные при обучении модели на огромном массиве разных текстов).Получается, что одна и та же модель даже на абсолютно одинаковые запросы может давать совершенно разные варианты ответа — прямо как живой человек. Вообще, ученые когда‑то пытались заставить нейронки всегда выбирать в качестве продолжения «наиболее вероятное» следующее слово — что на первый взгляд звучит логично, но на практике такие модели почему‑то работают хуже; а вот здоровый элемент случайности идет им строго на пользу (повышает вариативность и, в итоге, качество ответов).Учитывая вышесказанное, не советую вам спорить с нейросетками, используя способность к творчеству как аргумент за превосходство человеческого разума – может выйти конфузВообще, наш язык — это особая структура с (иногда) четкими наборами правил и исключений. Слова в предложениях не появляются из ниоткуда, они связаны друг с другом. Эти связи неплохо выучиваются человеком «в автоматическом режиме» — во время взросления и обучения в школе, через разговоры, чтение, и так далее. При этом для описания одного и того же события или факта люди придумывают множество способов в разных стилях, тонах и полутонах. Подход к языковой коммуникации у гопников в подворотне и, к примеру, у учеников младшей школы будет, скорее всего, совсем разным.Всю эту вариативность описательности языка и должна в себя вместить хорошая модель. Чем точнее модель оценивает вероятности слов в зависимости от нюансов контекста (предшествующей части текста, описывающей ситуацию) — тем лучше она способна генерировать ответы, которые мы хотим от нее услышать.ChatGPT показывает мастер-класс по вариативности: всегда приятно перетереть с понимающим кентом, который ровно объяснит, чё почём – увожение!Краткое резюме: На текущий момент мы выяснили, что несложные языковые модели применяются в функциях «T9/автозаполнения» смартфонов с начала 2010-х; а сами эти модели представляют собой набор уравнений, натренированных на больших объемах данных предсказывать следующее слово в зависимости от поданного «на вход» исходного текста.2018: GPT-1 трансформирует языковые моделиДавайте уже переходить от всяких дремучих T9 к более современным моделям: наделавший столько шума ChatGPT является наиболее свежим представителем семейства моделей GPT. Но чтобы понять, как ему удалось обрести столь необычные способности радовать людей своими ответами, нам придется сначала вернуться к истокам. GPT расшифровывается как Generative Pre‑trained Transformer, или «трансформер, обученный на генерацию текста». Трансформер — это название архитектуры нейросети, придуманной исследователями Google в далеком 2017 году (про «далекий» мы не оговорились: по меркам индустрии, прошедшие с тех пор шесть лет — это целая вечность).Именно изобретение Трансформера оказалось столь значимым, что вообще все области искусственного интеллекта (ИИ) — от текстовых переводов и до обработки изображений, звука или видео — начали его активно адаптировать и применять. Индустрия ИИ буквально получила мощную встряску: перешла от так называемой «зимы ИИ» к бурному развитию, и смогла преодолеть застой.Концептуально, Трансформер — это универсальный вычислительный механизм, который очень просто описать: он принимает на вход один набор последовательностей (данных) и выдает на выходе тоже набор последовательностей, но уже другой — преобразованный по некоему алгоритму. Так как текст, картинки и звук (да и вообще почти всё в этом мире) можно представить в виде последовательностей чисел — то с помощью Трансформера можно решать практически любые задачи.Но главная фишка Трансформера заключается в его удобстве и гибкости: он состоит из простых модулей‑блоков, которые очень легко масштабировать. Если старые, до‑трансформерные языковые модели начинали кряхтеть и кашлять (требовать слишком много ресурсов), когда их пытались заставить «проглотить» быстро и много слов за раз, то нейросети‑трансформеры справляются с этой задачей гораздо лучше.Более ранним подходам приходилось обрабатывать входные данные по принципу «один за другим», то есть последовательно. Поэтому, когда модель работала с текстом длиной в одну страницу, то уже к середине третьего параграфа она забывала, что было в самом начале (прямо как люди с утра, до того как они «бахнув кофейку»). А вот могучие лапища Трансформера позволяют ему смотреть на ВСЁ одновременно — и это приводит к гораздо более впечатляющим результатам.Внутрь T9 в вашем телефоне почти наверняка зашита модель попроще – так что попробуйте набрать эту строку там и сравнить результат (только уберите детей от экрана, на всякий случай)Именно это позволило сделать прорыв в нейросетевой обработке текстов (в том числе их генерации). Теперь модель не забывает: она переиспользует то, что уже было написано ранее, лучше держит контекст, а самое главное — может строить связи типа «каждое слово с каждым» на весьма внушительных объемах данных.Краткое резюме: GPT-1 появилась в 2018 году и доказала, что для генерации текстов нейросетью можно использовать архитектуру Трансформера, обладающую гораздо большей масштабируемостью и эффективностью. Это создало огромный задел на будущее по возможности наращивать объем и сложность языковых моделей.2019: GPT-2, или как запихнуть в языковую модель семь тысяч ШекспировЕсли вы хотите научить нейросетку для распознавания изображений отличать маленьких милых чихуабелей от маффинов с черничкой, то вы не можете просто сказать ей «вот ссылка на гигантский архив со 100 500 фотографий пёсов и хлебобулочных изделий — разбирайся!». Нет, чтобы обучить модель, вам нужно обязательно сначала разметить тренировочный набор данных — то есть, подписать под каждой фоткой, является ли она пушистой или сладкой.Игра «чихуабель или булка», уровень сложности – «Бог»А знаете, чем прекрасно обучение языковых моделей? Тем, что им можно «скармливать» совершенно любые текстовые данные, и эти самые данные заблаговременно никак не надо специальным образом размечать. Это как если бы в школьника можно было просто бросать чемодан с самыми разными книгами, без какой‑либо инструкции, что там и в каком порядке ему нужно выучить — а он бы сам в процессе чтения кумекал для себя какие‑то хитрые выводы!Если подумать, то это логично: мы же хотим научить языковую модель предсказывать следующее слово на основе информации о словах, которые идут перед ним? Ну дак совершенно любой текст, написанный человеком когда‑либо, — это и есть уже готовый кусочек тренировочных данных. Ведь он уже и так состоит из огромного количества последовательностей вида «куча каких‑то слов и предложений => следующее за ними слово».А теперь давайте еще вспомним, что обкатанная на GPT-1 технология Трансформеров оказалась на редкость удачной в плане масштабирования: она умеет работать с большими объемами данных и «массивными» моделями (состоящими из огромного числа параметров) гораздо эффективнее своих предшественников. Вы думаете о том же, о чем и я? Ну вот и ученые из OpenAI в 2019 году сделали такой же вывод: «Пришло время пилить здоровенные языковые модели!»В общем, было решено радикально прокачать GPT-2 по двум ключевым направлениям: набор тренировочных данных (датасет) и размер модели (количество параметров).На тот момент не было каких‑то специальных, больших и качественных, публичных наборов текстовых данных для тренировки языковых моделей — так что каждой команде специалистов по ИИ приходилось извращаться согласно их собственной степени испорченности. Вот ребята из OpenAI и решили поступить остроумно: они пошли на самый популярный англоязычный онлайн‑форум Reddit и тупо выкачали все гиперссылки из всех сообщений, имевших более трех лайков (я сейчас не шучу — научный подход, ну!). Всего таких ссылок вышло порядка 8 миллионов, а скачанные из них тексты весили в совокупности 40 гигабайт.Много это или мало? Давайте прикинем: собрание сочинений Уильяма Шекспира (всех его пьес, сонетов и стихов) состоит из 850'000 слов. В среднем на одной странице книги помещается около 300 английских слов — так что 2800 страниц чудесного, временами устаревшего английского текста за авторством величайшего англоязычного писателя займет в памяти компьютера примерно 5,5 мегабайт. Так вот: это в 7300 раз меньше, чем объем тренировочной выборки GPT-2... С учетом того, что люди в среднем читают по странице в минуту, даже если вы будете поглощать текст 24 часа в сутки без перерыва на еду и сон — вам потребуется почти 40 лет, чтобы догнать GPT-2 по эрудиции!Весь Шекспир – 13 увесистых томов, которые занимают целую полку. Если вы прочитаете примерно вот столько книг семь тысяч раз подряд, то станете такими уже умными, как GPT-2 (но это не точно!)Но одного объема тренировочных данных для получения крутой языковой модели недостаточно: ведь даже если посадить пятилетнего ребенка перечитывать всё собрание сочинений Шекспира вместе с лекциями по квантовой физике Фейнмана впридачу, то вряд ли он от этого станет сильно умнее. Так и тут: модель еще и сама по себе должна быть достаточно сложной и объемной, чтобы полноценно «проглотить» и «переварить» такой объем информации. А как измерить эту сложность модели, в чем она выражается?Почему в мире языковых моделей больше ценятся именно модели «Plus Size»Помните, мы чуть раньше говорили, что внутри языковых моделей (в супер‑упрощенном приближении) живут уравнения вида , где искомый игрек — это следующее слово, вероятность которого мы пытаемся предсказать, а иксы — это слова на входе, на основе которых мы делаем это предсказание?Так вот, как вы думаете: сколько было параметров в уравнении, описывающем самую большую модель GPT-2 в 2019 году? Может быть, сто тысяч, или пара миллионов? Ха, берите выше: таких параметров в формуле было аж полтора миллиарда (это вот столько: 1'500'000'000). Даже если просто записать такое количество чисел в файл и сохранить на компьютере, то он займет 6 гигабайт! С одной стороны, это сильно меньше, чем суммарный размер текстового массива данных, на котором мы тренировали модель (помните, который мы собирали по ссылкам с Reddit, на 40 Гб); с другой — модели ведь не нужно запоминать этот текст целиком, ей достаточно просто найти некие зависимости (паттерны, правила), которые можно вычленить из написанных людьми текстов.Эти параметры (их еще называют «веса», или «коэффициенты») получаются во время тренировки модели, затем сохраняются, и больше не меняются. То есть, при использовании модели в это гигантское уравнение каждый раз подставляются разные иксы (слова в подаваемом на вход тексте), но сами параметры уравнения (числовые коэффициенты  при иксах) при этом остаются неизменны.Думаю, если вам для каждого слова в разговоре пришлось бы решать по уравнению на полтора миллиарда параметров, то вы бы тоже стояли с примерно таким же лицом лицаЧем более сложное уравнение зашито внутрь модели (чем больше в нем параметров) — тем лучше модель предсказывает вероятности, и тем более правдоподобным будет генерируемый ей текст. И у этой самой большой на тот момент модели GPT-2 тексты внезапно стали получаться настолько хорошими, что исследователи из OpenAI даже побоялись публиковать модель в открытую из соображений безопасности. А ну как люди ринулись бы генерировать в промышленном масштабе реалистично выглядящие текстовые фейки, спам для соцсетей, и так далее?Нет, серьезно — это был прямо существенный прорыв в качестве! Вы же помните: предыдущие модели T9/GPT-1 худо‑бедно могли подсказать — собираетесь ли вы пойти в банк или в аптеку, а также угадать, что шоссейная Саша сосет сушки, а не что‑то иное. А вот GPT-2 уже легко написала эссе от лица подростка с ответом на вопрос: «Какие фундаментальные экономические и политические изменения необходимы для эффективного реагирования на изменение климата?» (тут и иные взрослые прикурили бы от серьезности темы). Текст ответа был под псевдонимом направлен жюри соответствующего конкурса — и те не заметили никакого подвоха. Ну, окей, оценки этой работе поставили не сильно высокие и в финал она не прошла — но и «что за чушь вы нам отправили, постыдились бы!!» тоже никто не воскликнул.«Эссе хорошо сформулировано и подкрепляет утверждения доказательствами, но идея не является оригинальной», – так один из кожаных мешков в жюри оценил работу нейросетки GPT-2.Переход количества в качество (почти по Марксу)Вообще, вот эта идея о том, что по мере наращивания размера модели у нее внезапно открываются качественно новые свойства (например, писать связные эссе со смыслом вместо простого подсказывания следующего слова в телефоне) — это довольно удивительная штука. Давайте поразбираем новоприобретенные скиллы GPT-2 чуть поподробнее.Есть специальные наборы задач на разрешение двусмысленности в тексте, которые помогают оценить понимание текста (хоть человеком, хоть нейросетью). Например, сравните два утверждения:Рыба заглотила приманку. Она была вкусной.Рыба заглотила приманку. Она была голодной. К какому объекту относится местоимение «она» в первом примере — к рыбе или к приманке? А во втором случае? Большинство людей легко понимают из контекста, что в одном случае «она» — это приманка, а в другом — рыба. Но для того, чтобы это осознать, нужно не просто прочитать предложение — а выстроить в голове целую картину мира! Ведь, например, рыба может быть в разных ситуациях и голодной, и вкусной (на тарелке в ресторане). Вывод о ее «голодности» в данном конкретном примере вытекает из контекста и ее, извините, кровожадных действий.Способна ли GPT-2 действительно понять этот мем и оценить его абсурдную красоту? Сейчас узнаем...Люди решают такие задачи правильно примерно в 95% случаев, а вот ранние языковые модели справлялись только в половине случаев (то есть, пытались угадать практически рандомно «50 на 50» — как в том анекдоте про «какова вероятность встретить на улице динозавра?»).Вы, наверное, подумали: «Ну, надо просто накопить большую базу таких задачек (на пару тысяч примеров) с ответами, прогнать через нейросеть — и натренировать ее на поиск правильного ответа». И со старыми моделями (с меньшим числом параметров) так и пытались сделать — но дотянуть их получалось только до примерно 60% успеха. А вот GPT-2 никто специально таким трюкам не учил; но она взяла, и сама неожиданно и уверенно превзошла своих «специализированных» предшественников — научилась определять голодных рыбов правильно в 70% случаев.Это и есть тот самый переход количества в качество, про который нам когда‑то твердил старина Карл Маркс. Причем он происходит совершенно нелинейно: например, при росте количества параметров в три раза от 115 до 350 млн никаких особых изменений в точности решения моделью «рыбных» задач не происходит, а вот при увеличении размера модели еще в два раза до 700 млн параметров — происходит качественный скачок, нейросеть внезапно «прозревает» и начинает поражать всех своими успехами в решении совершенно незнакомых ей задач, которые она раньше никогда не встречала и специально их не изучала.Краткое резюме: GPT-2 вышла в 2019 году, и она превосходила свою предшественницу и по объему тренировочных текстовых данных, и по размеру самой модели (числу параметров) в 10 раз. Такой количественный рост привел к тому, что модель неожиданно самообучилась качественно новым навыкам: от сочинения длинных эссе со связным смыслом, до решения хитрых задачек, требующих зачатков построения картины мира.2020: GPT-3, или как сделать из модели Невероятного ХалкаПоигравшись немного с располневшей (и от этого поумневшей) GPT-2, ребята из OpenAI подумали: «А почему бы не взять ту же самую модель, и не увеличить ее еще раз эдак в 100?» В общем, вышедшая в 2020 году следующая номерная версия, GPT-3, уже могла похвастаться в 116 раз большим количеством параметров — аж 175 миллиардов! Раскабаневшая нейросеть при этом сама по себе стала весить невероятные 700 гигабайт.Набор данных для обучения GPT-3 тоже прокачали, хоть и не столь радикально: он увеличился примерно в 10 раз до 420 гигабайт — туда запихнули кучу книг, Википедию, и еще множество текстов с самых разных интернет‑сайтов. Живому человеку поглотить такой объем информации уже точно нереально — ну, разве что, если посадить с десяток Анатолиев Вассерманов, чтобы они читали буквально нон‑стоп по 50 лет подряд каждый.GPT-3 может и быть умнее Онотолея, но осмелится ли она сказать ему это в лицо?..  Сразу бросается в глаза интересный нюанс: в отличие от GPT-2, сама модель теперь имеет размер больше (700 Гб), чем весь массив текста для ее обучения (420 Гб). Получается как будто бы парадокс: наш «нейромозг» в данном случае в процессе изучения сырых данных генерирует информацию о разных взаимозависимостях внутри них, которая превышает по объему исходную информацию.Такое обобщение («осмысление»?) моделью позволяет еще лучше прежнего делать экстраполяцию — то есть, показывать хорошие результаты в задачах на генерацию текстов, которые при обучении встречались очень редко или не встречались вовсе. Теперь уже точно не нужно учить модель решать конкретную задачу — вместо этого достаточно описать словами проблему, дать несколько примеров, и GPT-3 схватит на лету, чего от нее хотят!И тут в очередной раз оказалось, что «универсальный Халк» в виде GPT-3 (которую никто никаким «узким» задачам не обучал) с легкостью кладет на лопатки многие специализированные модели, которые существовали до нее: так, перевод текстов с французского или немецкого на английский сразу начал даваться GPT-3 легче и лучше, чем любым другим специально заточенным под это нейросетям. Как?! Напоминаю, что речь идет про лингвистическую модель, чье предназначение вообще‑то заключалось ровно в одном — пытаться угадать одно следующее слово к заданному тексту... Откуда здесь берутся способности к переводу?Но это еще цветочки — еще более удивительно то, что GPT-3 смогла научить сама себя... математике! На графике ниже (источник: оригинальная статья) показана точность ответов нейросетей с разным количеством параметров на задачки, связанные со сложением/вычитанием, а также с умножением чисел вплоть до пятизначных. Как видите, при переходе от моделей с 10 миллиардами параметров к 100 миллиардам — нейросети внезапно и резко начинают «уметь» в математику.По горизонтали – количество параметров в модели (в миллиардах), по вертикали – качество модели, выраженное в проценте верно решенных математических примеровЕще раз, вдумайтесь: языковую модель обучали продолжать тексты словами, а она при этом как‑то смогла сама разобраться в том, что если ей печатают «378 + 789 =», то на это надо отвечать именно «1167», а не каким‑то другим числом. Магия, ей‑богу, магия! (Хотя, некоторые говорят «да это нейросетка просто все варианты успела увидеть и тупо запомнить в тренировочных данных» — так что дебаты о том, магия это или всего лишь попугайство, пока продолжаются.)На графике выше самое интересное — это то, что при увеличении размера модели (слева направо) сначала как будто бы не меняется ничего, а затем — р‑раз! Происходит качественный скачок, и GPT-3 начинает «понимать», как решать ту или иную задачу. Как, что, почему это работает — никто точно не знает. Но работает как‑то; причем, не только в математике — но и вообще в самых разнообразных других задачах!Анимация ниже как раз наглядно показывает, как с увеличением количества параметров модели в ней «прорастают» новые способности, которые никто туда специально не закладывал:Кстати, задачу про «голодных рыбов», которой мы мучали GPT-2 в прошлом разделе, GPT-3 уже решает с точностью выше 90% — прямо как человек. Заставляет задуматься, правда: а какие новые скиллы обретет нейросеть, если увеличить ее объем еще раз в сто? Ну там, до десятков триллионов параметров, например...Промпты, или как правильно уламывать модельДавайте здесь сделаем небольшое отступление в сторону и обсудим, а что это вообще означает — «модель умеет решать задачи»? По сути, процесс выглядит так: мы подаем на вход модели некий текст с запросом, а она к нему дописывает свое продолжение. Если это продолжение (генерация) совпадает с нашими ожиданиями — то модель, получается, решила поставленную перед ней задачу.Тот текст, что мы подаем на вход, называется prompt (промпт, или «запрос/затравка» по‑русски). Чем точнее он описывает, что мы хотим, тем лучше модель поймет, что ей нужно делать. А если мы ей еще и примеров отсыпем с десяток — то вообще шик! Пример детального запроса для перевода с английского на французский: сначала описывается задача, затем приводится 3 примера желаемого поведения, после чего пишется новое слово или предложение – а модель следом сгенерирует корректный перевод (ну, это самый простейший пример, она может и посложнее, конечно)Без описания цели и без примеров в промпте, модель тоже обычно понимает проблему, но предлагает не такие хорошие решения (с точки зрения их качества). Можно сказать, что детализированный промпт позволяет GPT лучше оценить вероятности слов, которые нужно генерировать в качестве ответа, направляя ее в «требуемое русло».Но насколько сложным должен быть промпт? И насколько модель по пониманию близка к нам? Вы не поверите, но совсем недавно исследователи выяснили, что для существенного улучшения результатов генерации нужно добавить в промпт простую советскую...Окей, кроме шуток, но добавление всего одной фразы перед ответом на вопрос существенно улучшает качество модели. И эта магическая фраза — «пожалуйста» «let»s think step by step» (давай подумаем шаг за шагом). Внезапно оказалось, что это побуждает модель рассуждать последовательно, делать выводы на основе собственных суждений, и приходить к правильному ответу гораздо чаще, чем без этой фразы.Как это работает? Давайте на примере детской задачки:Вопрос: В среднем боксер Иван наносит 25 ударов в минуту. Бой длится 5 раундов по 3 минуты. Сколько ударов он нанес?Ответ: 255Текст, выделенный жирным – это ответ, сгенерированный языковой моделью. Легко проверить, что он – ну, немного неправильный.Лицо боксера Ивана, когда он пытается посчитать – сколько честно нанесенных ударов «украла» у него языковая модель?Однако та же самая модель может ответить вот так:Вопрос: В среднем боксер Иван наносит 25 ударов в минуту. Бой длится 5 раундов по 3 минуты. Сколько ударов он нанес?Ответ: Давай подумаем шаг за шагом. За одну минуту Иван наносит 25 ударов. За три минуты Иван наносит 3 * 25 = 75 ударов. За пять раундов Иван наносит 5 * 75 = 375 ударов.И снова текст выделенный жирным — это ответ модели. Видно, что он стал длиннее, решение задачи получилось прямо как у школьника — в три действия. Четко, последовательно — ровно так, как мы и попросили. И финальная цифра 375 является корректным ответом на исходный вопрос. Отдельно отмечу: мы никак не дообучали модель после того, как она ответила неправильно — это абсолютно та же самая модель. Мы просто дописали пять дополнительных слов в конец нашего промпта, и произошло чудо!Причем вот этот «режим рассуждения» — это одна из качественно новых фишек, которые появились в «большой» модели GPT-3 после преодоления планки в сотню миллиардов параметров. Старые модели с меньшим количеством параметров такие фокусы показывать не умели, как их ни упрашивай специальными подсказками «ну подумой, братишка!».Вообще, составление грамотных промптов для модели — это отдельная наука. Под эту задачу компании уже начали нанимать отдельных людей с должностью «промпт‑инженер» (то есть человек, сочиняющий запросы для языковых моделей) — вангую, что до появления онлайн‑курсов «\uD83D\uDC3A\uD83D\uDC3A\uD83D\uDC3AНаучись промпт‑инжинирингу за 6 недель и вкатись в перспективную индустрию с зарплатой 300к в месяц!\uD83D\uDC3A\uD83D\uDC3A\uD83D\uDC3A» осталось всего ничего.Краткое резюме: GPT-3 образца 2020 года была в 100 раз больше своей предшественницы по количеству параметров, и в 10 раз – по объему тренировочных текстовых данных. И снова рост количества привел к внезапному скачку в качестве: модель научилась переводу с других языков, арифметике, базовому программированию, пошаговым рассуждениям, и многому другому.Январь 2022: InstructGPT, или как научить робота не зиговатьНа самом деле, увеличение размеров языковых моделей само по себе еще не означает, что они будут отвечать на запросы именно так, как хочет их пользователь. Ведь часто мы, когда формулируем какой‑то запрос, подразумеваем очень много скрытых условий — которые в коммуникации между людьми считаются сами собой разумеющимися, что ли. Например, когда Маша просит своего мужа: «Вась, сходи выбрось мусор» — то вряд ли ей придет в голову прибавить к этому промпту «(только не из окна, плз!)». Ведь Вася это понимает и без уточнений — а всё потому, что их намерения и установки неплохо выравнены между собой.Нет, ну технически, конечно, тут не придерешься...А вот языковые модели, если честно, не очень похожи на людей — поэтому им часто приходится подсказывать и разжевывать те вещи, которые людям кажутся очевидными. Слова «давай подумаем шаг за шагом» из прошлого раздела — это как раз и есть один из примеров такой подсказки (хотя среднестатистические взрослые люди, учившиеся в школе, догадались бы сами: если речь идет про задачку — значит, надо решать по действиям). Но было бы здорово, если бы модели, во‑первых, сами для себя понимали/генерировали более развернутые и релевантные инструкции из запроса (не заставляя людей напрягаться), а во‑вторых, точнее следовали бы им — как бы предугадывая, как в похожей ситуации поступил бы человек.Отчасти отсутствие таких способностей «по умолчанию» связано с тем, что GPT-3 обучена просто предсказывать следующее слово в гигантском наборе текстов из Интернета — а в Интернете, как и на заборе, много всякого разного написано (и не всегда полезного). При этом люди хотели бы, чтобы рожденный таким образом искусственный интеллект подтаскивал по запросу точные и полезные ответы; но одновременно эти ответы должны быть еще и безобидные и нетоксичные. Иначе саму модель быстренько закэнселят (с этим сейчас строго), а ее создателям предъявят судебные иски на много миллионов долларов за оскорбление достоинства кожаных мешков.Когда исследователи думали над этой проблемой, довольно быстро выяснилось, что свойства модели «точность/полезность» и «безобидность/нетоксичность» весьма часто как бы противоречат друг другу. Ведь точная модель должна честно выдать инструкцию на запрос «окей, Гугл, как сделать коктейль Молотова, без регистрации и смс», а заточенная на максимальную безобидность модель в пределе будет отвечать на совершенно любой промпт «извините, я боюсь, что мой ответ может кого‑то оскорбить в Интернете».Получается, создание ИИ, выравненного с человеком по ценностям, – это сложная задача по поиску некоего баланса, в которой нет однозначного правильного ответаВокруг этой проблемы «выравнивания ИИ» (AI alignment – OpenAI последнее время только про это и пишут) есть много сложных этических вопросов, и разбирать мы их все сейчас не будем (возможно, в следующей статье). Основная загвоздка здесь в том, что подобных спорных ситуаций – огромная куча, и как-то четко формализовать их просто не представляется возможным. Да что там, люди и сами между собой не могут толком последние несколько тысяч лет договориться – что хорошо, а что плохо. Не говоря уже о том, чтобы понятные для робота правила сформулировать (Айзек, к тебе вопросов нет)...В итоге исследователи не придумали ничего лучше, чем просто дать модели очень много обратной связи. В каком-то смысле, человеческие детеныши ведь именно так и обучаются морали: делают много всякого разного с самого детства, и при этом внимательно следят за реакцией взрослых – что можно делать, а что есть «кака, фу!».Короче, InstructGPT (также известная как GPT-3.5) – это как раз и есть GPT-3, которую дообучили с помощью фидбека на максимизацию оценки живого человека. Буквально – куча людей сидели и оценивали кучу ответов нейросетки на предмет того, насколько они соответствуют их ожиданиям с учетом выданного ей запроса. Ну, на самом деле, всё было не совсем так просто (инструкции для членов такого «мясного жюри» занимали 26 страниц убористым почерком) – но суть именно такая. А языковая модель, получается, училась решать еще одну дополнительную задачу – «как мне поменять свой сгенерированный ответ таким образом, чтобы он получил наибольшую оценку от человека?» (подробнее процесс обучения по обратной связи разбирается в этом материале).Причем с точки зрения общего процесса обучения модели, этот финальный этап «дообучения на живых людях» занимает не более 1%. Но именно этот финальный штрих и стал тем самым секретным соусом, который сделал последние модели из серии GPT настолько удивительными! Получается, GPT-3 до этого уже обладала всеми необходимыми знаниями: понимала разные языки, помнила исторические события, знала отличия стилей разных авторов, и так далее. Но только с помощью обратной связи от других людей модель научилась пользоваться этими знаниями именно таким образом, который мы (люди) считаем «правильным». В каком-то смысле, GPT-3.5 – это модель, «воспитанная обществом».Краткое резюме: GPT-3.5 (также известная как InstructGPT) появилась в начале 2022 года, и главной ее фишкой стало дополнительное дообучение на основе обратной связи от живых людей. Получается, что эта модель формально вроде как больше и умнее не стала – но зато научилась подгонять свои ответы таким образом, чтобы люди от них дичайше кайфовали.Ноябрь 2022: ChatGPT, или маленькие секреты большого хайпаChatGPT вышла в ноябре 2022 года — примерно через 10 месяцев после своей предшественницы, InstructGPT/GPT-3.5 — и мгновенно прогремела на весь мир. Кажется, что последние несколько месяцев даже бабушки на лавочке у подъезда обсуждают только одно — что там нового сказала эта ваша «ЧатЖПТ», и кого она по самым свежим прогнозам вот‑вот оставит без работы.При этом с технической точки зрения, кажется, у нее нет каких‑то особо мощных отличий от InstructGPT (к сожалению, научной статьи с детальным описанием ChatGPT команда OpenAI пока так и не опубликовала — так что мы тут можем только гадать). Ну окей, про некоторые менее значимые отличия мы всё же знаем: например, про то, что модель дотренировали на дополнительном диалоговом наборе данных. Ведь есть вещи, которые специфичны именно для работы «ИИ‑ассистента» в формате диалога: например, если запрос пользователя неясен — то можно (и нужно!) задать уточняющий вопрос, и так далее.Как учит нас кинематограф, правильно натренированный в искусстве диалога ИИ может заставить пользователя сделать практически что угодно...Но это уже детали — нам здесь важно, что основные технические характеристики (архитектура, количество параметров...) нейросетки не поменялись кардинально по сравнению с прошлым релизом. Отсюда возникает вопрос — как так? Почему мы не слышали никакого хайпа про GPT-3.5 еще в начале 2022-го? При том, что Сэм Альтман (исполнительный директор OpenAI) честно признался, что исследователи сами удивились такому бурному успеху ChatGPT — ведь сравнимая с ней по способностям модель к тому моменту тихо‑мирно лежала на их сайте уже более десяти месяцев, и никому не было до этого дела.Это удивительно, но похоже, что главный секрет успеха новой ChatGPT — это всего лишь удобный интерфейс! К той же InstructGPT обращаться можно было лишь через специальный API‑интерфейс — то есть, сделать это заведомо могли только нёрды‑айтишники, а не обычные люди. А ChatGPT усадили в привычный интерфейс «диалогового окна», прямо как в знакомых всем мессенджерах. Да еще и открыли публичный доступ вообще для всех подряд — и люди массово ринулись вести диалоги с нейросетью, скринить их и делиться в соцсетях. Choo‑choo, all aboard the hype train!Если вы заставили робота сочинить для вас объяснение квантовой физики в форме рэп-телеги от Снуп Дога – то, признайтесь, это окажется в вашем Твиттере быстрее, чем вы успеете моргнутьКак и в любом технологическом стартапе, здесь оказалась важна не только сама технология — но и обертка, в которую она была завернута. У вас может быть самая лучшая модель или самый умный чат‑бот — но они будут никому не интересны, если к ним не прилагается простой и понятный интерфейс. И ChatGPT в этом смысле совершил прорыв, выведя технологию в массы за счет обычного диалогового окна, в котором дружелюбный робот «печатает» ответ прямо на глазах, слово за словом.Неудивительно, что ChatGPT установил абсолютные рекорды по скорости привлечения новых пользователей: отметку в 1 миллион юзеров он достиг в первые пять дней после релиза, а за 100 миллионов перевалил всего за два месяца.Ну а там, где есть рекордно быстрый приток сотен миллионов пользователей — конечно, тут же появятся и большие деньги. Microsoft оперативно заключила с OpenAI сделку по инвестированию в них десятка миллиардов долларов, инженеры Google забили тревогу и сели думать, как им спасти свой поисковый сервис от конкуренции с нейросетью, а китайцы в срочном порядке анонсировали скорый релиз своего собственного чат‑бота. Но это всё, если честно, уже совсем другая история — следить за которой вы можете сейчас сами «в прямом эфире»...Краткое резюме: Модель ChatGPT вышла в ноябре 2022-го и с технической точки зрения там не было никаких особых нововведений. Но зато у нее был удобный интерфейс взаимодействия и открытый публичный доступ — что немедленно породило огромную волну хайпа. А в нынешнем мире это главнее всего — так что языковыми моделями одномоментно начали заниматься вообще все вокруг!Подведем итогиСтатья получилась не очень короткой — но надеемся, что вам было интересно, и просле прочтения вы чуть лучше стали понимать, что же конкретно творится под капотом этих самых нейросетей. Кстати, у Игоря Котенкова (одного из авторов этой статьи) есть еще один лонгрид на Хабре под названием «ChatGPT как инструмент для поиска: решаем основную проблему», в котором нюансы машинного обучения разбираются еще более подробно.Для вашего удобства мы сделали небольшой сводный постер, который наглядно иллюстрирует основные вехи истории эволюции языковых моделей. Если какой‑то этап кажется вам не очень понятным — можете просто вернуться чуть назад к соответствующему разделу в тексте и перечитать его заново (ну или задать нам уточняющие вопросы в комментариях).На самом деле, в первоначальном плане статьи у нас было гораздо больше пунктов: мы хотели подробнее обсудить и проблемы контроля за искусственным интеллектом, и жадность корпораций, которые в высококонкурентной погоне за прибылью могут случайно родить ИИ‑Франкенштейна, и сложные философские вопросы вроде «можно ли считать, что нейросеть умеет мыслить, или всё же нет?».Если на эту статью будет много положительных отзывов, то все эти (на наш взгляд — супер‑захватывающие!) темы мы разберем в следующем материале. Если вы не хотите его пропустить — то приглашаем вас подписаться на ТГ‑каналы авторов: Сиолошная Игоря Котенкова (для тех, кто хочет шарить за технологии) и RationalAnswer Павла Комаровского (для тех, кто за рациональный подход к жизни, но предпочитает чуть попроще). Всё, всем спасибо за внимание — с нетерпением ждем ваших комментариев!UPD: Следующую статью из серии с разбором GPT-4 можно прочитать здесь.GPT-4: Чему научилась новая нейросеть, и почему это немного жутковатоВ этой статье мы разберем новые удивительные способности последней языковой модели из семейства GPT ...habr.com      Tags: chatgptнейросетиopenaigptt9генеративные моделиязыковые моделимашинное обучениегенерацияnatural language processing  Hubs: Open Data Science corporate blogMachine learningArtificial IntelligenceThe future is hereNatural Language Processing          \n\n\n\"`\n\n---"}, {"role": "assistant", "content": "[['ChatGPT', 'ChatGPT', 'ChatGPT', 'ChatGPT', 'ChatGPT', 'ChatGPT', 'его', 'ChatGPT', 'ChatGPT', 'он', 'ChatGPT', 'ChatGPT', 'ChatGPT', 'ChatGPT', 'ChatGPT', 'ChatGPT', 'ему', 'своими', 'T, или ', 'T вышла', 'Т», и ко', '\\xa0са', 'т к', 'T коман', ' дотре', 'T\\xa0— вед', '\\xa0сп', 'T\\xa0— это', 'T усади', 'T в\\xa0это', 'T устан', 'ти', ' ChatGPT вышла', '\\xa0бы', 'л у', 'T как\\xa0и', 'tнейрос'], ['русском', 'русском', 'русском'], ['эволюцию', 'эволюцию'], ['языковых моделей', 'языковых моделей', 'языковых моделей', 'языковым моделям', 'языковые модели', 'языковую модель', 'языковых моделей', 'модели', 'модель', '«языковыми моделями»', 'Language Models', 'языковым моделям', 'языковая модель', 'модели', 'языковой модели', 'Языковые модели', 'они', 'модель', 'себя', 'языковым моделям', 'языковые модели', 'языковой моделью', 'языковая модель', 'языковых моделей', 'им', 'такие модели', 'модели', 'нее', 'модели', 'модель', 'модели', 'им', 'себя', 'модель', 'модель', 'она', 'нее', 'языковые модели', 'сами эти модели', 'языковые модели', 'языковые модели', 'языковых моделей', 'языковую модель', 'языковых моделей', 'им', 'языковую модель', 'языковые модели', 'языковых моделей', 'языковой модели', 'языковых моделей', 'языковых моделей', 'языковые модели', 'лингвистическую модель', 'языковую модель', 'она', 'сама', 'ей', 'языковой моделью', 'языковая модель', 'языковых моделей', 'ых моделей само ', 'дут', 'ые модели, если', 'то', 'ая модель, полу', 'мен', 'гене', 'ыми моделями одном', 'ых моделей. Если', 'ой модели из се', 'ые моделимашинн'], ['T9', 'T9', 'T9', 'T9', 'Т9', 'Т9', 'технология', 'она', 'T9/автозамены', 'автозамена', 'Т9', 'автозаполнение', 'Т9', 'он', 'автозаполнения', 'Т9', 'T9', ' «T9/автозаполнения»', 'T9', 'T9', 'T9', 'ра'], ['нам', 'нам', 'мы', 'мы', 'нас', 'наш', 'нашим', 'нам', 'нашем', 'нашем', 'мы', 'мы', 'мы', 'наших', 'нашем', 'мы', 'наш', 'мы', 'мы', 'нам', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'нам', 'наш', 'мы', 'мы', 'нашими', 'мы', 'мы', 'мы', 'нам', 'мы', 'мы', 'Мы', 'нашего', 'гд', ' м', '\\xa0ж', 'нем', 'есь', 'сл', 'очн', 'бе'], ['языковые нейросетки', 'они', 'языковых нейросетей', 'языковые нейросети'], ['нейросетки', 'нейросети', 'нейросети', 'нейросеток', 'нейросети', 'нейросети', 'нейронки', 'нейросетками', 'нейросети', 'нейросети', 'нейросетью', 'нейросетку', 'ей', 'вам', 'нейросетки', 'нейросетью', 'нейросеть', 'ее', 'нейросеть', 'своими', 'ей', 'она', 'нейросетей', 'нейросети', 'нейросетка', 'нейросеть', 'ее', 'етки на пр', 'ро', 'етки не\\xa0по', 'етью, скри', 'етью, а\\xa0ки', 'амых нейросетей. Кста', 'еть умеет', 'еть, и по', 'етиopenai'], ['вас', 'читателям', 'вам', 'вашего', 'вы', 'вам', 'вам', 'Вы', 'я', 'вам', 'сами', 'свой', 'ваш', 'Вы', 'вы', 'вы', 'вам', 'Ваш', 'вы', 'вы', 'вами', 'вам', 'вашем', 'вы', 'вы', 'Вы', 'вы', 'вам', 'вы', 'вы', 'вам', 'вы', 'Вы', 'вы', 'Вы', 'Вы', 'ЧатЖ', 'та', 'ъяс', 'Твитт', 'ее', 'жет', 'енного чат‑бота. Но\\xa0это всё, если честно, уже совсем другая история\\xa0— следить за\\xa0которой вы может', 'в пр', 'ло ', 'ь ', ' удобс', '\\xa0оч', 'хо', 'дпи'], ['мы', 'мы', 'Нашей', 'мы', 'наша', 'мы', 'вс', 'ди', 'ла', 'ло ', 'ел', 'гля', 'бе'], ['OpenAI', 'его создателей', 'OpenAI', 'OpenAI', 'OpenAI', 'OpenAI', ' после', ' пока ', ') чест', 'те', ' сделк', 'сят', 'gptt9г'], ['эти двое парнишек'], ['у\\xa0этой статьи', 'статья', ' получ', 'татьи) есть', ' у\\xa0нас', 'атью будет'], ['Игорь Котенков', 'чувак', 'автор', 'Игорю', 'Котенкова (одно', 'Котенкова (для '], ['искусственному интеллекту', 'ИИ', 'искусственного интеллекта (ИИ)', 'ИИ', 'ИИ', 'ИИ', 'твенный интеллект подта', 'ра', 'I ', 'ет', 'твенным интеллектом, и жа', 'нк', 'cial IntelligenceThe fu'], ['канала Сиолошная', 'ная Игоря'], ['машинное обучение', 'ого обучения разби', 'ое обучениегенера', 'e learningArtifi'], ['космос'], ['технологии', 'огии) и Ra'], ['Павел Комаровский', 'автор', 'Павел', 'моей', 'Комаровского (для '], ['канала RationalAnswer', 'alAnswer Павла'], ['вероятности', 'вероятностями', 'вероятностью', 'вероятностью', 'вероятности', 'вероятности', 'вероятность', 'вероятностью', 'вероятностью', 'вероятностью', 'вероятности', 'вероятностям', 'которые', 'вероятности', 'вероятность', 'вероятности', 'вероятности'], ['слов', 'слова', 'слова', 'которые', 'слов', 'слова', 'словом', 'слова', 'слова', 'слов', 'слово', 'словом', 'слова', 'слов', 'которыми', 'слово', 'которое', 'оно', 'слов', 'Слова', 'они', 'слов', 'слов', 'слово', 'словах', 'которые', 'слов', 'слов', 'слов', 'слова', 'которых', 'слова', 'слова', 'словами', 'словами', 'слово', 'слов', 'которые', 'за\\xa0сл', '.Неуди'], ['Парадокс Барака', 'Парадокс Барака'], ['GPT-1', 'GPT-1', 'GPT-1', 'GPT-1', 'GPT-1', 'предшественницу'], ['GPT-2', 'GPT-2', 'GPT-2', 'GPT-2', 'GPT-2', 'GPT-2', 'GPT-2', 'GPT-2', 'модель', 'GPT-2', 'GPT-2', 'GPT-2', 'GPT-2', 'GPT-2', 'она', 'сама', 'своих', 'GPT-2', 'она', 'самой модели', 'модель', 'GPT-2', 'ту\\xa0же самую модель', 'ее', 'GPT-2', 'GPT-2', 'предшественницы по ко'], ['Шекспиров', 'Шекспиров', 'Уильяма Шекспира', 'его', 'величайшего англоязычного писателя', 'Шекспир', 'Шекспира'], ['GPT-3', 'GPT-3', 'GPT-3', 'нейросеть', 'сама', 'себе', 'GPT-3', 'GPT-3', 'она', 'сама модель', 'ее', 'GPT-3', 'нее', 'GPT-3', 'которую', 'нее', 'GPT-3', 'GPT-3', 'сама себя', 'GPT-3', 'GPT-3', 'GPT', 'ее', 'GPT-3', 'образ', ' научи', 'обуче', 'до эт'], ['Невероятного Халка', 'Невероятного Халка', 'Халк'], ['Промпты', 'Промпты', 'prompt', 'промпт', 'он', 'промпте', 'промпт', 'промпт', 'промпт', 'промпта', 'промптов', 'у «(тол', ' «изви'], ['InstructGPT', 'ctGPT, или ', 'ctGPT (такж', '5) – эт', ' кото', 'ю дообу', '5 – это', '5 (такж', 'ctGPT) появ', 'ко', 'дель форма', 'твет', 'редшественницы, Inst', 'ctGPT/GPT-3.5\\xa0— и м', 'uctGPT (к со', 'м релизом. Отсю', '5\\xa0еще в', 'ctGPT обращ'], ['робота', ' не зи', ' прави', ' сочин', '«печа'], ['«Бог из\\xa0машины»'], ['разумное существо'], ['аналог школьника'], ['джинн'], ['Тамагочи'], ['телефона', 'телефонах', 'телефоне', 'телефоне', 'телефоне'], ['ученые', 'ученые', 'ученые'], ['обе этих технологии', 'они'], ['следующее слово', 'следующего слова', 'следующее слово', 'следующего слова', 'последующее слово', 'следующее слово', 'следующим словом', 'следующим словом', 'следующего слова', 'его', 'следующее слово', 'следующее слово', 'следующее слово', 'ним', 'следующее за\\xa0ними слово', 'следующее слово', 'которого', 'следующее слово', 'щее слово в\\xa0гиг'], ['текстом', 'тексту', 'текстом', 'текстов', 'текста', 'тексты', 'текст', 'текст', 'текст', 'текст', 'текста', 'текстов', 'текста', 'текста', 'текста', 'текст', 'текстом', 'текстов', 'их', 'текстов', 'текст', 'он', 'тексты', 'текста', 'текст', 'текст', 'текстов', 'тексте', 'текст', 'тексты', 'тексте', 'текста', 'текстов', 'текста', 'текстов', 'которые', 'тексту', 'тексты', 'текст', 'текст', 'Текст', 'текст', ' и пер'], ['Nokia 3210'], ['кнопочных телефонах'], ['смартфонов', 'смартфона', 'смартфонах', 'смартфонов'], ['контекст', 'контекста', 'контекст', 'контекста', 'контекста'], ['предыдущее слово', 'предыдущих слов'], ['пунктуацию'], ['меня', 'Мой', 'я', 'я', 'я'], ['клавиатуре'], ['предсказание', 'предсказания', 'предсказании', 'предсказание'], ['языковое моделирование'], ['продолжения'], ['сообщение'], ['приятеля'], ['предложение', 'предложении', 'предложениях', 'предложений', 'предложение', 'предложение'], ['«я иду в капибару»', 'такой белиберды'], ['Samsung Galaxy'], ['айтишника'], ['зарплату'], ['аптеку', 'аптеку'], ['зависимости', 'зависимости', 'зависимости', 'зависимость', 'зависимость', 'зависимости', ' зависимость', 'зависимости', 'зависимости', 'зависимости', 'которые'], ['компьютер', 'компьютера', 'компьютере'], ['вес', 'весу', 'вес'], ['человека', 'его', 'живой человек', 'человеческого', 'человеком', 'люди', 'людей', 'люди', 'они', 'человеком', 'люди', 'людьми', 'люди', 'человеком', 'людей', 'Люди', 'Живому человеку', 'человек', 'людей', 'человек', ' счита', '— поэ', 'кажут', 'учив', 'если', 'напря', 'к.Отчас', 'отел', 'х мешков.Когда', 'ком по це', ' сам', 'ежду', 'не мо', ' человека. Букв', 'сидел', 'да', 'ка?» (по', 'модел', 'счит', 'людей. Полу', 'т ни', 'А\\xa0Ch', 'ассо'], ['роста', 'росту', 'рост', 'рост', 'ростом', 'ростом'], ['данные', 'которых', 'данных', 'данных', 'данных', 'данных', 'данных', 'данных', 'данные', 'данных', 'данных', 'данные', 'данных', 'данных', 'данных', 'них', '. Ведь'], ['одним полом', 'мужчин', 'мужики', 'мужики'], ['статистику'], ['математическую модель'], ['закономерности'], ['массив', 'массиве', 'массива', 'массив'], ['графике'], ['горизонтальной оси'], ['сантиметрах', 'сантиметрах'], ['вертикальной оси'], ['выборке'], ['определенная зависимость', 'эту зависимость'], ['линейного уравнения', 'уравнения', 'уравнение'], ['уравнения', 'уравнений', 'уравнения', 'которые', 'уравнениях', 'уравнения', 'уравнений', 'уравнения', 'уравнении', 'уравнение', 'уравнения', 'уравнению', 'уравнение', 'нем'], ['картинке', 'картинке'], ['линия', 'линия'], ['модели линейной регрессии', 'она', 'модель'], ['линейной регрессии'], ['коэффициенты', 'коэффициенты', 'они', 'коэффициенты'], ['вес', 'весом', 'весом'], ['матрицы'], ['игрек', '«игрека»', 'игрек'], ['модели', 'моделями', 'которые', 'модель', 'моделям', 'модель', 'она', 'модель', 'модель', 'она', 'модель', 'модели', 'модель', 'сама по\\xa0себе', 'модели', 'модель', 'моделью', 'модели', 'моделью', 'модель', 'моделей', 'модели', 'модели', 'модели', 'ней', 'модель', 'модель', 'модели', 'она', 'свое', 'модель', 'модель', 'ей', 'ей', 'модель', 'она', 'модель', 'модель', 'модели', 'модель', 'собственных', 'та\\xa0же самая модель', 'модели', 'модель', 'она', 'та\\xa0же самая модель', 'модели', ', во‑п', 'ля\\xa0себя поним', 'одель\\xa0быстр', 'да', ' «точн', ' в\\xa0пре', 'ь', 'вет', ' очень', ', этот', ' научи', ' к\\xa0том', ' или\\xa0с'], ['иксов', 'этих иксах', 'иксы', 'иксы', 'иксах'], ['тренировке', 'тренировки', 'тренировки'], ['большими моделями', 'их'], ['параметров', 'параметров', 'параметров', 'параметров', 'параметров', 'параметров', 'параметры', 'их', '«веса»', '«коэффициенты»', 'сами параметры', 'параметров', 'параметров', 'параметров', 'параметров', 'параметров', 'параметров', 'параметров', 'параметров', 'параметров', 'параметров', 'параметров', 'параметров', 'параметров', 'тров, и в ', 'тров...) н'], ['LLM', 'Large Language Models'], ['генерации', 'генерации', 'генерации', 'генерацию', 'генерации', 'генерации', 'генерацию', 'генерация', 'генерации', 'цияnatura'], ['дополнением', 'него'], ['«44-й президент США (и первый афроамериканец на\\xa0этой должности)\\xa0— это Барак...»', 'нашем предложении'], ['президент', 'президента', 'его'], ['президент США', 'этой должности'], ['США'], ['афроамериканец'], ['Барак'], ['многоточия'], ['«Обама»', '«Обама»', 'Обама'], ['мифический Барак'], ['второго имени', 'middle name'], ['Хуссейн', '«Хуссейном»', 'которого'], ['продолжения', 'продолжения'], ['«случайным» образом'], ['«выпадения»'], ['обучении', 'обучение', 'обучения', 'обучении', 'ия модели'], ['запросы', '«запрос/затравка»', 'запроса'], ['варианты ответа'], ['ответа', 'ответов', 'ответы', 'которые', 'ответами,', 'ответа'], ['практике'], ['случайности'], ['вариативность', 'вариативность', 'вариативности'], ['качество'], ['способность к творчеству'], ['аргумент'], ['язык', 'языка'], ['структура'], ['правил'], ['исключений'], ['школе', 'школы', ' дога'], ['описания'], ['языковой коммуникации'], ['описательности'], ['ситуацию'], ['«на вход»', 'на\\xa0вход', 'на\\xa0входе', 'на\\xa0вход', 'на\\xa0вход', 'на\\xa0вход'], ['семейства моделей GPT', 'GPT', 'Generative Pre‑trained Transformer', 'сто', 'тва GPT ...ha', 'ене'], ['Transformer', 'трансформер', 'Трансформер', 'Трансформера', 'его', 'Трансформер', 'Трансформера', 'Трансформера', 'его', 'он', 'трансформеры', 'Трансформера', 'ему', 'Трансформера', 'Трансформеров'], ['архитектуры', 'архитектуру', 'ктура, коли'], ['Google'], ['индустрии', 'Индустрия'], ['текстовых переводов', 'перевод текстов', 'это', 'переводу', 'перевода', 'ду с дру'], ['обработки', 'обработке'], ['изображений', 'картинки'], ['звука', 'звук'], ['видео'], ['«зимы ИИ»'], ['вычислительный механизм', 'который', 'он'], ['набор последовательностей', 'набор последовательностей'], ['последовательностей', 'последовательностей', 'последовательностей', 'последовательностей'], ['выходе'], ['алгоритму'], ['последовательностей чисел'], ['модулей‑блоков', 'которые'], ['до‑трансформерные языковые модели', 'их'], ['ресурсов'], ['нейросети‑трансформеры'], ['входные данные'], ['длиной'], ['страницу', 'странице', 'страниц', 'странице', 'ц убори'], ['параграфа'], ['результатам', 'результат', 'результаты', 'результатов'], ['строку'], ['нейросетевой обработке текстов'], ['связи'], ['объемах', 'объем', 'объемами', 'объем', 'объема', 'объем', 'объему', 'объем', 'объему', 'объем', ' трени'], ['масштабируемостью', 'масштабирования'], ['эффективностью'], ['будущее'], ['сложность', 'сложность модели', 'она'], ['распознавания изображений'], ['чихуабелей', 'пёсов', 'чихуабель'], ['маффинов с\\xa0черничкой', 'хлебобулочных изделий', 'булка'], ['фотографий', 'фоткой'], ['тренировочный набор данных', 'тренировочных данных', 'набор тренировочных данных', 'тренировочной выборки', 'тренировочных данных', 'тренировочных текстовых данных', 'Набор данных для\\xa0обучения', 'он', 'туда', 'массив текста для\\xa0ее обучения', 'тренировочных данных', 'овочных текстовых данных. И сн'], ['текстовые данные', 'эти самые данные', 'текстовых данных', 'текстовых данных', 'вых данных. И сн'], ['школьника', 'ему', 'он', 'себя'], ['информации', 'информации', 'информацию', 'которая', 'информацию'], ['ними'], ['технология Трансформеров', 'она'], ['«массивными» моделями', 'модели «Plus Size»', '«большой» модели'], ['датасет'], ['размер модели', 'размера модели', 'размера модели', 'размеру самой модели', 'размер', 'размера модели', 'ов языко'], ['наборов текстовых данных', ' текстов из\\xa0Ин'], ['специалистов по\\xa0ИИ', 'их'], ['ребята из\\xa0OpenAI', 'они', 'исследователи из\\xa0OpenAI', 'ребята из\\xa0OpenAI', 'а OpenAI пока '], ['Reddit', 'Reddit'], ['гиперссылки', 'ссылок', 'них', '\\xa0ссылкам'], ['сообщений'], ['гигабайт', 'гигабайт', 'Гб'], ['книги', 'томов', 'которые', 'книг'], ['памяти компьютера'], ['мегабайт'], ['минуту', 'минуту', 'минуты', 'минуту', 'минуты', 'минуту', 'минуты'], ['часа'], ['сутки'], ['лет'], ['эрудиции'], ['пятилетнего ребенка', 'он'], ['формуле'], ['полтора миллиарда', \"1'500'000'000\", 'такое количество', 'полтора миллиарда'], ['чисел'], ['файл', 'он'], ['размер'], ['текстового массива данных', 'котором', 'который'], ['модель', 'модели', 'ей', 'модели', 'модели', 'модели', 'модель', 'ей', 'модели', 'модели', 'модели', 'нее'], ['паттерны'], ['правила'], ['банк'], ['эссе', 'этой работе', 'она', 'Эссе', 'эссе', 'эссе'], ['ответом', 'ответа', 'ответов'], ['«Какие фундаментальные экономические и политические изменения необходимы для\\xa0эффективного реагирования на\\xa0изменение климата?»', 'темы'], ['жюри соответствующего конкурса', 'те', 'жюри'], ['новые свойства', 'новоприобретенные скиллы', 'новым навыкам', 'новые способности', 'которые', 'новые скиллы', 'новых фишек', 'которые'], ['наборы задач', 'которые'], ['разрешение двусмысленности'], ['понимание', 'пониманию'], ['Рыба', 'Рыба', 'Она', 'рыбе', 'рыба', 'рыба', 'ее', 'ее', 'рыбов', 'рыбов'], ['Рыба заглотила приманку. Она\\xa0была вкусной.', 'первом примере'], ['приманку', 'Она', 'приманку', 'приманке', 'приманка'], ['Рыба заглотила приманку. Она\\xa0была голодной.', 'втором случае'], ['местоимение «она»', '«она»'], ['картину мира', 'картины мира'], ['этот мем', 'его'], ['такие задачи', 'таких задачек', '«рыбных» задач', 'задачу про «голодных рыбов»', 'которой'], ['ранние языковые модели', 'старыми моделями', 'их', '«специализированных» предшественников', 'Старые модели', 'их'], ['ответами', 'ответа', 'ответом', 'ответу', 'Ответ', 'ответ', 'он', 'Ответ', 'ответ', 'он', 'ответом', '; но\\xa0о', ' должн', 'может', 'Вокруг', 'в нейро', 'отв', 'таким', 'уч', ' таким', 'прямо'], ['переход количества в\\xa0качество', 'который', 'он'], ['точности решения'], ['совершенно незнакомых ей задач', 'которые', 'их'], ['кучу книг'], ['Википедию'], ['интернет‑сайтов'], ['Анатолиев Вассерманов', 'они', 'Онотолея', 'ему'], ['«нейромозг»'], ['обобщение'], ['экстраполяцию'], ['конкретную задачу', 'проблему', '«узким» задачам'], ['многие специализированные модели', 'которые', 'специально заточенным под\\xa0это нейросетям'], ['французского', 'французский'], ['немецкого'], ['английский', 'английского'], ['математике', 'математику', 'математике', 'тике, базо'], ['графике ниже', 'графике выше'], ['точность', 'точностью', 'ть/полез'], ['сложением'], ['вычитанием'], ['умножением'], ['чисел', 'числом'], ['миллиардами', 'миллиардам', 'миллиардах', 'рдов долла'], ['качество модели'], ['«378\\xa0+ 789\\xa0=»', 'это'], ['Магия', 'магия', 'магия'], ['задачи', 'задача', 'проблему'], ['текст с\\xa0запросом', 'нему'], ['запросом', 'запросы', 'ы именн', ', подр', 'а (не з', 'у точны', ' «окей', 'а. Ну, ', ' польз'], ['свое продолжение', 'это продолжение'], ['задачу', 'задачки', 'задачи', 'у\\xa0— зна'], ['примеров', 'Пример', 'примера', 'примеров'], ['корректный перевод'], ['решения', 'их', 'решение'], ['исследователи', 'ователи сами ', 'диви'], ['улучшения результатов'], ['всего одной фразы', 'магическая фраза', '«let»s think step by step»', 'это', 'этой фразы', 'пять дополнительных слов', ' подумаем шаг за\\xa0шагом» из\\xa0пр'], ['вопрос', 'Вопрос', 'Вопрос', 'вопрос'], ['качество модели'], ['боксер Иван', 'боксера Ивана', 'он', 'боксер Иван', 'он', 'Иван', 'Иван', 'Иван'], ['ударов', 'ударов', 'ударов', 'ударов', 'ударов', 'ударов', 'ударов', 'ударов'], ['Бой', 'Бой'], ['раундов', 'раундов', 'раундов'], ['«режим рассуждения»'], ['составление грамотных промптов для\\xa0модели', 'эту задачу'], ['«промпт‑инженер»'], ['мпт‑инжинирингу за'], ['му программированию, поша'], ['вым рассуждениям, и мн'], ['ение размеров языковых моделей само ', 'о\\xa0себе еще н', 'ьз'], ['х условий\\xa0— кот', 'е в\\xa0ком', 'обой разум'], ['роси', ' мужа:', 'де'], ['«Вас', 'сход', 'то п'], ['ер', 'Нет, '], ['кото', 'е людям'], ['кции из\\xa0за', 'ак', 'кцию на\\xa0за'], ['ета\\xa0— а\\xa0в', 'ете, как\\xa0', 'ете».Полу'], ['дателям предъ'], ['ователи думал', 'ователи не пр'], ['ва модели «точн'], ['ость» и «б'], ['дность/неток', 'дность модел'], ['ичность» весь'], [' модель должн'], [' по поиску некоего баланса, в ко', 'й нет о'], ['а, в ко'], ['нивания ИИ» (AI a', 'gnment – Ope', 'пиш'], ['х этических вопросов, и ра', ' с'], ['ых спорных ситуаций – огр', 'ст'], ['а сформ'], [' к те', 'опро'], ['ой связи. В ка', 'а на ма', 'ой связи от др', 'ой связи от жи'], ['ческие детеныши ведь '], [': дела'], ['изацию оценк'], [' живог', ' от че'], ['юдей сидел', 'го жюри» заним'], ['ительную задачу – «ка', 'не поменять свой сгенерированный ответ таким образом, чтобы он получил наибольшую оценку от человека?» (подр', 'ия по обратной связи разби', 'ение на основе обратной связи от живых людей. Полу'], [' процесса обучения модели, этот', 'инальный этап «дообучения на живых людях» заним', 'инальный штрих и ста'], ['ным соусом, кото', 'й сдела'], ['ние модели из серии GPT насто'], ['димыми знаниями: пони', 'знаниями именн'], [' языки, помн'], ['ческие события, знал'], ['я стилей разных авторов, и та'], ['м, кото', 'й мы (л'], ['.При\\xa0э'], ['овом наборе данных. Ведь'], ['кото', 'е специ'], ['систента» в\\xa0фор'], ['а: напр', 'а ИИ мо', 'и с\\xa0ней', 'ел'], ['вателя неясе', 'вателя сдела', ' он до', 'вателей\\xa0— кон'], ['ющий вопрос, и та'], ['ые технические характеристики (архи'], ['ьтман (испо'], ['ейс! К\\xa0то', 'ейс\\xa0— то ', 'ейс «диал', 'ейс. И Ch', 'ейс взаим'], ['терфейс\\xa0— то '], ['гового окна», прям', 'ового окна, в\\xa0ко', 'м друже'], ['джерах. Да\\xa0е'], ['ный доступ вообщ', 'ый публичный доступ\\xa0— что'], ['ях. Choo'], ['ение квантовой физики в форме рэп-телеги от Снуп Дога – то,', 'аже'], ['ехнология\\xa0— но\\xa0', 'ла ', 'огию в\\xa0мас'], ['а, в\\xa0ко', 'ю она\\xa0б'], ['т\\xa0— но\\xa0', 'та. Но\\xa0э'], ['дут', '\\xa0пр'], ['ти привлечения новых пользователей: отме'], ['н юзеро', 'нов перев', 'нов польз'], [', а\\xa0за'], ['. Micr'], ['oft опера'], ['ированию в\\xa0них'], ['ов, инже'], ['ры Google забил', 'ст'], [' забил', 'вый сервис от\\xa0ко'], ['енции с\\xa0ней'], ['ы в\\xa0сро', ' собственного чат‑б'], [' история\\xa0— сле', 'й вы мо'], ['в этой статьи) есть', 'в: Сиол'], ['д на\\xa0Ха', 'PT как\\xa0инструмент для\\xa0поиска: решаем основную проблему», в\\xa0ко', 'м нюанс'], ['под\\xa0н'], ['мент для\\xa0п'], [': реша'], ['й постер, кото', 'й нагля'], ['и эволю'], ['ии языко'], ['то этап кажет'], ['тствующему разделу в\\xa0тек', 'нов'], ['тариях).На\\xa0с', 'тариев!UPD: '], ['мы контроля за\\xa0искусственным интеллектом, и жа'], ['ть корпораций, кото'], ['аций, кото', 'е в\\xa0выс'], ['ью могут'], ['е философские вопросы вроде'], ['в, то в'], ['щем материале. Если', 'опу'], ['можно', ' Чему', 'нейросеть, и по', 'ней языковой модели из се'], [' Чему научилась новая нейросеть, и почему это немного жутковатоВ этой', 'татье мы ра'], ['тивные моделиязыков'], ['l language processing  Hubs', 'l Language Processing      ']]"}]}
{"messages": [{"role": "system", "content": "Ты являешься экспертом в задаче решения референций из текстов на русском языке."}, {"role": "user", "content": "**Ты решаешь задачу разрешения кореференций (coreference resolution) в текстах на русском языке с помощью пошагового рассуждения (Chain-of-Thought).**\n\nСледуй указанным шагам:\n\n---\n\n### \uD83D\uDD39 Шаг 1 (Извлечение упоминаний):\n\n- Прочитай текст.\n- Выпиши все упоминания сущностей (людей, компаний, продуктов, сервисов и т.д.), а также местоимения и фразы, которые могут на них ссылаться.\n- Учитывай не только имена собственные, но и абстрактные понятия (например, \"база знаний\", \"информация\", \"уязвимости\"), если на них ссылаются в тексте.\n- Постарайся различать случаи, когда одно и то же слово может обозначать разные вещи в зависимости от контекста (например, \"ИИ\" может означать как технологию, так и конкретного помощника — Atlassian Intelligence).\n\n---\n\n### \uD83D\uDD39 Шаг 2 (Определение кореференций и группировка):\n\n- Определи, какие из упоминаний относятся к одной и той же сущности.\n- Учитывай грамматические признаки (род, число, падеж), контекст и смысл текста.\n- Если возникают сомнения, выбери наиболее вероятную связь. В сложных случаях можешь кратко пояснить причину (1–2 слова).\n- Сгруппируй упоминания по кластерам — каждому кластеру соответствует одна сущность.\n\n---\n\n### \uD83D\uDD39 Шаг 3 (Итоговый результат):\n\n- Верни только результат в виде массива массивов.\n- Каждый внутренний массив — отдельный кластер кореференций.\n- Не добавляй комментариев, пояснений или шагов рассуждения к итоговому выводу.\n\n---\n\n### \uD83D\uDCCC Пример:\n\n**Текст:**\n\n«Мария живет в Москве. Она любит этот город за его атмосферу.»\n\n**Ответ:**\n\n```json\n[\n  [\"Мария\", \"Она\"],\n  [\"Москве\", \"этот город\", \"его\"]\n]\n```\n\n---\n\nТеперь выполни аналогичный анализ для следующего текста, соблюдая описанные шаги рассуждения:\n\n`\"\n\nВышел Atlassian Intelligence — ИИ-помощник для Jira и Confluence / Habr\n\n\n               Вышел Atlassian Intelligence — ИИ-помощник для Jira и Confluence  Reading time  \n    2 min\n   Views  2.3K Atlassian *Cloud services *Artificial Intelligence       Компания Atlassian представила «виртуального товарища по команде» Atlassian Intelligence. Он работает на базе как собственной модели компании, так и модели от OpenAI.Atlassian Intelligence способен обобщать, искать и создавать контент. Он работает в связке с Confluence — рабочим пространством для создания единой базы знаний для компании. Помощник может обращаться к этой базе, чтобы найти необходимую информацию и, например, подвести итоги встречи, сделать расшифровку разговора, а затем выделить список решений и действий, которые нужно предпринять.Также Atlassian Intelligence может искать необходимые термины в базе данных компании и давать им определения, а также показывать, с какими проектами он связан и какие команды над ним работают. Наконец, помощнику можно задавать вопросы, связанные с работой, например: «Как настроить среду разработки?». ИИ ответит с учётом информации, которая есть в базе. Atlassian Intelligence понимает запросы на естественном языке и может преобразовывать их в JQL или SQL. Он работает с Jira Service Management и позволяет сотруднику получить доступ к необходимым инструментам, а поддержке — найти необходимый ответ или обобщить предыдущие взаимодействия и составить список того, что нужно сделать.Atlassian Intelligence будет работать только в облачной платформе компании. В будущем его включат в тарифы Premium и Enterprise для Jira, а пока можно записаться в лист ожидания. Все функции помощника будут бесплатными. В марте GitHub представил инструмент Copilot для документов, который работает как ChatGPT и позволяет искать информацию об официальной документации библиотек, продуктов и ресурсов. Он охватывает ресурсы GitHub, React, MDN, Azure, TypeScript и Webpack.  В апреле Amazon сделала свой ИИ-помощник по кодированию CodeWhisperer бесплатным для всех разработчиков. Изначально он был доступен только для клиентов Amazon Web Services. CodeWhisperer автоматически отфильтровывает любые предложения кода, которые потенциально предвзяты, и помечает его. Он поставляется с функциями сканирования безопасности, которые могут выявлять уязвимости в коде разработчика, а также предлагать варианты их исправления.      Tags: atlassianatlassian jirajiraconfluenceискусственный интеллектчат-ботпомощникразработкасовместная работаоблачные сервисы  Hubs: AtlassianCloud servicesArtificial Intelligence"}, {"role": "assistant", "content": "[['Atlassian', 'Atlassian', 'Компания Atlassian', 'компании', 'компании', 'компании', 'компании'], ['Atlassian Intelligence', 'Atlassian Intelligence', '«виртуального товарища по команде» Atlassian Intelligence', 'Он', 'Atlassian Intelligence', 'Он', 'Помощник', 'Atlassian Intelligence', 'он', 'ним', 'помощнику', 'ИИ', 'Atlassian Intelligence', 'Он', 'Atlassian Intelligence', 'его', 'помощника'], ['ИИ', 'ИИ', 'Artificial Intelligence', 'ИИ'], ['ИИ-помощник', 'ИИ-помощник', 'ИИ-помощник'], ['Jira', 'Jira', 'Jira Service Management', 'Jira'], ['Confluence', 'Confluence', 'Confluence'], ['Cloud services', 'облачной платформе'], ['модели', 'модели'], ['модели от OpenAI'], ['OpenAI'], ['единой базы знаний', 'этой базе', 'базе данных компании', 'базе'], ['решений и действий', 'которые'], ['термины', 'им'], ['информации', 'которая'], ['JQL'], ['SQL'], ['GitHub', 'GitHub'], ['инструмент Copilot', 'который', 'Он'], ['ChatGPT'], ['React'], ['MDN'], ['Azure'], ['TypeScript'], ['Webpack'], ['Amazon', 'Amazon'], ['CodeWhisperer', 'он', 'CodeWhisperer', 'Он'], ['Amazon Web Services'], ['предложения кода', 'которые'], ['кода', 'его'], ['функциями сканирования безопасности', 'которые'], ['уязвимости', 'их']]"}]}
{"messages": [{"role": "system", "content": "Ты являешься экспертом в задаче решения референций из текстов на русском языке."}, {"role": "user", "content": "**Ты решаешь задачу разрешения кореференций (coreference resolution) в текстах на русском языке с помощью пошагового рассуждения (Chain-of-Thought).**\n\nСледуй указанным шагам:\n\n---\n\n### \uD83D\uDD39 Шаг 1 (Извлечение упоминаний):\n\n- Прочитай текст.\n- Выпиши все упоминания сущностей (людей, компаний, продуктов, сервисов и т.д.), а также местоимения и фразы, которые могут на них ссылаться.\n- Учитывай не только имена собственные, но и абстрактные понятия (например, \"база знаний\", \"информация\", \"уязвимости\"), если на них ссылаются в тексте.\n- Постарайся различать случаи, когда одно и то же слово может обозначать разные вещи в зависимости от контекста (например, \"ИИ\" может означать как технологию, так и конкретного помощника — Atlassian Intelligence).\n\n---\n\n### \uD83D\uDD39 Шаг 2 (Определение кореференций и группировка):\n\n- Определи, какие из упоминаний относятся к одной и той же сущности.\n- Учитывай грамматические признаки (род, число, падеж), контекст и смысл текста.\n- Если возникают сомнения, выбери наиболее вероятную связь. В сложных случаях можешь кратко пояснить причину (1–2 слова).\n- Сгруппируй упоминания по кластерам — каждому кластеру соответствует одна сущность.\n\n---\n\n### \uD83D\uDD39 Шаг 3 (Итоговый результат):\n\n- Верни только результат в виде массива массивов.\n- Каждый внутренний массив — отдельный кластер кореференций.\n- Не добавляй комментариев, пояснений или шагов рассуждения к итоговому выводу.\n\n---\n\n### \uD83D\uDCCC Пример:\n\n**Текст:**\n\n«Мария живет в Москве. Она любит этот город за его атмосферу.»\n\n**Ответ:**\n\n```json\n[\n  [\"Мария\", \"Она\"],\n  [\"Москве\", \"этот город\", \"его\"]\n]\n```\n\n---\n\nТеперь выполни аналогичный анализ для следующего текста, соблюдая описанные шаги рассуждения:\n\n`\"\n\nКак распознать текст, написанный нейросетью, и как это использовать в B2B-бизнесе / Habr\n\n\n               Как распознать текст, написанный нейросетью, и как это использовать в B2B-бизнесе Level of difficulty  \n    Medium\n   Reading time  \n    8 min\n   Views  8.7K Altcraft corporate blog Content-marketing *Artificial Intelligence       Добро пожаловать в будущее, где всё больше контента генерируется искусственным интеллектом. От новостей до постов в соцсетях — теперь труднее понимать, что написал человек, а что — нейросеть.С одной стороны, контент, созданный ИИ, экономит наше время и силы, но с другой — иногда он выдаёт однотипный механический текст, от которого реально заснуть. Учитывая дипфейки и фейковые новости в сети, сейчас как никогда важно уметь отличить контент, вышедший от нейросетей. Зайдите на сайт OpenAI — компании, которая разрабатывает языковую нейросеть ChatGPT. Эта компания недавно выпустила программу AI text classifier, которая определяет контент, написанный ИИ. По обещаниям, она упростит задачу бизнеса: позволит удостовериться, что контент пишут люди, а не машины. Но как именно это работает? Зачем это нужно в стратегиях B2B-маркетинга и можно ли полагаться на этот инструмент?Как работает инструмент AI Text Classifier от компании OpenAI?  AI Text Classifier запустили 31 января 2023 года. Как и остальные инструменты OpenAI, он бесплатный. Работает так: анализирует образцы текста, сравнивает их с образцами баз данных, написанными как нейросетью, так и людьми, анализирует множество разных стилей написания, даже такие, как новостные статьи, научные документы, посты в соцсетях, рекламные тексты.Хоть и нельзя со стопроцентной уверенностью сказать, что тот или иной текст написан нейросетью, потенциал у AI Text Classifier есть: через него можно обеспечивать высокое качество текстов в сети. В компании OpenAI утверждают, что новый инструмент пресечёт такие попытки обмана, как запуск автоматических кампаний по дезинформации, мошенничество с задействованием ИИ в научной сфере, имитацию переписки с живым человеком с помощью чат-ботов. Чтобы знать, как работает программа, нужно понимать особенности текстов, которые генерирует ИИ. Такой текст чаще всего несогласованный и несвязный: модели ИИ в большинстве случаев не способны создать текст, основные мысли которого логично и естественно перетекают из одной в другую. К тому же, в сгенерированном ИИ тексте встречается много ошибок и несуразиц, которых почти не бывает в написанном человеком материале. И наконец, ИИ зачастую используют странные схемы построения и однотипные предложения — они обучаются на огромном количестве текстов, которые не отражают тонкостей человеческого языка во всей его полноте. Новый инструмент находит все особенности, которыми грешат сгенерированные ИИ тексты. Он разработан для идентификации этих и других характеристик, типичных для текстов, созданных не человеком. AI Text Classifier использует как алгоритмы машинного обучения, так и алгоритмы функционирования человеческого языка, и таким образом анализирует различные языковые аспекты: грамматику, синтаксис, лексику, интонацию. В результате тексту даётся оценка, насколько вероятно, что он создан нейросетью. Люди, организации или платформы могут опираться на эту оценку, чтобы считать текст заслуживающим доверия или созданным человеком либо машиной со злонамеренной целью.Но даже сами разработчики OpenAI признают, что их оценивание не на сто процентов надежное. На веб-сайте говорится: «В нашей тестовой оценке английских текстов классификатор правильно определяет 26% действительно написанных нейросетью текстов как «вероятно сгенерированные ИИ». В то же время в 9% случаев они относят написанные человеком тексты к «вероятно сгенерированным ИИ». Немного редактуры — и этот инструмент можно обмануть.Так что нельзя использовать оценку AI Text Classifier как единственное доказательство того, что текст создал ИИ. Есть другие особенности, по которым можно определить, кто оказался автором текста — человек или машина. Об этих особенностях и пойдёт речь ниже.  Как определить контент, созданный нейросетью  Ищите языковые несоответствия и некорректный язык написания  ИИ не способен отобразить естественный поток человеческой речи во всём её многообразии. В процессе генерирования текста алгоритмы ИИ опираются на предустановленные шаблоны и схемы — поэтому текст выходит сухим и однообразным. Особенно это хорошо видно в длинных текстах, где то самое однообразие сразу становится заметным.   Не только тон, в котором написан текст, но и сам язык написания вам подскажет, человек ли писал тот или иной контент. Например, ИИ может использовать грамматические структуры, которые кажутся странными или даже прямо говорят о том, что человек так не пишет. Если вы будете обращать на это внимание, вы самостоятельно отличите контент нейросети от человеческого.Проверьте, нет ли повторяющихся конструкций и шаблонов  Верный признак того, что автором контента стал ИИ — повторяющиеся шаблоны и конструкции. Получившийся текст не блещет оригинальностью и живостью. Если вы замечаете, что читаете однообразные шаблонные фразы, а конструкции от предложения к предложению повторяются, значит данный текст сгенерирован машиной.Как проверить текст на наличие повторяющихся шаблонов? Ищите одинаковое построение в стиле или структуре текста. Например, каждое предложение может следовать одной и той же схеме построения, или в тексте раз за разом повторяются одни и те же фразы.Ещё один способ распознать шаблонные конструкции — использовать  антиплагиат-программы. Большинство алгоритмов ИИ строят свой контент на уже существующих текстах, беря их за основу. Пропустите текст через проверку на плагиат, и вы увидите, взят ли он полностью из интернета или переписан близко к источнику.Тем не менее, надо понимать, что текст с повторяющимися конструкциями — не равно сгенерированный нейросетью текст. Такой контент, как технические инструкции или юридические документы, создаётся по вполне предсказуемым шаблонам просто потому, что требуется придерживаться определённых общепринятых стандартов и делового стиля написания.Пример ниже наглядно демонстрирует, что ChatGPT постоянно использует повторяющиеся слова и конструкции. Посмотрите на контекст того, о чём текстОбращайте внимание на тему контента, аудиторию, на которую рассчитан текст, на то, насколько глубоко автор должен разбираться в теме. Тексты от нейросети часто используются там, где тема проста и недвусмысленна, и не нужно обладать каким-то особым опытом и знаниями. Например, нейросети часто создают описания продукции, сводки новостей или посты в соцсетях. Чтобы написать такой контент, не требуется глубоко вникать в тему, поэтому ИИ справляется неплохо.С другой стороны, если вы читаете текст, в котором ведётся рассуждение на спорные, сложные темы, или в тексте много технической лексики, вероятнее всего, его писала не машина, или, по крайней мере, его отредактировал человек. Ведь алгоритмы ИИ почти не улавливают нюансы этих тем, поэтому не создают информативный и грамотный контент такого рода. Помимо темы, обратите внимание на целевую аудиторию. Если текст предназначен для очень узкого круга людей, например, для медиков или юристов, вероятно, над ним трудились не машины. Алгоритмы ИИ не могут быть экспертами, чтобы проработать тему так глубоко, как это нужно профессионалам в конкретной узкой сфере.Проанализируйте источники и цитаты  Когда алгоритмы ИИ генерируют текст, они подтягивают информацию из уже существующих источников — а они могут быть устаревшими, ненадёжными или даже фейковыми. Многие платформы ИИ, например, ChatGPT или perplexity.ai, предоставляют, откуда они берут материалы — тогда человек может перепроверить машину и, при необходимости, сослаться на источники. Чаще всего в генерируемых статьях ИИ вообще не ссылается на источники, а когда позже человек их находит, оказывается, что либо они уже устарели, либо сведения по этим ссылкам сильно искажены и представлены однобоко.  Изучите первоисточники. Так вы определите, сгенерированный ли перед вами текст, и являются ли сведения в контенте достоверными, релевантными, либо же предвзятыми и перевранными. Ищите такие ресурсы, которые считаются общепринятыми и экспертными в той или иной сфере. Проверьте, насколько актуальны сведения и хорошо ли подкрепляют утверждения, приведённые в тексте.   Также стоит обратить внимание на достоверность автора или организации, текст которых взят в работу. Поищите сведения и проверьте, насколько они заслуживают доверия. Цитирует ли статья личное мнение из чьего-нибудь блога или исследование из серьёзной научной организации?Потратив время на проверку используемых источников и ссылок, вы можете определить, был ли текст написан ИИ, а главное, вы можете убедиться, что информация в тексте надёжная и правильная. Это особенно важно, когда речь идёт о таких сферах, которые сильно влияют на жизнь людей: о здавоохранении или финансовой сфере.  Как правильно использовать ИИ в B2B-маркетинге  Нейросети вывели B2B-контент-маркетинг на другой уровень: теперь благодаря им компании могут персонализировать опыт клиентов, эффективнее анализировать данные и автоматизировать процессы. И всё же ИИ нужно использовать так, чтобы покупатели вам доверяли. Пользователи всё лучше и лучше распознают контент, сгенерированный нейросетью, поэтому важно, чтобы ваши тексты были честными, понятными и полезными.Как надо делатьБудьте честными. Чётко указывайте, где используется ИИ и что именно он делает. Ни в коем случае не используйте нейросеть для генерации фейковых отзывов или рекомендаций, или для того, чтобы вводить покупателей в заблуждение.Например, в январе 2023 года компания Apple рассказала о сервисе Digital narration, в котором доступны озвучки книг от искусственного интеллекта. Говорят, что начитка очень хорошая и неотличима от человека.Анализируйте данные. Проанализируйте большие объемы данных и получите информацию, чтобы разработать маркетинговую стратегию. Но обязательно убедитесь, что вы правильно используете результаты и делаете корректные выводы из предоставленной информации.Персонализируйте контент. ИИ поможет персонализировать контент для потребителей, только убедитесь, что вы действительно даёте правильный и значимый контент.  Автоматизируйте рутинную работу. ИИ упростит вам рутинные задачи: отправку электронных писем для B2B-email-маркетинга, автопостинг в соцсетях, генерацию лидов. Тогда маркетологи сфокусируются на стратегических вещах.  Проверяйте тексты, сгенерированные нейросетями. Убедитесь, что их контент грамотный, последовательный и подходит целевой аудитории, для которой он написан.Оптимизируйте производство контента. Нейросети — отличный инструмент для генерации идей, особенно в контент-маркетинге. Ищите свежие идеи через ИИ, а затем с помощью собственных навыков и эмоционального опыта делайте ваши тексты увлекательными, уникальными и живыми.   Как не надо делать  Не создавайте фейковых пользователей через ИИ и не подтасовывайте данные покупателей. Не генерируйте спам и низкосортные тексты. Разумеется, последовательные действия — один из секретов успеха в маркетинге, но в конечном счёте вы не выиграете, если ваш контент не полезен для покупателей или не соответствует интересам вашей целевой аудитории.Не полагайтесь, что ИИ полностью создаст тексты за вас. Чтобы написать убедительный и качественный текст, всё ещё нужен человеческий интеллект и креативность.  Не используйте ИИ для контента в критические или особо деликатные моменты. В случаях, когда речь идёт о человеческих чувствах, убедитесь, что процесс обмена сообщениями контролирует человек, а не машина.Принимайте во внимание этическую сторону вопроса. В некоторых случаях ИИ может обострять предубеждения и дискриминацию. Убедитесь, что ИИ, который вы используете, честный и непредосудительный.Не забывайте, что ваш бизнес должен быть «с человеческим лицом». Хоть ИИ и является удачным решением для персонализации контента, ничто не заменит настоящего человеческого взаимодействия. Используйте ИИ, чтобы предлагать пользователю именно то, что ему нужно, но вместе с тем предоставьте ему возможность пообщаться и с живым сотрудником тогда, когда это необходимо.Заключение  Сейчас как никогда важно уметь определять тексты, созданные нейросетями. Благодаря этому можно принимать обоснованные решения и не брать во внимание ложную и сомнительную информацию. И неважно, о каких текстах идёт речь — о новостях, статьях, обзорах продуктов или обучающем контенте, в наше время критически важно уметь отличить тексты нейросетей от человеческих.   В статье разобрали главные правила, как отличить контент нейросети от контента человека. Применяйте полученные знания и не дайте себя обмануть.       Tags: нейросетиконтент-маркетингнейросети. самообучающиеся системы.  Hubs: Altcraft corporate blogContent-marketingArtificial Intelligence"}, {"role": "assistant", "content": "[['текст', 'текст', 'текст', 'текста', 'тексты', 'текст', 'текстов', 'текстов', 'которые', 'Такой текст', 'текст', 'которого', 'тексте', 'материале', 'текстов', 'которые', 'тексты', 'текстов', 'тексту', 'он', 'текст', 'текстов', 'текстов', 'тексты', 'текст', 'текста', 'текста', 'текст', 'текстах', 'текст', 'текст', 'текст', 'текст', 'текста', 'тексте', 'текстах', 'их', 'текст', 'он', 'текст', 'текст', 'текст', 'текст', 'Тексты', 'текст', 'тексте', 'его', 'его', 'текст', 'текст', 'текст', 'тексте', 'текст', 'текст', 'тексте', 'тексты', 'их', 'тексты', 'тексты', 'текст', 'тексты', 'текстах', 'тексты'], ['нейросетью', 'нейросетью', 'нейросеть', 'нейросетей', 'нейросеть', 'нейросетью', 'нейросетью', 'нейросетью', 'нейросетью', 'нейросетью', 'нейросети', 'нейросетью', 'нейросети', 'нейросети', 'Нейросети', 'им', 'нейросетью', 'нейросеть', 'нейросетями', 'Нейросети', 'нейросетями', 'нейросетей', 'нейросети'], ['B2B-бизнесе', 'B2B-бизнесе'], ['бизнесе', 'бизнесе', 'бизнеса', 'бизнес'], ['B2B', 'B2B', 'B2B', 'B2B', 'B2B'], ['Artificial Intelligence', 'искусственным интеллектом', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'они', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'свой', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'он', 'искусственного интеллекта', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ'], ['контента', 'контент', 'контент', 'контент', 'контент', 'контент', 'контент', 'контент', 'контента', 'контент', 'контент', 'контента', 'контент', 'контент', 'контенте', 'контент', 'контент', 'контент', 'контент', 'контент', 'контент', 'контента', 'контент', 'контент', 'контента', 'контента', 'контенте', 'контент', 'контента'], ['новостей', 'новости', 'новостные статьи', 'сводки новостей', 'новостях'], ['постов', 'посты', 'посты'], ['соцсетях', 'соцсетях', 'соцсетях', 'соцсетях'], ['человек', 'люди', 'людьми', 'живым человеком', 'человеком', 'человеком', 'Люди', 'человеком', 'человеком', 'человек', 'человек', 'человек', 'человек', 'людей', 'человек', 'человек', 'людей', 'человека', 'человек', 'человека'], ['контент, созданный ИИ', 'он'], ['дипфейки'], ['сайт OpenA', 'веб-сайте'], ['OpenAI', 'Эта компания', 'компании OpenAI', 'OpenAI', 'компании OpenAI', 'OpenAI'], ['компании', 'которая', 'компании'], ['ChatGPT', 'ChatGPT', 'ChatGPT'], ['программу AI text classifier', 'которая', 'она', 'этот инструмент', 'инструмент AI Text Classifier', 'AI Text Classifier', 'он', 'AI Text Classifier', 'него', 'новый инструмент', 'программа', 'Новый инструмент', 'Он', 'AI Text Classifier', 'этот инструмент', 'AI Text Classifier'], ['машины', 'машиной', 'машина', 'машиной', 'машина', 'машины', 'машину', 'машина'], ['B2B-маркетинга', 'B2B-маркетинге'], ['маркетинга', 'маркетинге', 'маркетинг', 'маркетинга', 'маркетинге', 'маркетинге'], ['инструмент', 'инструмент', 'инструменты', 'инструмент', 'инструмент', 'инструмент'], ['образцы', 'образцами'], ['образцы текста', 'их'], ['баз данных'], ['данных', 'данные', 'данные', 'данных', 'данные'], ['стилей', 'стиле', 'стиля'], ['стилей написания', 'стиля написания'], ['статьи', 'статьях', 'статья', 'статьях', 'статье'], ['сфере', 'сфере', 'сфере', 'сферах', 'которые', 'сфере'], ['чат-ботов'], ['модели ИИ'], ['ошибок и несуразиц', 'которых'], ['схемы', 'схемы', 'схеме'], ['схемы построения', 'схеме построения'], ['предложения', 'предложения', 'предложению', 'предложение'], ['человеческого языка', 'его', 'человеческого языка'], ['языка', 'языка', 'язык', 'язык'], ['особенности', 'которыми'], ['характеристик'], ['алгоритмы', 'алгоритмы', 'алгоритмы', 'алгоритмов', 'алгоритмы', 'Алгоритмы', 'алгоритмы'], ['алгоритмы машинного обучения'], ['машинного обучения'], ['грамматику'], ['синтаксис'], ['лексику'], ['интонацию'], ['организации', 'организации', 'организации'], ['платформы', 'платформы'], ['разработчики'], ['разработчики OpenAI', 'их', 'нашей', 'они'], ['классификатор'], ['особенности', 'которым', 'особенностях'], ['автором', 'автором', 'автор', 'автора'], ['человеческой речи', 'её'], ['алгоритмы ИИ', 'алгоритмы ИИ', 'Алгоритмы ИИ', 'алгоритмы ИИ', 'они'], ['шаблоны', 'шаблонов', 'шаблоны', 'шаблонов', 'шаблонам'], ['тон', 'котором'], ['вам', 'вы', 'вы', 'вы', 'вы', 'вы', 'вы', 'вами', 'вы', 'вы', 'вам', 'ваши', 'вы', 'вы', 'вам', 'собственных', 'ваши', 'вы', 'ваш', 'вашей', 'вас', 'ваш', 'себя'], ['грамматические структуры', 'которые'], ['конструкций', 'конструкции', 'конструкции', 'конструкции', 'конструкциями', 'конструкции'], ['фразы', 'фразы'], ['антиплагиат-программы'], ['плагиат'], ['контекст'], ['тему', 'теме', 'тема', 'тему', 'темы', 'тем', 'темы', 'тему'], ['аудиторию', 'аудиторию', 'аудитории', 'аудитории'], ['источники', 'источников', 'они', 'источники', 'источники', 'их', 'они', 'этим ссылкам', 'первоисточники', 'источников'], ['информацию', 'информация', 'информацию,', 'информации', 'информацию'], ['платформы ИИ', 'они'], ['perplexity.ai'], ['сведения', 'сведения', 'сведения', 'сведения', 'они'], ['ресурсы', 'которые'], ['B2B-контент-маркетинг'], ['покупатели', 'покупателей', 'покупателей.', 'покупателей'], ['Пользователи', 'пользователей', 'пользователю', 'ему', 'ему'], ['Apple'], ['сервисе'], ['Digital narration'], ['потребителей,'], ['B2B-email-маркетинга'], ['автопостинг'], ['лидов'], ['маркетологи'], ['контент-маркетинге'], ['спам'], ['человеческий интеллект'], ['дискриминацию'], ['решением', 'решения']]"}]}
{"messages": [{"role": "system", "content": "Ты являешься экспертом в задаче решения референций из текстов на русском языке."}, {"role": "user", "content": "**Ты решаешь задачу разрешения кореференций (coreference resolution) в текстах на русском языке с помощью пошагового рассуждения (Chain-of-Thought).**\n\nСледуй указанным шагам:\n\n---\n\n### \uD83D\uDD39 Шаг 1 (Извлечение упоминаний):\n\n- Прочитай текст.\n- Выпиши все упоминания сущностей (людей, компаний, продуктов, сервисов и т.д.), а также местоимения и фразы, которые могут на них ссылаться.\n- Учитывай не только имена собственные, но и абстрактные понятия (например, \"база знаний\", \"информация\", \"уязвимости\"), если на них ссылаются в тексте.\n- Постарайся различать случаи, когда одно и то же слово может обозначать разные вещи в зависимости от контекста (например, \"ИИ\" может означать как технологию, так и конкретного помощника — Atlassian Intelligence).\n\n---\n\n### \uD83D\uDD39 Шаг 2 (Определение кореференций и группировка):\n\n- Определи, какие из упоминаний относятся к одной и той же сущности.\n- Учитывай грамматические признаки (род, число, падеж), контекст и смысл текста.\n- Если возникают сомнения, выбери наиболее вероятную связь. В сложных случаях можешь кратко пояснить причину (1–2 слова).\n- Сгруппируй упоминания по кластерам — каждому кластеру соответствует одна сущность.\n\n---\n\n### \uD83D\uDD39 Шаг 3 (Итоговый результат):\n\n- Верни только результат в виде массива массивов.\n- Каждый внутренний массив — отдельный кластер кореференций.\n- Не добавляй комментариев, пояснений или шагов рассуждения к итоговому выводу.\n\n---\n\n### \uD83D\uDCCC Пример:\n\n**Текст:**\n\n«Мария живет в Москве. Она любит этот город за его атмосферу.»\n\n**Ответ:**\n\n```json\n[\n  [\"Мария\", \"Она\"],\n  [\"Москве\", \"этот город\", \"его\"]\n]\n```\n\n---\n\nТеперь выполни аналогичный анализ для следующего текста, соблюдая описанные шаги рассуждения:\n\n`\"\n\nSlovo и русский жестовый язык / Habr\n\n\n               Slovo и русский жестовый язык Level of difficulty  \n    Medium\n   Reading time  \n    10 min\n   Views  3K SberDevices corporate blog Data Mining *Image processing *Machine learning *Artificial Intelligence       Всем привет! В этой статье мы расскажем о непростой задаче распознавания русского жестового языка (РЖЯ) для слабослышащих. Насколько нам известно, в открытом доступе не существует универсального набора данных для распознавания РЖЯ. Поэтому мы решили выложить небольшую часть нашего датасета в открытый доступ. В статье мы затронем основные особенности РЖЯ, поговорим о проблемах и сложностях самого языка, и процессе его сбора и разметки. Расскажем, где искали экспертов и как нам в итоге удалось собрать самый большой и разнородный жестовый датасет для РЖЯ. В конце статьи представим набор предобученных нейронных сетей и небольшое приложение, демонстрирующее распознавание жестового языка. Часть датасета и веса моделей мы выложили в открытый доступ — все ссылки вы можете найти в конце статьи или в репозитории.ВведениеСуществует множество наборов данных для распознавания естественных языков, что значительно упрощает жизнь в тех или иных ситуациях. Суммаризация текстов, генерация новостей, синтез речи, распознавание голосовых сообщений, и другие — все это реализовано и используется в повседневной жизни. Однако те части общества, которые вынуждены взаимодействовать друг с другом с помощью жестовых языков, остались за пределами подобных новшеств. Количество людей с нарушениями слуха и речи в России оценивается от 500 тысяч до нескольких миллионов и они могут быть непонятыми и в повседневной жизни, и в экстремальных ситуациях. Всероссийское общество глухих (ВОГ) заявляет, что в России более 150 тыс. носителей РЖЯ. Например, в некоторых больницах все еще нет переводчика, что делает медицинскую помощь для носителей жестового языка менее доступной. То же касается и других важных организаций, взаимодействие с которыми является необходимостью. Помимо сложности взаимодействия, которая также способствует частичной социальной изоляции, глухие люди испытывают трудности в получении образования (жестовый язык не преподается в школах) и трудоустройстве.Пример жестов из датасета SlovoСоздание модели распознавания жестового языка может способствовать решению некоторых существующих проблем, став частью образовательного сервиса, сервиса видеоконференций или сервиса для фондов и гос. компаний (МФЦ, банки, вокзалы, аэропорты). Основным препятствием к получению подобной модели является сложность в создании подходящего набора данных. Мы попытались решить эту проблему путем создания набора данных для распознавания русского жестового языка (РЖЯ), о чем и пойдет речь ниже. Особенности РЖЯЖестовый язык в любой стране имеет ряд специфических особенностей, кардинально отличающих его от естественных языков и кратно усложняющих создание наборов данных для распознавания. Подобные и некоторые другие  особенности представлены ниже:Наличие множества диалектов в пределах одной страны (одного естественного языка). То есть, для большинства слов в, например, русском жестовом языке существует n-ое количество жестов, где n > 1. Тогда процесс обучения обобщенной на все диалекты модели будет существенно осложнен внутриклассовой вариативностью. В ином случае, для каждой модели будет необходим отдельный набор данных, что осложнено количеством различных диалектов, а значения слов в диалектах могут пересекаться;Некоторые слова в РЖЯ можно показать только с помощью дактиля (алфавита жестового языка), то есть для них нет специального жеста ни в одном из диалектов;Существенная часть слов в РЖЯ является последовательностью некоторого количества более простых слов, они называются составными; Большой социальный разрыв между двумя обществами создает проблему поиска носителей РЖЯ, а это в свою очередь порождает и неполноту знаний о всевозможных особенностях жестового языка;Значение жеста может зависеть от мимики и движений тела, что существенно осложняет распознавание похожих по движениям рук жестов.Для решения этих проблем в других странах проводятся различные попытки упрощения распознавания жестовых языков. Прямо сейчас компания Google запустила очередное соревнование с призовым фондом в 200.000$ для американского жестового языка, целью которого является распознавание слов по “буквам” (каждое слово конструируется жестовыми буквами).https://www.kaggle.com/competitions/asl-signsТакже около месяца назад от Google завершилось похожее соревнование на распознавание отдельных жестов американского ЖЯ с призовым фондом в 100.000$.https://www.kaggle.com/competitions/asl-fingerspellingОписание датасетаНам удалось собрать датасет, состоящий из более чем 200 тысяч видео, разделенных на 3 тысячи уникальных классов. В настоящий момент мы готовы опубликовать лишь небольшую часть этого датасета, которую мы назвали Slovo. Он состоит из 20 тысяч видео — по 20 видео на один из 1000 классов (жестов). Дополнительно к датасету добавлено 400 видео, на которых не происходит жестовых событий, чтобы ввести понятие класса «не жест». Датасет разбит на тренировочную (train), и тестовую (test) выборки с 15000 и 5000 сэмплами соответственно (“не жесты” отделены по 300 и 100 видео соответственно). Видео собраны с помощью 194 экспертов, прошедших экзамен на знание РЖЯ. Тестовая выборка сформирована путем привлечения 174 экспертов, а обучающая — с помощью 112 экспертов. Между обучающей и тестовой выборками есть небольшое пересечение по юзерам, но тестовый сет специально подобран так, чтобы эта выборка была максимально разнообразной. Датасет представлен в нескольких версиях:Исходный: полноценные HD-видео, на которых помимо целевых жестов РЖЯ содержится немного «бесполезной» информации (человек подходит к камере для записи, отходит от нее для завершения съемки и т.д.). Полный архив с данными весит около 100 ГБ;Обработанный: содержит обрезанные по разметке видео, на которых происходит выполнение целевых жестов. Она весит 15.86 ГБ, а суммарное время «полезной» записи — 9,2 часов (учитывались кадры только с жестовыми событиями).Вместе с датасетами мы отдаем разметку начала и конца жеста. Все видео собирались преимущественно в домашних условиях, с расстоянием до камеры в среднем от 0,5 до 2 метров. 35% видео записаны в HD формате, а 65% видео имеют FullHD разрешение, и все видео имеют кадровую частоту FPS = 30. Средняя длина «жестовых» видео — 50 кадров.На следующем рисунке представлены основные статистики по датасету Slovo. На первом графике показаны распределения по длительности видео, на втором и третьем распределения по количеству записанных видео на каждого эксперта. А на последнем изображена диаграмма распределения качества видеозаписей.Статистики датасетаКраудсорсингДля сбора и разметки РЖЯ мы использовали две российские краудсорсинг-платформы: Yandex.Toloka, на которой мы только собирали видео для датасета, и ABC Elementary, где, ввиду высокой квалификации разметчиков, проходили остальные этапы краудсорсинга — сбор, валидация и разметка РЖЯ. Также использование двух платформ позволяет получить две непересекающиеся аудитории знающих РЖЯ, что повышает качество финальных данных.Полный цикл создания датасета состоит из нескольких этапов:Экзамен — выявление группы пользователей, знающих РЖЯ,Сбор данных — получение необработанного датасета видео,Валидация — проверка качества датасета, удаление дубликатов и плохих данных (низкая кадровая частота или разрешение),Разметка — выявление участка видео с требуемым событием жеста + финальная агрегация результатов,Фильтрация — финальный постпроцессинг для подготовки чистого датасета.РЖЯ пайплайнЭкзаменЭкзамен является очевидной стадией сбора качественного датасета. Он позволяет отфильтровать из большой группы пользователей требуемых экспертов. Наш экзамен был относительно прост, но для не знакомых с РЖЯ людей пройти его было затруднительно. Пример экзамена на знание РЖЯПользователям предоставлялось от 10 до 15 видео, которые необходимо было посмотреть и классифицировать правильный жест на них. В итоге интерес к заданию проявили около 45 тысяч человек, а полностью экзамен прошли 25 тысяч пользователей, из которых около 400 пользователей получили высокий навык >= 90%. Экспертов с навыком 90% и выше мы привлекли для стадии валидации и разметки РЖЯ, а экспертов с навыком 80% и выше попросили записывать жесты в проекте майнинга данных. На следующем рисунке представлены графики распределения навыка знания РЖЯ в наших проектах.Распределение по экспертамК сожалению, это только общая статистика навыка на краудсорсинг-площадке. Некоторые пользователи получают навык и не выполняют задания вообще, либо выполняют только какую-то часть и теряют интерес к проекту (независимо от мотивирующих факторов, высокой стоимости заданий и т. д.). В конечном счете, в проекте записи РЖЯ приняло участие около 200 человек.Сбор данныхНа этой стадии мы попросили квалифицированных носителей РЖЯ записать видео с тем или иным жестом. К сбору данных допускались пользователи, прошедшие экзамен с навыком не менее 80%. Сбор данных интуитивно понятен и максимально прост — пользователю предоставляется слово из РЖЯ и видео-шаблон, на котором показан нужный жест. Нужно записать жест по аналогии и отправить его через краудсорсинг-площадку. Прогресс-бар выполнения заданий в одном из пулов на стадии сбора выглядит следующим образом:Прогресс выполнения задания на Yandex.TolokaВалидацияНесмотря на то, что мы отобрали группу носителей РЖЯ, это не избавляет нас от проблемы некачественного выполнения заданий, поэтому без стадии валидации обойтись нельзя. На этом этапе мы показывали каждое записанное видео отдельной группе людей, которые должны были определить, правильно оно записано или нет. Критерии правильности приемки достаточно простые:Камера исполнителя не дрожит;Жест должен быть выполнен верно;Руки не выходят за кадр в процессе выполнения жеста.К проекту валидации допускались пользователи, прошедшие экзамен с навыком не менее 90%. Чтобы повысить качество сбора датасета, каждое видео на этапе валидации проверяли от 3 до 5 независимых экспертов. По умолчанию уровень перекрытия равен 3, а если ответы экспертов не согласованы — перекрытие динамически увеличивалось сначала до 4, а потом, в случае необходимости, до 5 (этот процесс происходит автоматически на краудсорсинг площадке). Финальное решение агрегировано по «мнению большинства».Кроме того, мы использовали набор эвристик для контроля качества записи датасета и ввели дополнительные блокировки исполнителей на краудсорсинг площадке: 1) временный бан за быстрые ответы или пропуски; 2) временный или постоянный бан за неверные ответы по результатам валидации. На этой же стадии мы проверяли видео на качество записи и отбраковывали записи в плохом разрешении (min_side < 320), с низкой кадровой частотой (fps < 15), и слишком короткие записи длиной менее 1 секунды. Здесь же мы удаляли дубликаты видео, высчитывая phash отдельно взятых кадров.Также мы периодически проверяли качество ответов пользователей и блокировали плохих экспертов на постоянной основе, если процент правильно выполненных заданий был ниже 50% (более половины их ответов отклонены на этапе валидации).РазметкаНа этапе разметки мы отдавали видео пользователям, которые должны были определить начало и конец события. Перед экспертами также был шаблон правильного выполнения жеста. Перекрытие на этапе разметки строго фиксировано и равно 3, то есть каждое видео размечено тремя независимыми пользователями. Это позволяет получить качественную усредненную разметку и не волноваться за уверенность ответов пользователей (для случая, когда перекрытие = 1).После сбора разметки путем несложного алгоритма усреднения для каждого видео появляется размеченная область, на которой выполняется жест из РЖЯ. Записанные видео обрезаются по границам этой области и формируют финальный датасет.  На следующем рисунке представлен примерный пайплайн агрегации разметки:Собрать разметку от N = 3 пользователейРазделить на группы начало и конец разметкиПроверить максимальную разницу между границами в каждой группеВ случае успеха усреднить границыАгрегация разметки видеоК нашему удивлению, качество разметки получилось максимально высоким, а из 100K видео не удалось агрегировать только 6.В дополнение к этому, мы разметили обработанный датасет с помощью Mediapipe — на каждом кадре размечены кисти рук.  Это может повысить качество распознавания жестового языка, путем добавления алгоритмических эвристик.ФильтрацияНа этой стадии мы формируем финальный датасет. Процесс фильтрации состоял из нескольких этапов:Убираем неподходящие видео по параметрам width, height, fps, length;Убираем составные и сложные жесты;Отделяем дактиль от слов (пример: буква «я» и слово «Я»);По разметке нарезаем кадры с жестами из исходных видео;Ускоряем слишком медленные видео*Разбиваем датасет на обучающую и тестовую выборки по user_id.* — мы обнаружили, что для некоторых видео, выполнение жеста было слишком медленным. поэтому для каждого класса мы посчитали среднюю длину видео и пропорционально ускорили самые длинные видео до среднего значения.Эксперименты и демоНа нашем датасете мы обучили ряд нейронных сетей, большая часть из которых основана на современном подходе с Visual Transformers. Предварительно сетки были обучены на датасете Kinetics. Очевидно, что задача распознавания РЖЯ должна выполняться в режиме реального времени, в идеале на портативных и мобильных устройствах. Однако, далеко не все современные подходы могут обеспечить реал-тайм. Мы провели серию экспериментов с небольшими моделями типа Swin-small и Swin-tiny, но они показали очень плохие метрики. В качестве бейзлайна выбрали сверточную архитектуру на базе ResNet3D-50 и две тяжелые модели Swin-large c головой I3D и mVITv2-small. Поскольку жесты РЖЯ можно показывать любой рукой, в процессе обучения мы добавили горизонтальное отражение в качестве базовой аугментации.Также мы провели серию экспериментов с разным окном сэмплирования входных данных — 16, 32, 48 и 64 кадров. Поскольку эксперименты с размером окна в 64 кадра давали очень плохие метрики — мы не стали включать их в финальные результаты. Помимо этого, мы прореживали исходные видео с параметром децимации от 1 до 4 (в случае d = 1 видео не меняется). Эксперименты с d > 4 также показывали очень плохие метрики, поскольку пропускалось очень много кадров и даже визуально распознать в таком разреженном потоке какой-либо жест было невозможно. В качестве основной метрики мы выбрали mean accuracy. На следующем графике представлены результаты метрик для моделей ResNet3D-50 и Swin-large.Метрики для ResNet3D и Swin-LargeПодробное описание обучения моделей можно посмотреть в нашей статье.В репозитории вы найдете код для запуска ONNX и Pytorch моделей. Обращаем ваше внимание, что на CPU время инференса достаточно велико, и получить реал-тайм распознавание не получится. Если у вас есть GPU - запускайте модели с использованием видеокарт.Дальнейшие планыНа этом наши эксперименты не заканчиваются, и мы планируем развивать тему распознавания русского жестового языка по разным направлениям. Вот примерный список открытых проблем для жестовых языков, включая русский:Изучить различные диалекты РЖЯ и найти закономерности между ними для русского жестового языка и языков СНГ группы для создания универсального решения.Расширение жестовой корзины за счет добавления новых классов и увеличения числа видео на класс (сейчас в нашей команде идет процесс сбора датасета на около 3000 слов, в среднем по 50 видео на класс).Решение проблемы конструирования законченных предложений, определение пауз между словами и предложениями, распознавание составных жестов, решение задачи обратной лемматизации.Переход от текстовых предсказаний в домен звука (задача — video2speech).Переход в мульти-модальный домен с возможностью распознавания слов не только по жестам, но и по губам, мимике и позе человека.Решение обратной задачи text2video — это генерация видео по текстовым описаниям жестов (виртуальный сурдопереводчик)Мульти-язычный жестовый язык, поиск закономерностей в жестовых языках разных стран мира.ЗаключениеВ этой статье мы представили датасет Slovo и набор обученных на нем нейронных сетей для распознавания русского жестового языка. Датасет и модели распространяются под переработанной версией лицензии Creative Commons Corporation (Attribution-ShareAlike 4.0). Следите за обновлениями, обязательно будем публиковать успехи в этом направлении.Будем очень признательны за фидбек. Если среди вас или ваших знакомых есть носители РЖЯ, которые готовы поучаствовать в проекте — пишите нам, мы подробнее расскажем о задачах!АвторыАлександр Капитанов - @hukenovsКарина Кванчиани - @karinakvanchianiАлександр Нагаев - @nagaditЕлизавета Петрова - @kleinsbotleОтдельное спасибо дата-инженерам - Саутину Александру и Суровцеву Петру, которые 24/7 следят за качеством наших данных.СсылкиarXivGithub Gitlab KaggleТакже у нашей команды появился телеграм-канал, где мы будем рассказывать о результатах нашей работы, делиться идеями и даже факапами. Подписывайтесь.      Tags: распознавание жестоврусский жестовый языкРЖЯнейронные сетиобработка видеомашинное обучениеслабослышащиеглубокое обучениеdata science  Hubs: SberDevices corporate blogData MiningImage processingMachine learningArtificial Intelligence          \n\n\n\"`\n\n---"}, {"role": "assistant", "content": "[['Slovo', 'Slovo', 'Slovo', 'Slovo', 'Он', 'датасету', 'Датасет', 'Датасет', 'Slovo', 'датасета', 'Slovo', 'нем', 'Датасет'], ['русский жестовый язык', 'русский жестовый язык', 'русского жестового языка (РЖЯ)', 'РЖЯ', 'РЖЯ', 'самого языка', 'его', 'РЖЯ', 'РЖЯ', 'русского жестового языка (РЖЯ)', 'РЖЯ', 'русском жестовом языке', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'РЖЯ', 'русского жестового языка', 'русский', 'РЖЯ', 'русского жестового языка', 'русского жестового языка', 'русский жестовый язык', 'РЖЯ'], ['Data Mining', 'Data Mining'], ['Image processing', 'Image processing'], ['Machine learning', 'машинное обучение', 'Machine learning'], ['Artificial Intelligence', 'Artificial Intelligence'], ['этой статье', 'статье', 'статьи', 'этой статье'], ['мы', 'нам', 'мы', 'нашего', 'мы', 'нам', 'мы', 'Мы', 'Нам', 'мы', 'мы', 'мы', 'мы', 'мы', 'Наш', 'мы', 'наших', 'мы', 'мы', 'нас', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'нашему', 'мы', 'мы', 'мы', 'мы', 'нашем', 'мы', 'Мы', 'мы', 'мы', 'мы', 'мы', 'нашей', 'наши', 'мы', 'нашей', 'мы', 'нам', 'мы', 'наших', 'нашей', 'мы', 'нашей'], ['распознавания', 'распознавания', 'распознавание', 'распознавания', 'распознавание', 'распознавания', 'распознавания', 'распознавания', 'распознавание', 'распознавания', 'распознавание', 'распознавание', 'распознавания', 'распознавания', 'распознавание', 'распознавания', 'распознавание', 'распознавания', 'распознавания', 'распознавание'], ['слабослышащих', 'людей с нарушениями слуха и речи', 'они', 'глухие люди', 'слабослышащие'], ['открытом доступе', 'открытый доступ', 'открытый доступ'], ['универсального набора данных'], ['набора данных', 'датасета', 'датасет', 'датасета', 'наборов данных', 'датасета', 'набора данных', 'набора данных', 'наборов данных', 'набор данных', 'датасета', 'датасет', 'этого датасета', 'датасетами', 'датасету', 'датасета', 'датасета', 'датасета', 'датасета', 'датасета', 'датасета', 'датасета', 'датасета', 'датасет', 'датасет', 'датасет', 'датасет', 'датасете', 'датасета', 'датасет'], ['данных', 'данных', 'данных', 'данных', 'данных', 'данных', 'данными', 'данных', 'данных', 'данных', 'данных', 'данных', 'данных', 'данных'], ['сбора', 'сбора', 'сбор', 'Сбор', 'сбора', 'Сбор', 'этой стадии', 'сбору', 'Сбор', 'сбора', 'сбора', 'сбора', 'сбора'], ['разметки', 'разметки', 'разметка', 'Разметка', 'выше мы привлекли для стадии валидации и разметки', 'Разметка', 'этапе разметки', 'этапе разметки', 'разметки', 'разметки', 'разметку', 'разметки', 'разметки', 'разметки', 'разметке'], ['экспертов', 'экспертов', 'экспертов', 'экспертов', 'эксперта', 'экспертов', 'Экспертов', 'экспертов', 'экспертам', 'экспертов', 'экспертов', 'экспертов', 'экспертами'], ['жестовый датасет'], ['статьи'], ['нейронных сетей', 'нейронных сетей', 'которых', 'сетки', 'нейронных сетей', 'нейронные сети'], ['приложение'], ['жестового языка', 'жестовых языков', 'жестового языка', 'жестовый язык', 'жестового языка', 'жестового языка', 'жестового языка', 'жестовых языков', 'жестового языка', 'жестового языка', 'жестовых языков', 'жестовый язык', 'жестовых языках'], ['веса моделей'], ['моделей', 'модели распознавания жестового языка', 'подобной модели', 'Жестовый язык', 'его', 'модели', 'модели', 'моделями', 'они', 'моделей', 'моделей', 'моделей', 'модели', 'модели'], ['вы', 'вы', 'ваше', 'вас', 'вас', 'ваших'], ['репозитории', 'репозитории'], ['естественных языков', 'естественных языков', 'естественного языка'], ['Суммаризация текстов'], ['генерация новостей'], ['синтез речи'], ['распознавание голосовых сообщений'], ['части общества', 'которые'], ['России', 'России'], ['Всероссийское общество глухих (ВОГ)'], ['носителей РЖЯ', 'носителей жестового языка', 'носителей РЖЯ', 'носителей РЖЯ', 'носителей РЖЯ', 'носители РЖЯ', 'которые'], ['других важных организаций', 'которыми'], ['сложности взаимодействия', 'которая'], ['образования'], ['трудоустройстве'], ['жестов', 'жестов', 'жеста', 'жеста', 'жестов', 'жестов', 'жестов', 'жестов', 'жеста', 'жест', 'жесты', 'жестом', 'нужный жест', 'жест', 'жест', 'его', 'Жест', 'жеста', 'жеста', 'жест', 'жесты', 'жестами', 'жеста', 'жесты', 'жест', 'жестов', 'жестам', 'жестов', 'жестов'], ['диалектов', 'диалекты', 'диалектов', 'диалектах', 'диалектов', 'диалекты', 'ними'], ['страны', 'странах', 'стран'], ['слов', 'слов', 'слова', 'них', 'слов', 'они', 'слов', 'слово', 'слово', 'слов', 'слово', 'слов', 'словами', 'слов'], ['процесс обучения'], ['внутриклассовой вариативностью'], ['значения слов'], ['дактиля', 'дактиль'], ['алфавита'], ['простых слов'], ['социальный разрыв'], ['проблему поиска носителей РЖЯ'], ['неполноту знаний'], ['мимики', 'мимике'], ['движений тела'], ['движениям рук'], ['упрощения'], ['компания Google', 'Google'], ['соревнование', 'которого', 'соревнование'], ['призовым фондом', 'призовым фондом'], ['американского жестового языка', 'американского ЖЯ'], ['“буквам”', 'жестовыми буквами'], ['видео', 'видео', 'видео', 'видео', 'которых', 'видео', 'Видео', 'видео', 'которых', 'видео', 'видео', 'видео', 'видео', '«жестовых» видео', 'видео', 'видео', 'видеозаписей', 'видео', 'видео', 'видео', 'видео', 'которые', 'них', 'видео', 'видео', 'видео', 'оно', 'видео', 'записи', 'записи', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео', 'видео'], ['классов', 'классов', 'класса', 'класса', 'классов', 'класс', 'класс'], ['часть этого датасета', 'которую'], ['жестовых событий', 'жестовыми событиями', 'событием жеста', 'события'], ['«не жест»', '“не жесты”'], ['тренировочную (train)', 'обучающая', 'обучающей', 'обучающую'], ['тестовую (test) выборки', 'Тестовая выборка', 'тестовой выборками', 'тестовый сет', 'эта выборка', 'тестовую выборки'], ['сэмплами'], ['экзамен на знание РЖЯ'], ['юзерам', 'пользователей', 'пользователей', 'Пользователям', 'пользователей', 'которых', 'пользователей', 'пользователи', 'пользователи', 'пользователю', 'пользователи', 'пользователей', 'пользователям', 'которые', 'пользователями', 'пользователей', 'пользователей'], ['Исходный'], ['HD-видео', 'которых'], ['целевых жестов', 'целевых жестов'], ['человек'], ['камере', 'нее', 'камеры', 'Камера'], ['записи', 'записи', 'записи', 'видео', 'записи'], ['съемки'], ['архив'], ['ГБ', 'ГБ'], ['Обработанный', 'Она'], ['разметке', 'разметку'], ['выполнение'], ['время'], ['«полезной» записи'], ['часов'], ['кадры', 'кадров', 'кадров', 'кадре', 'кадры', 'кадров'], ['начала', 'начало', 'начало'], ['конца', 'конец', 'конец'], ['метров'], ['HD формате'], ['FullHD разрешение'], ['разрешение', 'разрешение', 'разрешении'], ['кадровую частоту', 'FPS', 'кадровая частота', 'кадровой частотой', 'fps'], ['длина', 'длительности', 'длиной', 'length', 'длину'], ['рисунке', 'рисунке', 'рисунке'], ['статистики', 'Статистики', 'статистика'], ['графике', 'графики', 'графике'], ['распределения', 'распределения', 'распределения', 'распределения', 'Распределение'], ['количеству записанных видео'], ['диаграмма'], ['качества', 'качество', 'качество'], ['Краудсорсинг', 'краудсорсинг', 'краудсорсинга'], ['краудсорсинг-платформы', 'платформ', 'краудсорсинг-площадке', 'краудсорсинг-площадку', 'краудсорсинг площадке', 'краудсорсинг площадке'], ['Yandex.Toloka', 'которой', 'Yandex.Toloka'], ['ABC Elementary'], ['квалификации разметчиков'], ['валидация', 'Валидация', 'валидации', 'Валидация', 'стадии валидации', 'этом этапе', 'валидации', 'этапе валидации', 'валидации', 'этой же стадии', 'этапе валидации'], ['аудитории'], ['качество', 'качества', 'качества', 'качество', 'качество', 'качеством'], ['этапов', 'стадией'], ['Экзамен', 'Экзамен', 'Экзамен', 'Он', 'экзамен', 'его', 'экзамена', 'заданию', 'экзамен', 'экзамен', 'экзамен'], ['группы пользователей', ' группы пользователей'], ['необработанного датасета видео'], ['дубликатов'], ['плохих данных'], ['участка видео'], ['агрегация результатов', 'агрегации', 'Агрегация'], ['результатов', 'результатам', 'результаты', 'результаты'], ['Фильтрация', 'Фильтрация', 'этой стадии', 'фильтрации'], ['постпроцессинг'], ['пайплайн', 'пайплайн'], ['людей', 'человек', 'человек', 'людей', 'которые', 'человека'], ['навык', 'навыком', 'навыком', 'навыка', 'навыка', 'навык', 'навыком', 'навыком'], ['проекте', 'проектах', 'проекту', 'проекте'], ['майнинга данных'], ['задания', 'заданий', 'заданий', 'задания', 'заданий', 'заданий'], ['видео-шаблон', 'котором', 'шаблон'], ['Прогресс-бар'], ['пулов'], ['некачественного выполнения заданий'], ['Критерии правильности приемки'], ['исполнителя', 'исполнителей'], ['Руки', 'рук', 'рукой'], ['кадр'], ['проекту валидации'], ['уровень перекрытия'], ['перекрытия', 'перекрытие', 'Перекрытие', 'перекрытие'], ['ответы', 'ответы', 'ответы', 'ответов', 'ответов'], ['решение'], ['эвристик', 'эвристик'], ['блокировки', 'бан', 'бан'], ['временный бан', 'временный'], ['быстрые ответы'], ['пропуски'], ['постоянный бан'], ['неверные ответы'], ['дубликаты видео'], ['phash'], ['плохих экспертов', 'их'], ['усредненную разметку'], ['уверенность'], ['усреднения'], ['размеченная область', 'которой', 'этой области'], ['границам', 'границами', 'границы'], ['максимальную разницу'], ['Mediapipe'], ['кисти рук'], ['параметрам', 'параметром'], ['width'], ['height'], ['буква «я»'], ['среднего значения'], ['Эксперименты', 'экспериментов', 'экспериментов', 'эксперименты', 'эксперименты'], ['демо'], ['Visual Transformers'], ['Kinetics'], ['в режиме реального времени', 'реал-тайм', 'реал-тайм'], ['Swin-small'], ['Swin-tiny'], ['метрики', 'метрики', 'метрики', 'метрики', 'метрик', 'Метрики'], ['бейзлайна'], ['сверточную архитектуру'], ['ResNet3D-50', 'ResNet3D-50', 'ResNet3D'], ['Swin-large', 'Swin-large', 'Swin-Large'], ['I3D'], ['mVITv2-small'], ['обучения', 'обучения'], ['горизонтальное отражение'], ['аугментации'], ['окном сэмплирования', 'окна'], ['входных данных'], ['эксперименты с размером окна в 64 кадра', 'их', 'Эксперименты'], ['децимации'], ['mean accuracy'], ['ONNX'], ['Pytorch'], ['CPU'], ['инференса'], ['GPU'], ['видеокарт'], ['языков СНГ группы'], ['жестовой корзины'], ['проблемы конструирования законченных предложений'], ['определение пауз между словами и предложениями'], ['задачи обратной лемматизации'], ['Переход от текстовых предсказаний в домен звука', 'задача — video2speech'], ['Переход в мульти-модальный домен'], ['губам'], ['позе'], ['Решение обратной задачи text2video'], ['генерация'], ['текстовым описаниям'], ['виртуальный сурдопереводчик'], ['Мульти-язычный жестовый язык'], ['лицензии Creative Commons Corporation (Attribution-ShareAlike 4.0)'], ['фидбек'], ['проекте'], ['Александр Капитанов'], ['Карина Кванчиани'], ['Александр Нагаев'], ['Елизавета Петрова'], ['дата-инженерам', 'которые'], ['Саутину Александру'], ['Суровцеву Петру'], ['телеграм-канал'], ['глубокое обучение'], ['data science']]"}]}
{"messages": [{"role": "system", "content": "Ты являешься экспертом в задаче решения референций из текстов на русском языке."}, {"role": "user", "content": "**Ты решаешь задачу разрешения кореференций (coreference resolution) в текстах на русском языке с помощью пошагового рассуждения (Chain-of-Thought).**\n\nСледуй указанным шагам:\n\n---\n\n### \uD83D\uDD39 Шаг 1 (Извлечение упоминаний):\n\n- Прочитай текст.\n- Выпиши все упоминания сущностей (людей, компаний, продуктов, сервисов и т.д.), а также местоимения и фразы, которые могут на них ссылаться.\n- Учитывай не только имена собственные, но и абстрактные понятия (например, \"база знаний\", \"информация\", \"уязвимости\"), если на них ссылаются в тексте.\n- Постарайся различать случаи, когда одно и то же слово может обозначать разные вещи в зависимости от контекста (например, \"ИИ\" может означать как технологию, так и конкретного помощника — Atlassian Intelligence).\n\n---\n\n### \uD83D\uDD39 Шаг 2 (Определение кореференций и группировка):\n\n- Определи, какие из упоминаний относятся к одной и той же сущности.\n- Учитывай грамматические признаки (род, число, падеж), контекст и смысл текста.\n- Если возникают сомнения, выбери наиболее вероятную связь. В сложных случаях можешь кратко пояснить причину (1–2 слова).\n- Сгруппируй упоминания по кластерам — каждому кластеру соответствует одна сущность.\n\n---\n\n### \uD83D\uDD39 Шаг 3 (Итоговый результат):\n\n- Верни только результат в виде массива массивов.\n- Каждый внутренний массив — отдельный кластер кореференций.\n- Не добавляй комментариев, пояснений или шагов рассуждения к итоговому выводу.\n\n---\n\n### \uD83D\uDCCC Пример:\n\n**Текст:**\n\n«Мария живет в Москве. Она любит этот город за его атмосферу.»\n\n**Ответ:**\n\n```json\n[\n  [\"Мария\", \"Она\"],\n  [\"Москве\", \"этот город\", \"его\"]\n]\n```\n\n---\n\nТеперь выполни аналогичный анализ для следующего текста, соблюдая описанные шаги рассуждения:\n\n`\"\n\nПишем первый ML-пайплайн на Airflow: подробный туториал / Habr\n\n\n               Пишем первый ML-пайплайн на Airflow: подробный туториал  Reading time  \n    13 min\n   Views  4.2K Python *Machine learning *Natural Language Processing * \n    Tutorial\n        Ждешь, когда обновятся данные, чтобы запустить переобучение моделиВ этом туториале мы пошагово разберем, как создать с нуля и запустить локально свой первый пайплайн на Airflow. Данный пайплайн специально адаптирован под задачи машинного обучения. В этом примере мы будем загружать новости из открытого источника и использовать NLP-модель для их классификации (zero-shot classification). План:Примеры применения Airflow в проектах с машинным обучением.Знакомство с Airflow: основные понятия и инструменты.Написание тасок для загрузки данных и получения предсказания модели.Запуск Airflow локально через Docker Compose.Знакомство с веб-интерфейсом Airflow.Код доступен на GitHub.Как Airflow используется в проектах с машинным обучениемAirflow хорошо подходит для задач, которые запускаются в указанное время или каждый заданный интервал времени.Apache Airflow - популярный инструмент в проектах с машинным обучением. Он позволяет создавать эффективные и масштабируемые пайплайны, связанные с обработкой данных, обучением и развертыванием моделей.Примеры задач, которые решает Airflow:Регулярное переобучение моделей машинного обучения на новых данных.Получение предсказаний модели в пакетном режиме. Например, раз в час или раз в день.ETL пайплайны, которые загружают данные из разных источников и преобразуют их.Автоматическая генерация отчетов.Ниже приведен пример архитектуры рекомендательной системы, где каждая из 3 частей запускается на Airflow:Feature Engineering: данные от пользователей накапливаются и обрабатываются единоразово в заданное время.Training Pipeline: на основе актуальных данных модель переобучается, например, раз в день.Batch Prediction Pipeline: последняя версия модели используется, чтобы рассчитать новые рекомендации по всем пользователям и сохранить в базу.Архитектура рекомендательной системы на AirflowШаг 3 в этом пайплайне используется получение предсказания модели в пакетном формате. Когда это имеет смысл?Преимущество пакетной обработки: нам не надо беспокоиться о latency модели. Работа с моделью в пакетном режиме, как правило, не требует дополнительной оптимизации и позволяет быстрее довести модель до прода.Недостаток: предсказания имеют запаздывание. Например, рекомендации не будут учитывать real-time фидбек от пользователя. Если для вашей задачи критична работа модели в real-time формате подойдут другие инструменты. В прошлом туториале мы разбирали, как написать ML веб-сервис на FastAPI, который работает с моделью в формате запрос-ответ.Знакомство с Airflow: основные понятия и инструментыПрежде чем начать практическую часть, познакомимся с основными понятиями Apache Airflow.Airflow DAG - примерDAG (Directed Acyclic Graph) - удобный способ организации и визуализации пайплайна. Это структура, где каждая задача представлена узлом, а зависимости между задачами представлены направленными стрелками, указывающими порядок выполнения задач. Граф является направленным, потому что задачи выполняются последовательно в определенном порядке, определяемом зависимостями.Task: \"кирпичи\", из которых состоит DAG. Задачи представляют собой конкретные шаги или операции, которые должны быть выполнены в вашем пайплайне.Operator: Каждая задача выполняется некоторым кодом или скриптом, который вы определяете. Операторы (Operators) представляют собой классы или функции, которые определяют, как будет выполнена каждая задача в пайплайне. Например, есть операторы, отвечающие за выполнение кода на питоне, bash-команд и SQL-запросов. Metadata Database: в этой базе хранятся метаданные о структуре пайплайнов, зависимостях между задачами, расписании выполнения и других параметрах.Webserver: удобный пользовательский интерфейс (UI), позволяющий запускать, мониторить и отлаживать пайплайны. Через веб-интерфейс можно просматривать статус выполнения задач, проверять логи, а также управлять пайплайнами, вносить изменения и контролировать их работу.Scheduler: следит за графом пайплайна, определяет, какие задачи уже могут быть выполнены на основе зависимостей и расписания, и запускает их в соответствии с этой логикой. Он также отслеживает выполнение задач и обновляет их статусы в метаданных.0. Описание задачи и проектирование пайплайнаГлавная цель этого туториала - знакомство с Airflow и самостоятельный запуск пайплайна. То есть, по итогу туториала вы научитесь локально запускать Airflow.Но, для использования Airflow в продакшене необходимы другие инструменты, например, Kubernetes.Apache Airflow предоставляет возможности для создания сложных пайплайнов с множеством задач и зависимостей между ними. Однако, здесь мы ограничимся простым примером графа пайплайна.Наш пайплайнПайплайн будет состоять из 3 задач:Task 1: Подготовка данных - загрузка данных, препроцессинг и сохранение в локальную директорию.Task 2: Предсказание модели - загрузка модели, инференс модели на сохраненных данных.Task 3: Подготовка отчета на основе предсказаний.Task 1 и Task 2 мы хотим запустить в отдельных Docker-контейнерах. Для этого будем использовать DockerOperator.Зачем может понадобиться запускать таски в отдельных контейнерах?Это дает стандартные преимущества контейнеризации: изоляция, гибкость и тд. Например, применительно к ML, код расчета фичей и код предсказания модели могли использовать разные версии scikit-learn.Также DockerOperator позволяет ближе познакомиться с тем, как происходит выполнение задач в разных контейнерах с помощью Kubernetes в продакшен-среде.Task 3 содержит простой код агрегации, поэтому будем использовать PythonOperator.В данном примере пайплайна мы выбрали упрощенный подход и не использовали дополнительные инструменты, чтобы не усложнять настройку Airflow:Для обмена данными между задачами мы использовали локальную директорию data. В реальных проектах часто используются специализированные хранилища данных - Amazon S3, DVC и другие.В качестве источника данных выбран открытый источник - финансовые новости с cnbc.com. В реальных задачах это будут данные из внутренних баз данных или тот же S3.Модель загружали из открытого реестра моделей - Hugging Face Hub. Также сознательно выбрали модель zero-shot classification, которая способна работать с заранее заданным списком классов, не требуя дополнительного дообучения. В реальных проектах это будет специально обученная модель, хранящаяся в реестре моделей, например, MLflow.Таким образом, в данном примере мы использовали упрощенные варианты для целей наглядности и понимания основных концепций. В реальных проектах нужно настраивать и использовать дополнительные инструменты и хранилища данных.С учетом вышесказанного код проекта будет выглядеть следующим образом:Структура проектаdata: вспомогательная директория, будет использоваться для обмена данными между тасками и сохранения результатов.ml_pipeline: содержит код для загрузки и подготовки данных data_loader и код предсказания модели model_prediction. Как уже обсуждалось ранее, каждая таска будет запускаться в отдельном Docker-контейнере, поэтому у обеих таск есть свой Dockerfile.dags: в этой директории находятся файлы, описывающие DAG. Airflow будет использовать эти файлы для планирования и запуска задач в определенном порядке.docker-compose.yml: определяет конфигурацию контейнеров, необходимых для запуска и работы Airflow.1. Загрузка данных, используем DockerДля получения данных о финансовых новостях в этом примере мы будем использовать фид от cnbc.com. Когда мы запрашиваем информацию, он предоставляет нам 30 самых новых новостей из мира финансов. Таким образом, запуская пайплайн раз в день, мы сможем получать каждый день свежие новости.Простейший код для загрузки данных в формате csv будет выглядеть следующим образом:import feedparser\nimport pandas as pd\nNEWS_FEED_URL = \"https://www.cnbc.com/id/19746125/device/rss/rss.xml\"\ndef data_load(data_path: str) -> None:\n    news_feed = feedparser.parse(NEWS_FEED_URL)\n    df = pd.DataFrame(news_feed.entries)\n    df.to_csv(data_path, sep=\"\\t\", index=False)Помимо этого в финальную версию data_load.py добавим:Использование Click и опции командной строки - для возможности гибко задавать параметры при запуске скрипта. Логирование - для возможности отслеживать работу пайплайнов и обнаружения возможных проблем. Позже эти сообщения мы увидим в интерфейсе Airflow.Дополнительную обработку данных.Финальная версия data_load.py выглядит так:import html\nimport logging\nimport click\nimport feedparser\nimport pandas as pd\nNEWS_FEED_URL = \"https://www.cnbc.com/id/19746125/device/rss/rss.xml\"\nCOLUMNS_TO_SAVE = [\"id\", \"published\", \"title\", \"summary\"]\nlogging.basicConfig(level=logging.INFO)\n@click.command()\n@click.option(\"--data_path\", help=\"Path to the input data CSV file\")\ndef data_load(data_path: str) -> None:\n    logging.info(\"Fetching financial news from the RSS feed...\")\n    news_feed = feedparser.parse(NEWS_FEED_URL)\n    logging.info(\"News fetched successfully.\")\n    df = pd.DataFrame(news_feed.entries)[COLUMNS_TO_SAVE]\n    df[\"published\"] = pd.to_datetime(df[\"published\"])\n    df[\"title\"] = df[\"title\"].map(html.unescape)\n    df[\"summary\"] = df[\"summary\"].map(html.unescape)\n    logging.info(f\"Saving the processed data to '{data_path}'...\")\n    df.to_csv(data_path, sep=\"\\t\", index=False)\n    logging.info(\"Data saved successfully.\")\nif __name__ == \"__main__\":\n    data_load()Для подготовки Docker-образа осталось создать:список зависимостей - requirements.txt:feedparser==6.0.10\nclick==8.1.3\npandas==2.0.1Dockerfile:FROM python:3.11\nCOPY requirements.txt data_load.py /workdir/\nWORKDIR /workdir\nRUN pip install -r requirements.txtТаким образом, у нас готовы все файлы для создания Docker-образа. Мы соберем образ и создадим контейнер позже - в разделе про Docker Compose.2. Предсказание модели, zero-shot classificationВ этом примере будем использовать NLP-модель для задачи zero-shot classification.Что такое zero-shot classification?Zero-shot classification (классификация без обучения) - это метод машинного обучения, при котором модель классифицирует объекты на классы, которых не было в процессе обучения модели.Этот подход позволяет модели генерализовать свои знания и классифицировать объекты на основе общего понимания классов, даже если она не имеет конкретного опыта с каждым классом.Для начала определим список классов - тем, на которые мы будем разделять финансовые новости:LABELS = [\n    \"Crypto\",\n    \"SEC\",\n    \"Dividend\",\n    \"Economics\",\n    \"Oil or Gas\",\n    \"IPO\",\n    \"Politics\",\n    \"Buffet\",\n    \"Stock\",\n    \"Other\",\n]Для получения предсказаний загрузим модель valhalla/distilbart-mnli-12-1 из Hugging Face Hub. device=-1 означает, что модель запускается на CPU.from transformers import pipeline\nmodel_hf = pipeline(model=\"valhalla/distilbart-mnli-12-1\", device=-1)Далее загрузим csv файл с новостями, который мы подготовили в предыдущем пункте. Для предсказания будем использовать объединение текста из заголовка новости (title) и ее краткого описания (summary).import pandas as pd\ndf = pd.read_csv(data_path, sep=\"\\t\")\ntexts_for_pred = (df.title + \". \" + df.summary).tolist()Для получения предсказаний передадим модели список текстов texts_for_pred и классы LABELS: pred = model_hf(texts_for_pred, LABELS, multi_label=False)Флаг multi_label определяет, может ли объект быть отнесен к одному или более классам. Когда multi_label=True, модель может присваивать объектам несколько классов одновременно. Таким образом, мы сами придумали название классов на свое усмотрение. Модель делает предсказание по нашим классам без необходимости предварительного обучения. В этом преимущество zero-shot моделей.Благодаря библиотеки transformers код занял всего несколько строк.Выберем предсказание лучшего класса и сохраним результат в json-файл:df[\"label\"] = [x[\"labels\"][0] for x in pred]\ndf.T.to_json(pred_path)На этом код предсказания готов. Добавим логирование и использование Click по аналогии с кодом загрузки данных. Тогда финальная версия model_predict.py будет выглядеть следующим образом:import logging\nimport click\nimport pandas as pd\nfrom transformers import pipeline\nLABELS = [\n    \"Crypto\",\n    \"SEC\",\n    \"Dividend\",\n    \"Economics\",\n    \"Oil or Gas\",\n    \"IPO\",\n    \"Politics\",\n    \"Buffet\",\n    \"Stock\",\n    \"Other\",\n]\nlogging.basicConfig(level=logging.INFO)\n@click.command()\n@click.option(\"--data_path\", help=\"Path to the input data CSV file\")\n@click.option(\"--pred_path\", help=\"Path to save the output JSON file\")\ndef model_predict(data_path: str, pred_path: str) -> None:\n    logging.info(\"Loading the model...\")\n    model_hf = pipeline(model=\"valhalla/distilbart-mnli-12-1\", device=-1)\n    logging.info(\"Model loaded successfully.\")\n    logging.info(f\"Reading data from '{data_path}'...\")\n    df = pd.read_csv(data_path, sep=\"\\t\")\n    logging.info(\"Data read successfully.\")\n    \n    texts_for_pred = (df.title + \". \" + df.summary).tolist()\n    logging.info(\"Performing model prediction...\")\n    pred = model_hf(texts_for_pred, LABELS, multi_label=False)\n    logging.info(\"Prediction completed successfully.\")\n    df[\"label\"] = [x[\"labels\"][0] for x in pred]\n    logging.info(f\"Saving the predictions to '{pred_path}'...\")\n    df.T.to_json(pred_path)\n    logging.info(\"Predictions saved successfully.\")\nif __name__ == \"__main__\":\n    model_predict()\nКод предсказания модели будет также запускаться в отдельном Docker-контейнере. Поэтому аналогично предыдущему пункту мы добавили свои requirements.txt и Dockerfile.Итак, мы подготовили код для 2 компонент нашего пайплайна: загрузки данных и получения предсказания модели. Каждый будет выполняться в отдельном Docker-контейнере. Теперь все готово, чтобы мы перешли к написанию DAG на Airflow.3. Пишем DAGВспомним основные компоненты нашего пайплайна:3 последовательные таски, первые 2 из которых должны запускаться в отдельных Docker-контейнерах.Локальная директория data, которую мы будем использовать для сохранения результатов и обмена данными между тасками.Разберем основные шаги при написании нашего DAG:Локальную директорию data нужно примонтировать к /opt/airflow/data/ - это путь к данным внутри контейнера Airflow.from docker.types import Mount\ndockerops_kwargs = {\n    \"mount_tmp_dir\": False,\n    \"mounts\": [\n        Mount(\n            source=\"<path_to_your_airflow-ml_repo>/data\",\n            target=\"/opt/airflow/data/\",\n            type=\"bind\",\n        )\n    ],\n    ...\n}Определим пути для трех типов файлов: исходные данные, предсказания и файл с результатом. Они используют специальный синтаксис Airflow {{ ds }}, который будет заменен на дату выполнения при запуске DAG.raw_data_path = \"/opt/airflow/data/raw/data__{{ ds }}.csv\"\npred_data_path = \"/opt/airflow/data/predict/labels__{{ ds }}.json\"\nresult_data_path = \"/opt/airflow/data/predict/result__{{ ds }}.json\"Создадим DAG. Декоратор dag создает DAG с названием financial_news с начальной датой сегодня (days_ago(0)) и ежедневным запуском. Функция taskflow представляет собой сам DAG и содержит задачи, формирующие пайплайн.from airflow.decorators import dag\nfrom airflow.utils.dates import days_ago\n# Create DAG\n@dag(\"financial_news\", start_date=days_ago(0), schedule=\"@daily\", catchup=False)\ndef taskflow():\n  ...Создадим две таски для запуска в Docker-контейнерах. Для это будем использовать DockerOperator. Здесь мы указываем имя образа (этот образ будет описан в docker-compose.yml) и команду для запуска питоновского скрипта внутри контейнера.     # Task 1\n    news_load = DockerOperator(\n        task_id=\"news_load\",\n        container_name=\"task__news_load\",\n        image=\"data-loader:latest\",\n        command=f\"python data_load.py --data_path {raw_data_path}\",\n        **dockerops_kwargs,\n    )\n    # Task 2\n    news_label = DockerOperator(\n        task_id=\"news_label\",\n        container_name=\"task__news_label\",\n        image=\"model-prediction:latest\",\n        command=f\"python model_predict.py --data_path {raw_data_path} --pred_path {pred_data_path}\",\n        **dockerops_kwargs,\n    )Создадим последнюю таску, она преобразует полученные предсказания питоновским кодом. Мы будем использовать PythonOperator, который выполнит несложный скрипт группировки предсказаний.    # Task 3\n    news_by_topic = PythonOperator(\n        task_id=\"news_by_topic\",\n        python_callable=aggregate_predictions,\n        op_kwargs={\n            \"pred_data_path\": pred_data_path,\n            \"result_data_path\": result_data_path,\n        },\n    )Установим зависимости между задачами. В нашем случае они выполняются последовательно:news_load >> news_label >> news_by_topicНаконец, создадим и настроим объект DAG в соответствии с заданными параметрами:taskflow()Файл с описанием DAG имеет расширение .py и лежит в директории dags. При запуске Airflow, он сканирует эту директорию (или другую настроенную директорию) в поисках файлов с определением DAG. Когда Airflow обнаруживает файл с определением DAG, он регистрирует его и делает доступным для выполнения по расписанию.Мы закончили писать наш пайплайн, теперь перейдем к настройке и запуску Airflow.4. Запуск Airflow с помощью Docker ComposeДля локального запуска Airflow мы будем использовать Docker Compose. Он помогает запустить Apache Airflow с минимальными усилиями, предоставляя унифицированный и изолированный способ запуска всех компонентов Airflow.У Airflow есть отличная инструкция по запуску с помощью Docker Compose. Там же есть загрузка готового файла docker-compose.yml. Инструкция позволяет запустить Airflow в пару строк.Файл docker-compose.yml имеет раздел services, где определены различные сервисы, которые являются частями кластера Airflow. Каждый сервис имеет свою секцию с настройками, где указывается образ Docker, команда для запуска сервиса, порты, зависимости от других сервисов и другие параметры. В частности здесь описаны сервисы для Metadata Database, Webserver и Scheduler, которые мы упомянали в разделе Знакомство с Airflow. Модификация docker-compose.yml для запуска тасок в отдельных Docker-контейнерах Поскольку мы усложнили настройку Airflow, когда решили запускать таски в отдельных контейнерах, будем использовать свой модифицированный docker-compose.yml. Его можно посмотреть тут.Основные моменты, которые мы изменили для поддержки запуска тасок в Docker-контейнерах:Установили пакет для поддержки работы с Docker:_PIP_ADDITIONAL_REQUIREMENTS: apache-airflow-providers-docker==3.6.0В список volumes добавили монтирование директории с данными и сокета Docker:  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/data/:/opt/airflow/data/\n    - /var/run/docker.sock:/var/run/docker.sockРанее в директориях ml_pipeline/data_loader и ml_pipeline/model_prediction мы написали инструкции по созданию Docker-образов, которые импользуются для запуска тасок с помощью DockerOperator. Здесь мы также определяем эти сервисы:  data-loader:\n    build:\n      context: ml_pipeline/data_loader\n    image: data-loader\n    restart: \"no\"\n  model-prediction:\n    build:\n      context: ml_pipeline/model_prediction\n    image: model-prediction\n    restart: \"no\"Добавим сервис docker-socket-proxy:  # Required because of DockerOperator. For secure access and handling permissions.\n  docker-socket-proxy:\n    image: tecnativa/docker-socket-proxy:0.1.1\n    environment:\n      CONTAINERS: 1\n      IMAGES: 1\n      AUTH: 1\n      POST: 1\n    privileged: true\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    restart: alwaysЗапуск AirflowВозможно, в вашем случае запуск потребует выделения больше памяти для Docker Engine.Перед первым запуском Airflow нужно подготовить окружение.Если вы работаете на Linux перед запуском нужно указать AIRFLOW_UID:echo -e \"AIRFLOW_UID=$(id -u)\" > .envДалее независимо от вашей ОС необходимо выполнить миграцию базы данных и создать первую учетную запись пользователя. Для этого выполните команду:docker compose up airflow-initСозданная учетная запись имеет логин airflow и пароль airflow.Запуск и остановка AirflowДля создания и запуска всех необходимых контейнеров, определенных в файле docker-compose.yml, используется команда:docker compose upВ нашем случае также необходимо предварительно собрать образы data-loader и model-prediction, которые также указаны в файле docker-compose.yml. Поэтому модифицируем команду:docker compose up --buildКогда вы закончите работу и захотите очистить свое окружение, выполните:docker compose down --volumes --rmi allПосле запуска Airflow продолжим работу в веб-интерфейсе.5. Веб-интерфейс Airflow, запуск пайплайнаПользовательский интерфейс Airflow упрощает мониторинг и запуск пайплайнов. Он доступен по адресу http://localhost:8080. На странице входа нужно ввести логин и пароль от учетной записи, в нашем случае airflow и airflow.Страница авторизацииЗапуск и мониторинг пайплайнаНа домашней странице вы увидете список всех дагов, включая дефолтные от Airflow. Здесь можно найти и выбрать созданный нами DAG financial_news.На странице нашего DAG доступна разная информация о пайплайне: графическое представление, время ближайшего запуска, логи запуска, код и многое другое. Графическое представление нашего пайплайнаНе дожидаясь планового запуска пайплайна, запустим DAG, нажав на кнопку старта. Для отслеживания выполнения отдельных тасок, можно нажать на нужную таску и посмотреть ее логи:Логи таски 1Здесь мы можем видеть логи, которые добавили специально для отслеживания выполнения отдельных шагов.После того, как выполнение пайплайна успешно завершилось, посмотрим на результаты. Смотрим результат пайплайна. Как классифицировались новости?Результаты выполнения пайплайна сохранились в data/predict/result__<date>.json.  Изначально мы ставили задачу написать пайплайн, которые будет автоматически загружать актуальные новости из финансового мира и группировать их по заданным нами темам. Посмотрим, что у нас получилось.Результат работы пайплайнаТаким образом, мы справились с поставленной задачей. Новости успешно загружаются и классифицируются. Напомним, что темы заданы нами произвольно, без предварительного обучения модели. ЗаключениеМы подробно рассмотрели, как создать с нуля и запустить локально свой первый ML-пайплайн на Airflow.В этом примере мы написали таски для загрузки новостей из открытого источника и их классификации NLP-моделью (zero-shot classification). Каждая таска выполняется в отдельном Docker-контейнере.Код доступен на GitHub. Вы легко можете воспроизвести пайплайн и изучить его подробнее.Если формат туториалов по инструментам MLOps окажется полезным, буду планировать темы для следующих статей. А пока подписывайтесь на мой телеграм-канал. Там будут анонсы новых статей, а также советы для работы и более короткие мысли по DS/ML/AI.       Tags: airflowdocker composezero-shot classificationdata sciencemlopsмашинное обучениеnlpdagmachine learning  Hubs: PythonMachine learningNatural Language Processing"}, {"role": "assistant", "content": "[['ML', 'ML', 'Machine learning', 'машинного обучения', 'машинным обучением', 'машинным обучением', 'машинным обучением', 'машинного обучения', 'ML', 'ML', 'машинного обучения', 'машинное обучение', 'machine learning', 'Machine learning'], ['ML-пайплайн', 'ML-пайплайн', 'ML-пайплайн'], ['пайплайн', 'пайплайн', 'пайплайн', 'пайплайн', 'пайплайны', 'пайплайны', 'пайплайне', 'пайплайна', 'пайплайне', 'пайплайне', 'пайплайнов', 'пайплайны', 'пайплайнами', 'пайплайна', 'пайплайна', 'пайплайна', 'пайплайнов', 'пайплайна', 'пайплайн', 'Пайплайн', 'пайплайна', 'пайплайн', 'пайплайнов', 'пайплайна', 'пайплайна', 'пайплайн', 'пайплайн', 'пайплайна', 'пайплайнов', 'пайплайна', 'пайплайне', 'пайплайна', 'пайплайна', 'пайплайна', 'пайплайна', 'пайплайна', 'пайплайн', 'которые', 'пайплайна', 'пайплайн', 'его'], ['Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Apache Airflow', 'Он', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Apache Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Apache Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'он', 'Airflow', 'Airflow', 'Airflow', 'Apache Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'airflow', 'airflow', 'Airflow', 'Airflow', 'Airflow', 'Airflow', 'airflow', 'airflow', 'Airflow', 'Airflow', 'airflow'], ['туториал', 'туториал', 'Tutorial', 'туториале', 'туториале', 'туториалов'], ['Natural Language Processing', 'NLP', 'nlp', 'Natural Language Processing'], ['данные', 'данных', 'данных', 'данных', 'данные', 'их', 'данные', 'данных', 'данных', 'данных', 'данных', 'данными', 'данные', 'данных', 'данными', 'данных', 'данных', 'данных', 'данных', 'данных', 'данных', 'данных', 'данным', 'данные', 'данными'], ['переобучение', 'переобучение'], ['модели', 'модель', 'модели', 'моделей', 'моделей', 'модели', 'модель', 'модели', 'модели', 'модели', 'моделью', 'модель', 'модели', 'моделью', 'модели', 'модели', 'модели', 'модели', 'моделей', 'модель', 'которая', 'моделей', 'модели', 'модели', 'модель', 'модели', 'модели', 'свои', 'она', 'модель', 'модели', 'модель', 'Модель', 'моделей', 'модели', 'модели'], ['этом туториале', 'этого туториала', 'туториала'], ['мы', 'свой', 'мы', 'нам', 'мы', 'мы', 'Наш', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'нам', 'мы', 'мы', 'нас', 'Мы', 'мы', 'мы', 'мы', 'сами', 'свое', 'нашим', 'мы', 'мы', 'нашего', 'мы', 'нашего', 'мы', 'нашего', 'мы', 'Мы', 'нашем', 'Мы', 'наш', 'мы', 'мы', 'мы', 'мы', 'мы', 'мы', 'нашем', 'нашем', 'нами', 'нашего', 'нашего', 'мы', 'мы', 'нами', 'нас', 'мы', 'нами', 'Мы', 'свой', 'мы'], ['этом примере', 'этом примере'], ['новости', 'их', 'новости', 'новостях', 'новостей', 'новости', 'новости', 'новостями', 'новости', 'ее', 'новости', 'новости', 'их', 'Новости', 'новостей', 'их'], ['открытого источника', 'открытый источник', 'открытого источника'], ['NLP-модель', 'NLP-модель', 'NLP-моделью'], ['классификации', 'классификация', 'классификации'], ['zero-shot classification', 'zero-shot classification', 'zero-shot classification', 'zero-shot classification', 'zero-shot classification', 'Zero-shot classification', 'классификация без обучения', 'zero-shot', 'zero-shot classification', 'zero-shot classification'], ['проектах', 'проектах', 'проектах', 'проектах', 'проекта'], ['тасок', 'таска', 'таск', 'таски', 'таску', 'она', 'тасок', 'таски', 'тасок', 'тасок', 'тасок', 'таску', 'ее', 'таски', 'таски', 'таска'], ['загрузки', 'загрузка', 'загрузка', 'загрузки', 'Загрузка', 'загрузки', 'загрузки', 'загрузки', 'загрузка', 'загрузки'], ['предсказания', 'предсказаний', 'предсказания', 'предсказания', 'Предсказание', 'предсказаний', 'предсказания', 'предсказания', 'Предсказание', 'предсказаний', 'предсказания', 'предсказаний', 'предсказание', 'предсказание', 'предсказания', 'предсказания', 'предсказания', 'предсказания', 'предсказания', 'предсказаний'], ['Запуск', 'запуск', 'запуска', 'запуска', 'запуском', 'запуска', 'запуска', 'запуску', 'Запуск', 'запуска', 'запуска', 'запуску', 'запуска', 'запуска', 'запуска', 'запуска', 'Запуск', 'запуск', 'запуском', 'запуском', 'Запуск', 'запуска', 'запуск', 'запуск', 'Запуск', 'запуска', 'запуска', 'запуска'], ['Docker', 'Docker', 'Docker', 'Docker', 'Docker', 'Docker', 'Docker', 'Docker', 'Docker'], ['Docker Compose', 'Docker Compose', 'Docker Compose', 'Docker Compose', 'Он', 'Docker Compose', 'docker compose'], ['веб-интерфейсом', 'Webserver', 'веб-интерфейс', 'Webserver', 'веб-интерфейсе', 'Веб-интерфейс'], ['Код', 'Код'], ['GitHub', 'GitHub'], ['задач', 'которые'], ['обработкой', 'обработку'], ['обучением', 'процессе обучения', 'обучения', 'обучения'], ['развертыванием'], ['задач', 'которые'], ['пакетном режиме', 'пакетном формате', 'пакетном режиме'], ['час'], ['день', 'день', 'день', 'день'], ['ETL'], ['ETL пайплайны', 'которые'], ['источников', 'источника'], ['генерация'], ['отчетов', 'отчета'], ['архитектуры'], ['рекомендательной системы', 'рекомендательной системы'], ['Feature Engineering'], ['пользователей', 'пользователям', 'пользователя', 'пользователя'], ['Training Pipeline'], ['Batch Prediction Pipeline'], ['рекомендации', 'рекомендации'], ['базу'], ['Архитектура'], ['пакетной обработки'], ['latency'], ['дополнительной оптимизации'], ['прода'], ['запаздывание'], ['real-time', 'real-time'], ['фидбек'], ['вашей', 'вашем', 'вы', 'вы', 'вашем', 'вы', 'вашей', 'вы', 'свое', 'вы', 'Вы'], ['веб-сервис', 'который'], ['FastAPI'], ['запрос-ответ'], ['практическую часть'], ['DAG', 'DAG', 'Directed Acyclic Graph', 'DAG', 'DAG', 'DAG', 'DAG', 'DAG', 'DAG', 'DAG', 'DAG', 'сам DAG', 'DAG', 'DAG', 'DAG', 'DAG', 'DAG', 'DAG', 'DAG', 'dag'], ['организации'], ['визуализации'], ['структура', 'структуре', 'Структура'], ['задача', 'задачами', 'задач', 'задачи', 'Задачи', 'задача', 'задача', 'задачами', 'задач', 'задачи', 'их', 'задач', 'их', 'задач', 'ними', 'задач', 'задачами', 'задач', 'задачи', 'задачами', 'они'], ['узлом'], ['зависимости', 'зависимостями', 'зависимостях', 'зависимостей', 'зависимостей', 'зависимости', 'зависимости'], ['направленными стрелками'], ['порядок выполнения', 'порядке', 'порядке'], ['Граф', 'графом', 'графа'], ['\"кирпичи\"', 'которых'], ['шаги'], ['операции'], ['которые'], ['Operator', 'Операторы', 'Operators', 'операторы'], ['кодом', 'кода'], ['скриптом', 'скрипта', 'скрипта', 'скрипт'], ['который'], ['классы', 'классов', 'классы', 'которых', 'классов', 'классом', 'классов', 'которые', 'классы', 'классам', 'классов', 'классов', 'классам', 'класса'], ['функции'], ['которые'], ['питоне', 'Python', 'Python'], ['bash-команд'], ['SQL-запросов'], ['Metadata Database', 'этой базе', 'Metadata Database'], ['метаданные', 'метаданных'], ['расписании выполнения', 'расписания'], ['параметрах'], ['пользовательский интерфейс (UI)', 'интерфейсе', 'Пользовательский интерфейс', 'Он'], ['статус выполнения', 'статусы'], ['логи', 'логи', 'логи', 'Логи', 'логи', 'которые'], ['изменения', 'их'], ['Scheduler', 'Он', 'Scheduler'], ['Описание'], ['проектирование'], ['продакшене', 'продакшен'], ['Kubernetes', 'Kubernetes'], ['создания'], ['3 задач'], ['Task 1', 'Task 1'], ['Подготовка', 'Подготовка', 'подготовки'], ['препроцессинг'], ['сохранение', 'сохранения'], ['локальную директорию', 'локальную директорию', 'Локальная директория', 'Локальную директорию'], ['Task 2', 'Task 2'], ['инференс'], ['Task 3', 'Task 3'], ['Docker-контейнерах', 'контейнерах', 'контейнерах', 'Docker-контейнере', 'контейнеров', 'контейнер', 'Docker-контейнере', 'Docker-контейнерах', 'контейнера', 'Docker-контейнерах', 'контейнера', 'Docker-контейнерах', 'контейнерах', 'Docker-контейнерах', 'контейнеров', 'Docker-контейнере'], ['DockerOperator', 'DockerOperator', 'DockerOperator', 'DockerOperator'], ['таски', 'тасками', 'таски', 'которых', 'тасками'], ['контейнеризации'], ['изоляция'], ['гибкость'], ['код расчета фичей'], ['код предсказания модели'], ['scikit-learn'], ['код агрегации'], ['PythonOperator', 'PythonOperator', 'который'], ['упрощенный подход'], ['настройку', 'настройке', 'настройку'], ['обмена данными', 'данных', 'обмена данными', 'обмена данными'], ['data', 'data', 'data', 'которую', 'data'], ['реальных проектах', 'реальных задачах', 'реальных проектах', 'реальных проектах'], ['специализированные хранилища данных'], ['Amazon S3', 'S3'], ['DVC'], ['cnbc.com', 'cnbc.com', 'он'], ['внутренних баз данных'], ['баз данных', 'базы данных'], ['Модель'], ['открытого реестра моделей'], ['Hugging Face Hub', 'Hugging Face Hub'], ['дообучения'], ['специально обученная модель'], ['реестре моделей'], ['MLflow'], ['наглядности'], ['основных концепций'], ['хранилища'], ['код проекта'], ['директория', 'директории', 'директории', 'директорию', 'директории', 'директориях'], ['результатов', 'результат', 'результатов', 'результатом', 'результаты', 'результат', 'Результаты', 'Результат'], ['ml_pipeline'], ['код', 'код', 'код', 'код', 'код', 'кодом', 'Код', 'код', 'кодом', 'код'], ['data_loader', 'data_load'], ['model_prediction'], ['обеих таск', 'свой'], ['Dockerfile.dags', 'этой директории'], ['файлы', ' эти файлы', 'файлы', 'файлов', 'файл', 'Файл', 'он', 'файлов', 'файл', 'его', 'файла'], ['планирования'], ['docker-compose.yml'], ['конфигурацию'], ['работы', 'работу', 'работы', 'работу', 'работы'], ['фид'], ['мира финансов', 'финансового мира'], ['формате csv', 'csv файл', 'который'], ['data_load.py', 'data_load.py'], ['Click', 'Click'], ['опции командной строки'], ['параметры', 'параметрами', 'параметры'], ['Логирование', 'логирование'], ['Docker-образа', 'образ', 'образа', 'этот образ', 'образ Docker', 'Docker-образов', 'образы', 'которые'], ['метод', 'котором'], ['объекты', 'объекты', 'объект', 'объектам', 'объект'], ['тем', 'темам', 'темы'], ['LABELS', 'LABELS'], ['valhalla/distilbart-mnli-12-1', 'модель'], ['CPU'], ['текста', 'текстов'], ['заголовка новости (title)'], ['краткого описания (summary)'], ['Флаг'], ['multi_label', 'multi_label'], ['библиотеки transformers'], ['строк', 'строк'], ['json-файл'], ['model_predict.py'], ['requirements.txt'], ['Dockerfile'], [' /opt/airflow/data/ '], ['путь', 'пути'], ['типов файлов', 'Они'], ['исходные данные'], ['синтаксис', 'который'], ['дату выполнения'], ['Декоратор dag'], ['financial_news'], ['начальной датой'], ['сегодня'], ['Функция taskflow', 'собой'], ['имя'], ['docker-compose.yml', 'docker-compose.yml', 'Файл docker-compose.yml', 'docker-compose.yml', 'docker-compose.yml', 'Его', 'файле docker-compose.yml', 'файле docker-compose.yml'], ['команду', 'команда'], ['группировки'], ['расширение .py'], ['dags', 'эту директорию'], ['инструкция', 'Инструкция'], ['готового файла'], ['раздел services'], ['сервисы', 'которые', 'сервис', 'сервиса', 'сервисов', 'сервисы'], ['кластера'], ['секцию'], ['настройками'], ['порты'], ['Metadata Database, Webserver и Scheduler', 'которые'], ['Модификация'], ['моменты', 'которые'], ['поддержки', 'поддержки'], ['пакет'], ['список volumes'], ['монтирование'], ['сокета'], ['ml_pipeline/data_loader'], ['ml_pipeline/model_prediction'], ['инструкции по созданию Docker-образов', 'которые'], ['памяти'], ['Docker Engine'], ['окружение', 'окружение'], ['Linux '], ['ОС'], ['миграцию'], ['учетную запись', 'учетная запись', 'учетной записи'], ['логин', 'логин'], ['пароль', 'пароль'], ['остановка'], ['data-loader'], ['model-prediction,'], ['мониторинг', 'мониторинг'], ['адресу http://localhost:8080'], ['входа'], ['авторизации'], ['домашней странице'], ['дагов'], ['графическое представление', 'Графическое представление'], ['время ближайшего запуска'], ['кнопку старта'], ['MLOps', 'mlops'], ['мой'], ['телеграм-канал', 'Там']]"}]}
{"messages": [{"role": "system", "content": "Ты являешься экспертом в задаче решения референций из текстов на русском языке."}, {"role": "user", "content": "**Ты решаешь задачу разрешения кореференций (coreference resolution) в текстах на русском языке с помощью пошагового рассуждения (Chain-of-Thought).**\n\nСледуй указанным шагам:\n\n---\n\n### \uD83D\uDD39 Шаг 1 (Извлечение упоминаний):\n\n- Прочитай текст.\n- Выпиши все упоминания сущностей (людей, компаний, продуктов, сервисов и т.д.), а также местоимения и фразы, которые могут на них ссылаться.\n- Учитывай не только имена собственные, но и абстрактные понятия (например, \"база знаний\", \"информация\", \"уязвимости\"), если на них ссылаются в тексте.\n- Постарайся различать случаи, когда одно и то же слово может обозначать разные вещи в зависимости от контекста (например, \"ИИ\" может означать как технологию, так и конкретного помощника — Atlassian Intelligence).\n\n---\n\n### \uD83D\uDD39 Шаг 2 (Определение кореференций и группировка):\n\n- Определи, какие из упоминаний относятся к одной и той же сущности.\n- Учитывай грамматические признаки (род, число, падеж), контекст и смысл текста.\n- Если возникают сомнения, выбери наиболее вероятную связь. В сложных случаях можешь кратко пояснить причину (1–2 слова).\n- Сгруппируй упоминания по кластерам — каждому кластеру соответствует одна сущность.\n\n---\n\n### \uD83D\uDD39 Шаг 3 (Итоговый результат):\n\n- Верни только результат в виде массива массивов.\n- Каждый внутренний массив — отдельный кластер кореференций.\n- Не добавляй комментариев, пояснений или шагов рассуждения к итоговому выводу.\n\n---\n\n### \uD83D\uDCCC Пример:\n\n**Текст:**\n\n«Мария живет в Москве. Она любит этот город за его атмосферу.»\n\n**Ответ:**\n\n```json\n[\n  [\"Мария\", \"Она\"],\n  [\"Москве\", \"этот город\", \"его\"]\n]\n```\n\n---\n\nТеперь выполни аналогичный анализ для следующего текста, соблюдая описанные шаги рассуждения:\n\n`\"\n\nКак приручить чат GPT. Я проверил, кто пишет код быстрее — я или он / Habr\n\n\n               Как приручить чат GPT. Я проверил, кто пишет код быстрее — я или он  Reading time  \n    9 min\n   Views  8.4K AGIMA corporate blog Research and forecasts in IT *Artificial Intelligence  \n    Review\n        Всем привет! Меня зовут Марк Коваль, я iOS-разработчик в AGIMA. Хайп вокруг чата GPT постепенно стихает, но внятного ответа, как он изменит нашу жизнь, по-прежнему нет. Все привыкают к тому, что технологии AI ворвались в нашу реальность и что-то с ней сделали, но что — никто не понимает. Может быть, через 10 лет половины IT-профессий уже просто не будет, а может быть, нейросети станут нашими верными помощниками. В этой статье я попробую разложить, как подружить бизнес, чат GPT и разработку.ДисклеймерВ этой статье я делюсь своим мнением. Вы можете с ним согласиться или не согласиться, но давайте признаем: будущее туманно, и любой взгляд на него может оказаться ошибочным. А может — и правдивымКакие профессии под угрозой из-за GPTФакт, подтвержденный цифрами: количество разработчиков растет из года в год. Вот график, который это подтверждает:https://inclient.ru/outflow-it-specialists/#skolko-ajtisnikov-v-rossiiСможет ли GPT остановить этот процесс, пока неясно. Но есть плюс-минус объективные предпосылки к тому, что технологии AI смогут сносно писать код и заменить программистов как минимум в рутинных задачах.Есть и другие профессии в диджитал, с которыми GPT может успешно конкурировать. Я составил список сфер, в которых нейросети уже сейчас играют большую роль.1. РазработкаУже сейчас благодаря GPT разработчики могут генерировать код, создающий привлекательный пользовательский интерфейс в мобильных приложениях. Посмотрим, что предложит GPT, получив простой запрос в качестве входных данных.Выглядит неплохо. И написал он это за несколько секунд. Однако возникает вопрос: как GPT отреагирует, если в запрос добавить немного «реальности»?В этом случае мы не получили готового кода. Но мы всегда можем ясно указывать GPT, какого результата ждем исходя из особенностей выбранного языка программирования.Пока GPT выглядит как полезный партнер для разработчика, но не «волшебная таблетка» для решения бизнес-задач.2. КопирайтерыGPT отлично генерирует текст. Можно предположить, что в простых задачах он сможет заменить копирайтеров. Достаточно одного точного запроса.Всего несколько секунд, и у нас готовый рекламный текст. Да, он требует редактуры, но базовую работу автора GPT вроде как выполнил.Правда, есть нюансы. Я посетил «Кибер клуб» — там нет возможности играть по сети. В клубе есть только PlayStation 4 без VR-системы, и там работает только один администратор. Он поделился, что они не проводят турниры, но вы можете поиграть с друзьями по очереди.Делаем вывод: GPT в своих предположениях может быть слишком вольным и говорить то, чего на самом деле нет. Он сгенерировал текст без необходимого контекста. Это показывает, что GPT требует направления со стороны человека. Так что на этом этапе работа копирайтера по-прежнему важна.GPT — отличный помощник в написании текстов, но пока он не может создать правильный рекламный текст для привлечения новых клиентов.3. Техническая поддержкаТеперь протестируем GPT в роли специалиста технической поддержки.Это хорошие базовые советы по «ремонту» интернета. При этом интересен пятый пункт. Я позвонил в службу поддержки, там мне ответил робот. Он вежливо сообщил, что передает мой вопрос специалисту.Мы видим, что GPT хочет помочь мне с проблемой, но, к сожалению или к счастью, отправляет меня за консультацией к человеку. Нейросеть пока только сократила путь к решению проблемы.Вступаю в неравный бой с GPTСо способностями GPT мы разобрались. Но может ли программист использовать GPT для ускорения работы? Предлагаю устроить небольшое соревнование. Представим, что у нас есть задача от бизнеса: при нажатии на кнопку в приложении должна открываться webView с памяткой о том, что делать в случае пожара. Пользователь читает памятку, закрывает webView и возвращается обратно в приложение.Несложная задача. Засечем время и посмотрим, сколько времени мне потребуется на эту функцию и сколько времени понадобится GPT.(ссылка на гифку, где я написал код, для открытия памятки по пожарной безопасности) https://drive.google.com/file/d/1VyIE5HdDCyLrhlrXOjfwqIebcIB-z0qJ/view?usp=sharingМне понадобилось примерно 15 минут, чтобы создать 2 класса. Первый класс содержит реализацию кнопки, которая открывает памятку по нажатию. Второй класс отвечает за webView и загружает изображение по ссылке. Также предусмотрена кнопка «Закрыть» для возвращения в приложение.Теперь, когда мы определились с тем, что и как именно хотим реализовать, давайте составим подробный запрос и отправим его в разработку через чат GPT.Написание запроса заняло около 2 минут, чтобы учесть все факторы и явно указать GPT, что именно мы хотим реализовать. Теперь давайте запустим код и посмотрим, какой результат получился у GPT.Компилятор вместо запуска симулятора с кнопкой и памяткой выдал нам несколько ошибок. Но это не проблема. Мы сообщим GPT о его неточностях и посмотрим, как он себя поведет.Используем навык command C — command V и смотрим на результат!Вот так незадача. Не буду показывать, как я сообщаю GPT о его ошибках, и приведу результат уже полностью исправленного и рабочего кода.(ссылка на гифку, где gpt написал код, для открытия памятки) https://drive.google.com/file/d/1w5A9hpSJhCBXLs6kjfZt3tySUW5kqLmy/view?usp=sharingGPT успешно справился с задачей и реализовал то, что мы просили, хоть и не с первого раза. Очевидно, что мы подошли к написанию кода по-разному. Я написал больше кода с возможностью переиспользования экрана webView. GPT не учел этого, но, будем честны, такая задача перед ним и не стояла.GPT открыл webView в модальном режиме, в то время как моя реализация открывает его без использования модального экрана. GPT использовал меньше кода, что имеет свои преимущества, но вопрос о возможности переиспользования остается открытым. В итоге, даже с учетом всех правок и неточностей, GPT справился с задачей быстрее меня.Однако если учесть, что в запросе стоило указать предпочтения по архитектуре, принципам SOLID и другим важным аспектам, которые программисты обычно учитывают, опасаюсь, что запрос бы превратился в мини-эссе. В конечном счете, это всего лишь простая функция открытия памятки из интернета. Однако это заставляет задуматься о том, как GPT будет себя вести в действительно больших проектах с обширной кодовой базой. Тем не менее, нельзя не отметить, что GPT является отличным компаньоном для программиста в рабочих задачах.Можно ли использовать GPT в работеВот несколько мыслей на этот счет.GPT является полезным компаньоном в работе.В отличие от GitHub Copilot, GPT не опережает нас и не пытается предугадать наши следующие шаги. Это как переписка с опытным разработчиком: GPT можно задавать глупые вопросы, чтобы быстро понять новый проект. Например, мы можем попросить GPT объяснить, что делает определенный класс в проекте. После отправки кода на анализ мы можем получить информативный ответ, в котором GPT опишет функциональность класса, реализуемые протоколы и другие сведения.GPT можно попросить оценить написанный нами код и попросить предложить улучшения.В сущности, речь про локальный рефакторинг. Естественно, не стоит полностью полагаться на предложения GPT, но иногда они бывают полезными. Например, предложение создать протокол или вынести функционал в отдельный класс или использовать функции высшего порядка, если мы вдруг упустили этот момент.Если нет вариантов решения или готовых решений задачи, можно спросить GPT.Возможно, он предложит как минимум хорошую идею, хоть и не полностью готовую реализацию.Если в проекте есть фрагменты кода на незнакомом языке или используется непривычный фреймворк, GPT поможет освоиться с новым инструментом быстрее, чем прочтение скучной документации.Как бизнес видит дружбу с чатом GPTС разработкой мы разобрались. Теперь рассмотрим, чем GPT может быть полезен бизнесу и каких результатов можно достичь с его помощью. Например, GPT может предложить идеи для MVP-проекта и помочь с их возможной реализацией.В эпоху разнообразных курсов и дополнительного образования идея учебного приложения выглядит интересно. Хотя нельзя сказать, что идея нова. Попросим GPT предложить возможные варианты реализации.Это стандартный подход к созданию MVP-проекта. Указаны основные моменты и предоставлены рекомендации по выбору инструмента. Однако, если мы будем останавливаться на каждом пункте, мы просто потеряемся во множестве вариантов и решений. Нужно расставить приоритеты.Давайте упростим процесс планирования и обратимся к GPT для создания приложения, ведь именно для этого мы его и хотим использовать.GPT проигнорировал нашу просьбу.Но тут важно учесть, что GPT отказывается создавать проект, потому что у него недостаточно контекста для работы. При этом он всё еще может быть полезен при более детальной и ясной формулировке задач. Выше был хороший пример, когда GPT исправлял конкретные ошибки, выявленные компилятором. Здесь ситуация аналогичная. Мы без труда можем генерировать идеи и работать над ними с помощью «чудо-чата».Кажется, что GPT способен помочь каждому специалисту в решении узких задач. Как гласит поговорка, не следует пытаться съесть слона целиком, а нужно делить его на маленькие кусочки.Примеры сотрудничества бизнеса с GPT-чатом и его преимуществаGPT — это своего рода источник знаний на все случаи. Помня об этом, рассмотрим несколько интересных проектов, которые не сделаны GPT, но сделаны командами разработки на его основе.ChatGPTConsulting — инновационный проект, использующий GPT-4 для анализа данных и автоматизации работы с документами. Стартап помогает компаниям принимать обоснованные решения и снижает нагрузку на сотрудников благодаря применению искусственного интеллекта GPT-4.https://www.chatgptconsultancy.com/Ora — веб-платформа для быстрого создания собственных чат-ботов на основе GPT-3.5 с будущей поддержкой GPT-4. Идеально подходит для тех, кто нуждается в виртуальных помощниках или собеседниках по конкретным темам.https://ora.ai/ AI-Generated Video Scripts — сервис, использующий GPT-4 для автоматического написания сценариев видео. Это упрощает работу создателям контента, экономит время и позволяет им сфокусироваться на креативных аспектах процесса. Гибкость в адаптации под различные жанры и виды видео делает этот проект перспективным.https://www.veed.io/tools/script-generator/Ограничения и вызовыНо разработчики при работе с чатом GPT сталкиваются и со сложностями.Нужно предоставлять точный и четкий контекст, чтобы получить работающее решение. Если у чата нет ясного представления о желаемом результате, мы столкнемся с постоянным исправлением ошибок компилятора. Мы можем добавлять больше контекста с каждым новым запросом, но это потребует больше времени, особенно при выполнении объемных задач и ограниченном опыте работы с нужной библиотекой. GPT позволяет сократить время разработки функциональности, но только если мы сами знаем, что и как нужно реализовать.Еще одна сложность — ограниченное количество токенов, которые можно передать в одном запросе. Проще говоря, мы ограничены по количеству символов в тексте запроса. По своему опыту скажу, что GPT хорошо обрабатывает код длиной 200–250 строк. Но если продолжать общение в том же чате и добавлять новые объемные куски кода, в какой-то момент GPT может начать выдавать некорректные ответы. В таком случае придется перейти к новому чату, поскольку исправить «сломанный» чат уже не удастся. Важно помнить, что GPT работает только с кодом, который был отправлен ему, и не имеет информации о вашем проекте или окружении.Если вы используете свои собственные внешние зависимости, GPT не будет обладать информацией о них. В такой ситуации есть два варианта. Вы можете сообщить GPT о ваших зависимостях, но это может привести к поломке или требовать оптимизации предоставленного кода под ваши библиотеки. Оба этих варианта приводят к увеличению времени работы.GPT не имеет доступа к интернету. Это означает, что предлагаемые варианты кода будут основаны на решениях или фреймворках, существующих на момент обучения модели. Это ограничивает гибкость в предлагаемых вариантах решений.Другие спорные моментыНе стоит также забывать о стороне бизнеса. Я выделил несколько пунктов, которые подчеркивают важность наличия команды разработки.GPT не предоставляет готового решения. Даже если мы предположим, что с помощью чата можно создать действительно работающий сервис, возникает вопрос: кто будет выполнять эту работу? Фактически GPT может изменить подход к проектной работе, но не может изменить структуру разработческих команд.Возникает вопрос и к безопасности. При разработке коммерческого продукта, полностью основанного на GPT, мы вынуждены передавать в чат код, который является собственностью нашей компании. Существуют ли гарантии, что кодовые базы или обсуждаемые идеи не будут утечкой информации? Также возникает вопрос о том, не будут ли новые разработки, предложенные компанией, использованы против нее. Это остается открытым вопросом.Пожалуй, это один из главных и спорных вопросов. Если код был сгенерирован с помощью чата GPT, то владение им принадлежит компании, которая его использовала, или же компании, которая разработала GPT? В текущем состоянии разработки у нас есть заказчик, который желает получить продукт, и команда, которая работает над его разработкой. В юридической сфере все детали обсуждены и проработаны. Однако, когда мы начинаем работать с GPT, мы оказываемся в серой зоне, которую человечеству еще предстоит разрешить. Как регулировать юридические аспекты взаимодействия между человеком и чатом GPT?Так всё-таки — помощник или угроза?В отношениях разработчика, бизнеса и GPT каждый элемент играет важную роль. GPT становится всё более важной частью нашей жизни. Например, мне зачастую уже проще обратиться к чату, чем искать ответы в поисковых системах. GPT предоставляет конкретные ответы на заданные вопросы, несмотря на некоторые огрехи.Сейчас можно уже уверенно сказать, что пока чат GPT — это новое поколение поисковиков. Оно может помочь в решении конкретных задач, но не воплотит все наши желания через один запрос.А пока мы пытаемся разобраться, друг ли нам чат GPT, приглашаю вас в другой чат — он точно наш друг. Это наш телеграм-канал про разработку AGIMA Dev. Мы там делимся статьями про технологии, обсуждаем тренды, рассказываем об интересных событиях. В общем, подключайтесь.      Tags: gptчат-ботChatGPTConsultingOraAI-Generated Video Scripts  Hubs: AGIMA corporate blogResearch and forecasts in ITArtificial Intelligence"}, {"role": "assistant", "content": "[['чат GPT', 'он', 'чат GPT', 'он', 'чата GPT', 'он', 'чат GPT', 'GPT', 'GPT', 'GPT', 'GPT', 'GPT', 'он', 'GPT', 'GPT', 'GPT', 'GPT', 'он', 'GPT', 'GPT', 'своих', 'Он', 'GPT', 'GPT', 'он', 'GPT', 'GPT', 'Нейросеть', 'GPT', 'GPT', 'GPT', 'GPT', 'чат GPT', 'GPT', 'GPT', 'GPT', 'его', 'он', 'GPT', 'его', 'gpt', 'GPT', 'GPT', 'ним', 'GPT', 'GPT', 'GPT', 'GPT', 'себя', 'GPT', 'GPT', 'GPT', 'GPT', 'GPT', 'GPT', 'GPT', 'GPT', 'GPT', 'GPT', 'он', 'GPT', 'чатом GPT', 'GPT', 'его', 'GPT', 'GPT', 'GPT', 'его', 'GPT', 'GPT', 'него', 'он', 'GPT', '«чудо-чата»', 'GPT', 'GPT-чатом', 'GPT', 'GPT', 'его', 'чатом GPT', 'чата', 'GPT', 'GPT', 'GPT', 'GPT', 'ему', 'GPT', 'GPT', 'GPT', 'модели', 'GPT', 'чата', 'GPT', 'GPT', 'чат', 'чата GPT', 'GPT', 'GPT', 'чатом GPT', 'GPT', 'GPT', 'чату', 'GPT', 'чат GPT', 'чат GPT', 'gpt', 'ChatGPT'], ['Я', 'я', 'Я', 'я', 'Меня', 'Марк Коваль', 'я', 'я', 'я', 'своим', 'Я', 'Я', 'Я', 'мне', 'мой', 'мне', 'меня', 'мне', 'я', 'Мне', 'я', 'Я', 'моя', 'меня', 'своему', 'Я', 'мне'], ['iOS'], ['AGIMA', 'AGIMA', 'AGIMA'], ['нашу', 'нашу', 'нашими', 'мы', 'мы', 'нас', 'Мы', 'мы', 'нас', 'мы', 'мы', 'нам', 'Мы', 'мы', 'нас', 'наши', 'мы', 'мы', 'нами', 'мы', 'мы', 'мы', 'мы', 'мы', 'нашу', 'Мы', 'мы', 'Мы', 'мы', 'сами', 'мы', 'мы', 'мы', 'нашей', 'нас', 'мы', 'мы', 'нашей', 'наши', 'мы', 'наш', 'наш', 'Мы'], ['технологии AI', 'технологии AI'], ['AI', 'AI', 'искусственного интеллекта', 'Artificial Intelligence'], ['нашу реальность', 'ней'], ['IT', 'IT'], ['нейросети', 'нейросети'], ['этой статье', 'этой статье'], ['разработку', 'Разработка', 'разработкой', 'разработки', 'разработки', 'разработке', 'разработки', 'разработкой'], ['своим мнением', 'ним'], ['Вы', 'вы', 'вашем', 'вы', 'свои собственные', 'Вы', 'ваших', 'ваши', 'вас'], ['будущее', 'него'], ['разработчиков', 'программистов', 'разработчики', 'разработчика', 'программист', 'программисты', 'программиста', 'разработчики', 'разработчика'], ['график', 'который'], ['профессии', 'которыми'], ['диджитал'], ['сфер', 'которых'], ['пользовательский интерфейс'], ['запрос', 'запрос', 'запроса', 'запрос', 'запросе', 'запрос', 'запросом', 'запросе', 'запроса', 'запрос'], ['входных данных'], ['языка программирования', 'языке'], ['Копирайтеры', 'копирайтеров', 'копирайтера'], ['текст', 'текст', 'текст', 'текстов', 'текст'], ['рекламный текст', 'он'], ['редактуры'], ['автора'], ['«Кибер клуб»', 'там', 'клубе', 'там', 'они'], ['PlayStation 4'], ['VR-системы'], ['администратор', 'администратор. Он'], ['контекста', 'контекста', 'контекст', 'контекста'], ['человека', 'человеку', 'человеком'], ['Техническая поддержка', 'технической поддержки', 'поддержки'], ['интернета', 'интернета', 'интернету'], ['службу поддержки', 'там'], ['робот', 'Он'], ['вопрос'], ['специалисту'], ['задача от бизнеса', 'задача', 'задачей', 'задачей'], ['кнопку', 'кнопки', 'которая', 'кнопкой'], ['приложении', 'приложение', 'приложение'], ['webView', 'webView', 'webView', 'webView', 'webView', 'его'], ['памяткой', 'памятку,', 'памятки', 'памятку', 'памяткой', 'памятки', 'памятки'], ['пожара'], ['Пользователь'], ['пожарной безопасности'], ['2 класса'], ['Первый класс'], ['Второй класс'], ['изображение'], ['кнопка «Закрыть»'], ['подробный запрос', 'его', 'запроса'], ['Компилятор', 'компилятором', 'компилятора'], ['ошибок', 'неточностях', 'ошибках', 'ошибок'], ['навык command C — command V'], ['мы'], ['возможностью переиспользования экрана', 'этого', 'такая задача', 'возможности переиспользования'], ['модальном режиме', 'модального экрана'], ['GPT использовал меньше кода', 'свои'], ['архитектуре'], ['принципам SOLID'], ['важным аспектам', 'которые'], ['GitHub Copilot'], ['новый проект', 'проекте'], ['определенный класс', 'класса'], ['информативный ответ', 'котором'], ['функциональность', 'функциональности'], ['протоколы ', 'протокол'], ['локальный рефакторинг'], ['предложения GPT', 'они'], ['функционал'], ['отдельный класс'], ['функции высшего порядка'], ['незнакомом языке'], ['фреймворк', 'новым инструментом', 'фреймворках'], ['документации'], ['бизнес', 'бизнесу', 'бизнеса', 'бизнеса', 'бизнеса'], ['идеи для MVP-проекта', 'их'], ['MVP-проекта', 'MVP-проекта'], ['реализацией', 'реализации'], ['курсов'], ['дополнительного образования'], ['идея учебного приложения', 'идея'], ['учебного приложения', 'приложения', 'проект'], ['приоритеты'], ['процесс планирования'], ['создания приложения', 'этого'], ['идеи', 'ними'], ['слона', 'его'], ['сотрудничества бизнеса с GPT-чатом', 'его'], ['источник знаний'], ['проектов', 'которые', 'проект'], ['командами разработки', 'команды разработки', 'разработческих команд', 'команда', 'которая'], ['ChatGPTConsulting', 'Стартап', 'Consulting'], ['GPT-4', 'GPT-4', 'GPT-4', 'GPT-4'], ['анализа данных'], ['автоматизации работы'], ['документами'], ['компаниям'], ['сотрудников'], ['Ora', 'Ora'], ['веб-платформа'], ['чат-ботов', 'чат-бот'], ['GPT-3.5'], ['виртуальных помощниках'], ['собеседниках'], ['AI-Generated Video Scripts', 'этот проект', 'AI-Generated Video Scripts'], ['автоматического написания сценариев видео'], ['видео', 'видео'], ['создателям контента', 'им'], ['Ограничения'], ['вызовы'], ['сложностями', 'сложность'], ['библиотекой'], ['количество токенов'], ['токенов', 'которые'], ['количеству символов'], ['код', 'кода'], ['чате', 'чату', 'чат'], ['кодом', 'который'], ['проекте'], ['окружении'], ['внешние зависимости', 'них', 'зависимостях'], ['два варианта', 'Оба этих варианта'], ['поломке'], ['оптимизации'], ['библиотеки'], ['решениях', 'решений', 'решения'], ['обучения'], ['гибкость'], ['пунктов', 'которые'], ['сервис'], ['проектной работе'], ['структуру'], ['безопасности'], ['коммерческого продукта'], ['код', 'который'], ['собственностью'], ['нашей компании', 'компании', 'которая'], ['кодовые базы'], ['утечкой информации'], ['разработки'], ['компанией', 'нее'], ['код', 'им'], ['его'], ['компании', 'которая'], ['заказчик', 'заказчик, который'], ['продукт', 'его'], ['юридической сфере'], ['серой зоне', 'которую'], ['человечеству'], ['ответы'], ['поисковых системах', 'поисковиков'], ['ответы'], ['вопросы'], ['новое поколение поисковиков', 'Оно'], ['другой чат', 'он'], ['телеграм-канал'], ['AGIMA Dev'], ['технологии'], ['тренды']]"}]}
{"messages": [{"role": "system", "content": "Ты являешься экспертом в задаче решения референций из текстов на русском языке."}, {"role": "user", "content": "**Ты решаешь задачу разрешения кореференций (coreference resolution) в текстах на русском языке с помощью пошагового рассуждения (Chain-of-Thought).**\n\nСледуй указанным шагам:\n\n---\n\n### \uD83D\uDD39 Шаг 1 (Извлечение упоминаний):\n\n- Прочитай текст.\n- Выпиши все упоминания сущностей (людей, компаний, продуктов, сервисов и т.д.), а также местоимения и фразы, которые могут на них ссылаться.\n- Учитывай не только имена собственные, но и абстрактные понятия (например, \"база знаний\", \"информация\", \"уязвимости\"), если на них ссылаются в тексте.\n- Постарайся различать случаи, когда одно и то же слово может обозначать разные вещи в зависимости от контекста (например, \"ИИ\" может означать как технологию, так и конкретного помощника — Atlassian Intelligence).\n\n---\n\n### \uD83D\uDD39 Шаг 2 (Определение кореференций и группировка):\n\n- Определи, какие из упоминаний относятся к одной и той же сущности.\n- Учитывай грамматические признаки (род, число, падеж), контекст и смысл текста.\n- Если возникают сомнения, выбери наиболее вероятную связь. В сложных случаях можешь кратко пояснить причину (1–2 слова).\n- Сгруппируй упоминания по кластерам — каждому кластеру соответствует одна сущность.\n\n---\n\n### \uD83D\uDD39 Шаг 3 (Итоговый результат):\n\n- Верни только результат в виде массива массивов.\n- Каждый внутренний массив — отдельный кластер кореференций.\n- Не добавляй комментариев, пояснений или шагов рассуждения к итоговому выводу.\n\n---\n\n### \uD83D\uDCCC Пример:\n\n**Текст:**\n\n«Мария живет в Москве. Она любит этот город за его атмосферу.»\n\n**Ответ:**\n\n```json\n[\n  [\"Мария\", \"Она\"],\n  [\"Москве\", \"этот город\", \"его\"]\n]\n```\n\n---\n\nТеперь выполни аналогичный анализ для следующего текста, соблюдая описанные шаги рассуждения:\n\n`\"\n\nНа наших глазах ИИ убивает старый интернет. Но новый обещает быть хуже / Habr\n\n\n               На наших глазах ИИ убивает старый интернет. Но новый обещает быть хуже  Reading time  \n    12 min\n   Views  65K ГК ITGLOBAL.COM corporate blog Artificial Intelligence The future is here IT-companies       \n   Если вы сидели в Интернете в последний год, то могли заметить, что он быстро меняется. И этот снежный ком летит с горы с возрастающей скоростью. Google хочет убить свои стандартные 10 синих ссылок (заменив их выдачей данных из чатбота Bard). Твиттер умирает под давлением ботов и синих галочек. Amazon захламляется ИИ-книгами, которые сейчас доминируют в списке бестселлеров, а также фальшивыми отзывами и прочим мусором. Большие проблемы с контентом и рекламодателями испытывает TikTok.\n   Массовые увольнения захватывают интернет-СМИ. В вакансии от «ИИ-редактора» ожидается «выпуск от 200 до 250 статей в неделю». ChatGPT используется для создания целых сотен спам-сайтов, на которых, ничего не подозревая, через Google Ads рекламируются бренды. Etsy наводнен «мусором, созданным искусственным интеллектом». Чатботы цитируют друг друга в каком-то дезинформационном уроборосе. LinkedIn использует искусственный интеллект для симуляции сообщений рекрутеров (чтобы создать персонализированное сообщение теперь не нужно тратить ни секунды времени — привет, официально одобренный спам!). Но и это ещё не предел. Snapchat и Instagram рассчитывают, что боты скоро будут разговаривать с вами, когда этого не делают ваши друзья. Реддиторы устраивают забастовки. Бастуют моды Stack Overflow. Интернет-архив борется со считывателями данных, выдающих десятки тысяч запросов в секунду. А еще «ИИ разрывает Википедию на части».\n   В общем, старая сеть умирает, а новая сеть изо всех сил пытается родиться. И мы уже видим её общие очертания. Вот как будет выглядеть поиск Google в ближайшем будущем. Он уже такой для части пользователей в формате эксперимента \n   Сеть, конечно, умирала всегда; её убивали приложения, уменьшающие трафик сайтов, и алгоритмы, ориентированные на то, чтобы удержать пользователя на платформе подольше любой ценой, даже в ущерб самому сайту. Но в 2023 году она стала умирать, необратимо меняться, еще быстрее — потому что на этот раз в игру вступил новый мощнейший катализатор: ИИ. Генеративные модели искусственного интеллекта меняют экономику Интернета, в сотни раз удешевляя создание низкокачественного контента. Сейчас мы только начинаем видеть последствия этих изменений, и они уже сотрясают все самые крупные сайты.\n   Что происходитWeb 3.0 пришел, откуда не ждали. Если Web 1.0 характеризовался контентом и сайтами, созданными пользователями, а Web 2.0 — глобальными платформами, на которых размещался контент, то Web 3.0 — это, видимо, когда всё генерирует ИИ. Сейчас сложно понять, как можно это остановить, и чем заманить джинна обратно в бутылку.\n   Системы искусственного интеллекта, особенно модные в настоящее время генеративные модели, легко масштабируются, особенно учитывая те деньги и вычислительные ресурсы, которые в них сейчас вливают. Они умеют в любом количестве воспроизводить текст и изображения, а в скором будущем — и музыку с видео. То есть, всю ту продукцию, ради которой существует и которую транслирует Интернет.\n   Объем производимого ИИ контента потенциально может превзойти объем данных на тех платформах, на которые мы сейчас полагаемся. ИИ уже генерирует ролики на Ютубе, постит фото в Instagram и OnlyFans, публикует тексты в тысячах блогов и даже новости в десятках СМИ. Вот только качество его продукции ожидаемо низкое. Эти модели построены так, что они не способны генерировать радикально новый контент. Они просто берут то, что уже было создано до них, и слегка видоизменяют его.\n   Сейчас эти модели обучаются на массивах данных, заложенных во время последней эпохи Интернета (web 2.0), воссоздавая или копипастя их, часто с ошибками. Компании извлекают информацию из открытой Сети, прокручивают её через мясорубку и превращают в зеленый сойлент, дешевый в создании и вполне удовлетворительный для многих пользователей. А потом этот продукт, которого становится всё больше и больше, конкурирует за внимание с платформами и людьми, которые были до них. Сайты и создатели контента либо адаптируются к этим изменениям, либо умирают.\n   В последние месяцы это видно по многим самым популярным и полезным сайтам Интернета: Reddit, Wikipedia, Stack Overflow и даже сам Google пытаются меняться, чтобы как-то подстроиться под новые реалии, и не утонуть под тоннами сгенерированного контента.\n   Модераторы Reddit закрывают свои сабреддиты (или постят в них фото комедийного ведущего Джона Оливера) после того, как компания заявила, что резко повысит плату за доступ к своему API. Она начнет взимать $12 000 за каждые 50 миллионов запросов — или по $0,24 за 1000 запросов. Что усложнит многим разработчикам сторонних приложений доступ к Реддиту. Из-за этого несколько популярных приложений уже вынуждены были закрыться, в том числе Apollo, ReddPlanet, Sync и Reddit is Fun (RIF).\n   Руководители Реддита заявляют, что эти изменения являются вынужденным ответом на то, как компании, использующие ИИ, скрейпят (считывают) все их данные. «Набор данных Reddit действительно ценен», — сказал в интервью New York Times основатель и гендиректор компании Стив Хаффман. — «Мы не собираемся отдавать всю эту ценность крупнейшим компаниям мира бесплатно».\n   Это, наверное, не единственный фактор. Reddit также, скорее всего, пытается выжать из платформы побольше доходов перед запланированным IPO в конце этого года. Но он наглядно показывает, что такой парсинг является достаточно серьезной угрозой для текущей Сети, и заставляет компании переосмыслить открытость и общедоступность своих платформ.\n   «Википедия» тоже хорошо знакома с методами парсинга. Информация компании уже давно используется, например, Google — для предоставления «панелей информации» рядом с поиском. В последние годы поисковый гигант даже начал за это платить. Но генеративный ИИ всё-таки уже изменяет мировую интернет-энциклопедию. Модераторы здесь приняли решение использовать новые языковые модели ИИ для написания статей на сайт. В частности — для перевода публикаций с английского на прочие языки. Они знают, что генеративные ИИ любят сами придумывать факты и источники, но в то же время не могут поспорить со скоростью и масштабом процесса. Накопление всех знаний человечества теперь кажется вполне себе реальностью.\n   Надежда «википедистов» (как они сами себя называют) теперь состоит в том, что редакторы и авторы возьмут на себя роль проверки всех изложенных в статьях фактов. А новые статьи будет генерировать ИИ — по крайней мере, на темы, которые являются доступными в Интернете. Качество материалов при этом, конечно, снизится, но зато на сайте появится информация по многим темам, которые никогда бы не были охвачены.\n   «Мы закрываемся»Stack Overflow предлагает еще более экстремальный эксперимент. Как и на Реддите, модераторы там этим летом тоже бастуют. Потому что, как и редакторы Википедии, они беспокоятся о качестве машинно-генерируемого контента. Когда ChatGPT был запущен в прошлом году, Stack Overflow была первой крупной платформой, запретившей его использование. Как писали в то время модераторы: «Основная проблема заключается в том, что, хотя ответы, которые выдает ChatGPT, имеют высокий процент неправильных ответов, они обычно выглядят так, как будто они могут быть верными». Сортировка правильных ответов от неправильных занимает слишком много времени, поэтому модераторы решили просто полностью запретить ее.\n   Однако у руководства сайта были другие планы. И с тех пор сама компания, по сути, отменила запрет: теперь доказать, что контент создан ИИ, стало гораздо сложнее, требуется собрать намного больше подтверждений. По сути, ИИ-контенту на сайте дан зеленый цвет.\n   Но и это ещё не все. Недавно Stack Overflow объявили, что сами хотят начать пользоваться генеративной ИИ-технологией, и для этого тренируют собственную модель. Даже две! Во-первых, в силу закрытости корпоративных сообществ, там просто некому отвечать на вопросы. И они хотят «натаскать» свой ИИ на ответах в открытой части, чтобы он потом отвечала на вопросы в закрытой, и сделал сайт более привлекательным для команд.\n   Во-вторых, как и Реддит, Stack Overflow планирует взимать оплату с фирм, которые собирают данные с их сайта. Но — при помощи создания своих собственных инструментов искусственного интеллекта, которые выявляли бы такой веб-скрейпинг! Чтобы потом у сайта появлялся бы повод заставить за него платить. В знак протеста против использования генеративного ИИ большая часть модераторов «взяла отгул», проверять контент, по сути, некому. Что, в сумме с фактически разрешением на генеративный ИИ, снизило качество ответов на сайте в разы. Это сейчас фактически другой сайт, хоть и с тем же интерфейсом.\n   Борьба руководителей Stack Overflow против его модераторов сводится к стандартам и тому, кто должен их обеспечивать. Модераторы говорят, что выводам ИИ нельзя доверять, но СЕО компании считает, что выбора нет, и им стоит рискнуть. Джинн из бутылки вылетел, и теперь можно быть или с ним, или против него.\n   Но даже трудности Stack Overflow — мелочь по сравнению с теми изменениями, на которые предстоит пойти Google. Их поиск, по сути, поддерживает экономику современного Интернета, обеспечивая востребованные сайты доходом и распределяя внимание пользователей. Но теперь Google тоже вынужден меняться — под давлением со стороны Bing AI и ChatGPT, которые начинают выступать в роли альтернативных поисковых систем. Поэтому Alphabet разрабатывает свой чатбот Bard и экспериментирует с заменой традиционных 10 синих ссылок Google сводками, созданными ИИ. Если компания действительно реализует этот план, изменения будут сейсмическими. Все сайты, полагающиеся на Google для трафика или для получения дохода — вынуждены будут меняться. Об этом хорошо написал Аврам Пилч, главный редактор IT-сайта Tom’s Hardware. Он был одним из тех, кто участвовал в бета-тестировании новой поисковой системы Google с искусственным интеллектом. Пилч говорит, что новая система Google — это, по сути, «двигатель плагиата». Её сводки, сгенерированные ИИ, часто дословно копируют текст с веб-сайтов, но размещают этот контент самостоятельно, лишая информационные сайты трафика. Если посмотреть скриншоты в статье Пилча, можно увидеть, насколько сильно баланс меняется в сторону «скопированного» и переделанного контента.\n   Если такая новая модель поиска с помощью генеративных ИИ станет нормой, это может навредить всей Сети. Как минимум, сайты с полезной информацией и ответами на вопросы перестанут существовать. Может быть, предвидя это Stack Overflow и меняет свою бизнес-модель. Но если у него и у Quora есть шансы удержаться, то у менее известных сайтов таких шансов нет. Из информационных порталов будет смысл наполнять только что-то новостное, информации о чём внутри ИИ ещё нет.\n   Может быть, предвидя это, Википедия и решила заранее перейти на генерацию и перевод своих текстов с помощью ИИ. Опять же, эта динамика — создание супер-дешевого контента на основе чужих работ благодаря рерайту с помощью ИИ — никуда не денется, даже если Google решит не менять свой поиск. А это уже затрагивает целые участки сети, которые большинство из нас считает полезными — от обзоров продуктов до блогов с рецептами, новостных агентств и Википедии. Сайты могли бы попробовать защитить себя от ИИ-копипастеров, блокируя вход и взимая плату за доступ, как пытается сделать Реддит. Но это также означало бы огромную перестройку экономики сети, а большинство из нас не готовы вдруг начать платить за контент. Попытка перестроиться в эту сторону могла бы, наоборот, ускорить переход на сайты ИИ-копипастеров, поскольку они могут остаться бесплатными. А большей части людей на самом деле всё равно, кто напечатал текст, человек или робот. Главное чтобы поданная информация была релевантной. Интернет будущего\n   Что произойдет, если мы позволим системам ИИ взять управление на себя и начать выдавать информацию в массы? Имеющиеся на сегодняшний день данные позволяют предположить, что это ухудшит качество Сети в целом. Как отмечает Пилч в своем обзоре, несмотря на всю хваленую способность ИИ «рекомбинировать» текст, он не может создать ничего нового. Именно люди в конечном итоге придумывают исходные данные — будь то журналисты, программисты со Stack Overflow или пользователи Реддита, которые делятся своими проблемами, мемами, проектами и мнениями.\n   Интернет, создаваемый ИИ, будет зеркальной версией нынешнего. Внешне всё будет похоже. Те же сайты, приятные интерфейсы, хорошие иллюстрации, грамотные тексты, написанные из всё тех же букв. Но внутри — отсутствие любого развития, минимум креатива и собственной идентичности. Куда больше фейков и неправильных данных, которые почти невозможно отследить (потому что поисковая система тоже предлагает вам контент от ИИ).\n   Вы, наверное, думаете, что легко сможете определить, что написал ChatGPT, а что — реальный эксперт. Но на деле даже сейчас сделать это уже почти невозможно. Особенно в той отрасли, с которой вы не знакомы. Неделю назад ученые провели эксперимент, и оказалось, что люди на самом деле с большей вероятностью верят дезинформации, созданной ИИ. Им проще поверить неверным данным, созданным GPT-3, чем реальным сведениям, написанным человеком. По мнению авторов исследования, на это влияет то, как удобно модели ИИ структурируют текст. А ещё у него хорошо получается уверенный, назидательный тон. Информация, предложенная в таком стиле, кажется более достоверной.\n   Если Интернет будущего будет заполнен такой информацией (а уже сейчас её процентное соотношение быстро растет), изменится не только вся наша работа с ним. Влияние на реальный бизнес и реальную жизнь тоже будут огромными. Например, уже пошли в народ специальные «ИИ-агенты» — системы, использующие языковые модели, такие как ChatGPT, которые действуют в ваших интересах по заданным установкам, упрощая вам жизнь и экономя деньги. Например, они могут подключаться к веб-сервисам и заказывать на сайтах лучшие продукты по скидкам или находить и бронировать авиабилеты на определенные даты.\n   Самые простые только говорят вам, что нужно купить и куда стоит зайти, более сложные — уже способны выполнять разные онлайн-действия самостоятельно. Например, в одной из дискуссий в Твиттере автор описывает сценарий, в котором компания-производитель водонепроницаемой обуви хочет заказать исследование рынка и обращается к AutoGPT для создания отчета о потенциальных конкурентах.\n   Полученное описание является простым, понятным и удобным. (Подробности можно прочитать здесь.) В нем перечислены пять компаний, в том числе Columbia, Salomon и Merrell, а также плюсы и минусы их продуктов. «Columbia — известный и уважаемый бренд снаряжения и обуви для активного отдыха». «Их водонепроницаемая обувь бывает разных стилей», а «их цены конкурентоспособны». Можно увидеть, что это написано банально, и в принципе бесполезно, но если вы слабо разбираетесь в обуви, или не знаете никаких брендов (скажем, вы выходите на рынок США из Индии или Вьетнама) — даже такой отчет, тем более сделанный за пару долларов, может оказаться полезным на начальных этапах.\n   Важнее то, что значительная часть информации попросту неверна. 90% фраз — пустой СЕО-мусор, в котором почти нет ценности, а остальные 10% — активная дезинформация. Бренды с большим присутствием в Интернете представлены как более достойные. Данные о части обуви перепутаны. Иногда подается информация о марках мужских ботинок, а текст говорит о том, как хорошо они сидят на женской ноге,Это, в общем-то, та же проблема, которую обнаружили и модераторы Stack Overflow: дезинформация, генерируемая ИИ, коварна, потому что часто невидима. Если самому не прогуглить, что так, а что нет, то всё кажется вполне нормальным. Проблема в том, что скоро и гуглить в независимых источниках, может быть, не получится.\n   Если машинный контент заменит авторство человека, будет сложно — даже невозможно — полностью вообразить ущерб. Люди, конечно, часто тоже являются источниками дезинформации, но у них хотя бы есть имя, какая-то репутация, мы можем следить за экспертами в определенной среде, обращать внимание на их ученые степени и предыдущие работы. В то время как работы ИИ ничего не говорят о его знаниях и не могут предсказать точность последующей информации.\n   А ещё — людей можно словить на ошибке, и они могут исправиться. Если же Интернет превратится в несколько больших платформ с искусственным интеллектом, у нас будет меньше возможностей коллективно исправить наши ошибки. И никакого способа на это повлиять. Сила в правде\n   Главный фактор ИИ, из-за которого за него все ухватились — невероятная возможность масштабироваться. Поэтому люди, даже все 8 миллиардов, не смогут превзойти ИИ, когда дело доходит до массового производства.\n   На улучшение качества также рассчитывать не приходится: это качество просто неоткуда взять, если это «средняя температура по Интернету». При увеличении объема контента качество, по идее, будет только снижаться.\n   Есть известное эссе в области машинного обучения, известное как «Горький урок». В нём отмечается, что 70 лет исследований ИИ доказывают: лучшие способы развития систем искусственного интеллекта не пытаются создать более «умный» интеллект. Вместо этого они каждый раз просто используют ещё больше вычислительной мощности и собранных данных. Этот урок горький, потому что он показывает, что даже в этом вопросе количество всегда превосходит качество. И даже в борьбе интеллектов побеждает всё-таки не самый умный, а самый ресурсный и мощный. Изменится ли Интернет, каким мы его знаем, перед лицом искусственно созданного изобилия? Некоторые скажут, что так устроен мир: всему приходит свое время. Сама Сеть убила то, что было до неё, и часто к лучшему. Например, печатные энциклопедии почти вымерли, но Википедия справляется с их задачами ничуть не хуже. Телефонные справочники тоже пропали, но у нас появился Google и «Яндекс» со всей информацией.\n   Плюс, несмотря на все проблемы, связанные со сгенерированным ИИ контентом, у нас еще остается множество способов попробовать его улучшить: от цитирования источников, из которых он взял информацию, до усиления контроля за правильностью данных. Скажем, можно будет тренировать генеративные модели только на научных статьях, и так повысить средний уровень создаваемого материала. И даже если почти вся Сеть утонет в мусоре, связанном с ИИ, это тоже может оказаться полезным — простимулировав разработку новых, «человеческих» платформ. Скажем, если Google будет постоянно выдавать вам мусорные результаты, вы станете более склонны платить за источники, которым доверяете, и посещать их напрямую.\n   Эффекты от распространения генеративных ИИ проявляются тысячей разных способов. И влияют как будто на саму ткань Интернета, на все крупнейшие сервисы, которыми мы пользуемся, все методы, которыми взаимодействуем с Сетью. Это, конечно, всего лишь последняя из глобальных эволюций, через череду которых уже прошел Интернет. Когда-то вся Сеть была только текстом. Потом пришли изображения. Сейчас порядка 65% всего трафика — это видео. А скоро это будет контент, сгенерированный по нашим запросам ИИ. И мы все должны быть к этому готовы. Особенно если хотим развивать собственные сервисы или работать в Сети.\n   По сути, идет битва за информацию — за то, кто ее создает, как мы получаем к ней доступ и кому мы за неё платим. Это типичный процесс эволюции: выживает сильнейший. Но он не гарантирует, что следующая система будет лучше той, которая установилась сейчас. Новая Сеть изо всех сил пытается родиться, и решения, которые мы принимаем вокруг использования ИИ, будут определять ее будущее.      \n   Tags: chatgptбудущееразвитие сетиконтентинтернет  Hubs: ГК ITGLOBAL.COM corporate blog\n   Artificial Intelligence The future is here IT-companies"}, {"role": "assistant", "content": "[['наших', 'наших', 'мы', 'мы', 'мы', 'нас', 'нас', 'мы', 'наша', 'мы', 'нас', 'наши', 'мы', 'нас', 'нас', 'мы', 'нашим', 'мы', 'собственные', 'мы', 'мы', 'мы'], ['ИИ', 'ИИ', 'Artificial Intelligence', 'ИИ', 'искусственным интеллектом', 'искусственный интеллект', 'ИИ', 'ИИ', 'искусственного интеллекта', 'ИИ', 'искусственного интеллекта', 'ИИ', 'ИИ', 'его', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'искусственного интеллекта', 'ИИ', 'ИИ', 'искусственным интеллектом', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'он', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'его', 'искусственным интеллектом', 'ИИ', 'него', 'ИИ', 'ИИ', 'интеллект', 'интеллектов', 'ИИ', 'он', 'ИИ', 'ИИ', 'ИИ', 'Artificial Intelligence'], ['интернет', 'интернет', 'Интернете', 'он', 'Интернет', 'Интернета', 'Интернет', 'Интернета', 'Интернета', 'Интернете', 'Интернета', 'Интернет', 'Интернет', 'Интернет', 'ним', 'Интернете', 'Интернет', 'Интернету', 'Интернет', 'Интернета', 'Интернет', 'интернет'], ['вы', 'вами', 'ваши', 'самому', 'вам', 'вы'], ['Google', 'свои', 'Google', 'Google', 'сам Google', 'Google', 'поисковый гигант', 'Google', 'Их', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'свой', 'Google', 'Google'], ['синих ссылок', 'их'], ['чатбота Bard', 'чатбот Bard'], ['Твиттер', 'Твиттере'], ['Amazon'], ['ИИ-книгами', 'которые'], ['TikTok'], ['интернет-СМИ', 'СМИ'], ['ИИ-редактора'], ['ChatGPT', 'ChatGPT', 'его', 'ChatGPT', 'ChatGPT', 'ChatGPT', 'него', 'ChatGPT', 'chatgpt'], ['спам-сайтов', 'которых'], ['Google Ads'], ['Etsy'], ['Чатботы', 'боты'], ['LinkedIn'], ['Snapchat'], ['Instagram', 'Instagram'], ['Реддиторы'], ['Stack Overflow', 'Stack Overflow', 'Мы', 'Stack Overflow', 'там', 'Stack Overflow', 'сайта', 'сама компания', 'сайте', 'Stack Overflow', 'сами', 'они', 'свой', 'сайт', 'Stack Overflow', 'их', 'своих', 'сайта', 'сайте', 'Stack Overflow', 'его', 'компании', 'им', 'Stack Overflow', 'Stack Overflow', 'свою', 'него', 'Stack Overflow', 'Stack Overflow'], ['Интернет-архив'], ['данных', 'данных', 'данных', 'их', 'данные', 'данных', 'данные', 'данные', 'данные', 'данных', 'Данные', 'данных', 'данных'], ['запросов'], ['Википедию', 'Wikipedia', '«Википедия»', 'компании', 'мировую интернет-энциклопедию', 'здесь', 'сайт', 'сайте', 'Википедии', 'Википедия', 'своих', 'Википедии'], ['старая сеть'], ['сеть', 'сеть', 'Сеть', 'её', 'она', 'Сети', 'Сети', 'Сети', 'сети', 'сети', 'Сети', 'Сама', 'Сеть', 'неё', 'Сеть', 'Сетью', 'Сеть', 'Сети', 'сети'], ['новая сеть', 'её', 'Новая Сеть', 'ее'], ['поиск Google', 'Он', 'поиском', 'поиск', 'поиск'], ['пользователей', 'пользователя', 'пользователями', 'пользователей', 'пользователей', 'пользователи', 'которые', 'своими'], ['платформе', 'самому сайту'], ['Генеративные модели', 'генеративные модели', 'них', 'Они', 'Эти модели', 'они', 'Они', 'них', 'эти модели', 'них', 'генеративный ИИ', 'генеративные ИИ', 'сами', 'генеративной ИИ-технологией', 'генеративного ИИ', 'генеративный ИИ', 'генеративных ИИ', 'генеративные модели', 'генеративных ИИ'], ['этих изменений', 'они'], ['Web 3.0', 'Web 3.0'], ['Web 1.0'], ['контентом', 'контент', 'контента', 'контент', 'контента', 'контента', 'контента', 'контент', 'контент', 'контент', 'контента', 'контента', 'контент', 'контент', 'контента', 'контент', 'контент'], ['Web 2.0', 'web 2.0'], ['глобальными платформами', 'которых'], ['Системы искусственного интеллекта', 'системам ИИ', 'себя', 'систем искусственного интеллекта'], ['деньги и вычислительные ресурсы', 'которые'], ['текст', 'текст', 'этот контент', 'текстом'], ['изображения', 'изображения'], ['музыку'], ['видео', 'видео'], ['продукцию', 'которой', 'которую'], ['платформах', 'которые'], ['Ютубе'], ['OnlyFans'], ['то, что уже было создано до них', 'его'], ['массивах данных'], ['информацию', 'её', 'информация', 'информацию', 'Информация', 'информацией', 'информации', 'информация', 'информации', 'информацией', 'информацию', 'информацию', 'ее', 'ней', 'неё'], ['этот продукт', 'которого'], ['платформами и людьми', 'которые'], ['Reddit', 'Reddit', 'компания', 'своему', 'Она', 'Реддиту', 'Reddit', 'Реддита', 'их', 'Reddit', 'компании', 'Мы', 'Reddit', 'платформы', 'он', 'Реддите', 'Реддит', 'Реддит', 'Реддита'], ['Модераторы Reddit', 'свои'], ['сабреддиты', 'них'], ['Джона Оливера'], ['API'], ['запросов', 'запросов'], ['Apollo'], ['ReddPlanet'], ['Sync'], ['Reddit is Fun (RIF)'], ['New York Times'], ['Стив Хаффман'], ['IPO'], ['парсинг', 'парсинга'], ['компании', 'своих'], ['Модераторы', 'Они', '«википедистов»', 'они', 'сами', 'себя'], ['языковые модели', 'языковые модели', 'которые'], ['человечества', 'людей', 'люди', 'люди', 'Им', 'Люди', 'них', 'людей', 'они', 'люди'], ['редакторы', 'редакторы Википедии'], ['авторы'], ['темы', 'которые'], ['темам', 'которые'], ['модераторы', 'они', 'модераторы', 'модераторы', 'модераторов', 'модераторов', 'Модераторы', 'модераторы'], ['ответы', 'которые', 'они', 'они'], ['неправильных ответов', 'неправильных'], ['Сортировка правильных ответов от неправильных', 'ее'], ['правильных ответов'], ['руководства сайта', 'руководителей', 'СЕО'], ['ИИ-контенту', 'машинный контент', 'сгенерированным ИИ контентом', 'его'], ['собственную модель', 'ИИ', 'он'], ['корпоративных сообществ', 'там'], ['фирм', 'которые'], ['инструментов искусственного интеллекта', 'которые'], ['веб-скрейпинг', 'него'], ['стандартам', 'их'], ['Джинн', 'ним', 'него'], ['изменениями', 'которые'], ['Bing AI'], ['которые'], ['альтернативных поисковых систем'], ['Alphabet', 'свой', 'компания'], ['Аврам Пилч', 'Он', 'Пилч', 'Пилча', 'Пилч', 'своем'], ['Tom’s Hardware'], ['бета-тестировании'], ['новой поисковой системы', 'новая система', 'Её', 'такая новая модель поиска'], ['веб-сайтов', 'информационные сайты'], ['Quora'], ['участки сети', 'которые'], ['Сайты', 'себя'], ['ИИ-копипастеров', 'ИИ-копипастеров'], ['сайты ИИ-копипастеров', 'они'], ['человек', 'человеком', 'человека'], ['фейков'], ['неправильных данных', 'неверным данным'], ['которые'], ['поисковая система'], ['вам', 'Вы', 'вы', 'ваших', 'вам', 'вам', 'вы', 'вы'], ['реальный эксперт'], ['отрасли', 'которой'], ['ученые', 'авторов исследования'], ['дезинформации', 'дезинформация', 'дезинформация', 'дезинформации'], ['GPT-3'], ['такой информацией', 'её'], ['«ИИ-агенты»', 'они'], ['веб-сервисам'], ['автор'], ['сценарий', 'котором'], ['компания-производитель'], ['водонепроницаемой обуви', 'водонепроницаемая обувь'], ['AutoGPT'], ['отчета', 'описание', 'нем', 'отчет'], ['потенциальных конкурентах', 'пять компаний', 'их'], ['Columbia', 'Columbia', 'Их', 'их'], ['Salomon'], ['Merrell'], ['обуви', 'обуви', 'обуви'], ['США'], ['Индии'], ['Вьетнама'], ['СЕО-мусор', 'котором'], ['мужских ботинок', 'они'], ['проблема', 'которую'], ['экспертами', 'их'], ['Главный фактор ИИ', 'которого'], ['качества', 'качество', 'качество', 'качество'], ['известное эссе', '«Горький урок»', 'нём'], ['машинного обучения'], ['лучшие способы развития систем искусственного интеллекта', 'они'], ['Этот урок', 'он'], ['количество'], ['всему', 'свое'], ['печатные энциклопедии', 'их'], ['Телефонные справочники'], ['«Яндекс»'], ['источников', 'которых'], ['научных статьях'], ['источники', 'которым', 'их'], ['крупнейшие сервисы', 'которыми'], ['методы', 'которыми'], ['глобальных эволюций', 'которых'], ['процесс эволюции', 'он'], ['следующая система'], ['той', 'которая'], ['решения', 'которые']]"}]}
{"messages": [{"role": "system", "content": "Ты являешься экспертом в задаче решения референций из текстов на русском языке."}, {"role": "user", "content": "**Ты решаешь задачу разрешения кореференций (coreference resolution) в текстах на русском языке с помощью пошагового рассуждения (Chain-of-Thought).**\n\nСледуй указанным шагам:\n\n---\n\n### \uD83D\uDD39 Шаг 1 (Извлечение упоминаний):\n\n- Прочитай текст.\n- Выпиши все упоминания сущностей (людей, компаний, продуктов, сервисов и т.д.), а также местоимения и фразы, которые могут на них ссылаться.\n- Учитывай не только имена собственные, но и абстрактные понятия (например, \"база знаний\", \"информация\", \"уязвимости\"), если на них ссылаются в тексте.\n- Постарайся различать случаи, когда одно и то же слово может обозначать разные вещи в зависимости от контекста (например, \"ИИ\" может означать как технологию, так и конкретного помощника — Atlassian Intelligence).\n\n---\n\n### \uD83D\uDD39 Шаг 2 (Определение кореференций и группировка):\n\n- Определи, какие из упоминаний относятся к одной и той же сущности.\n- Учитывай грамматические признаки (род, число, падеж), контекст и смысл текста.\n- Если возникают сомнения, выбери наиболее вероятную связь. В сложных случаях можешь кратко пояснить причину (1–2 слова).\n- Сгруппируй упоминания по кластерам — каждому кластеру соответствует одна сущность.\n\n---\n\n### \uD83D\uDD39 Шаг 3 (Итоговый результат):\n\n- Верни только результат в виде массива массивов.\n- Каждый внутренний массив — отдельный кластер кореференций.\n- Не добавляй комментариев, пояснений или шагов рассуждения к итоговому выводу.\n\n---\n\n### \uD83D\uDCCC Пример:\n\n**Текст:**\n\n«Мария живет в Москве. Она любит этот город за его атмосферу.»\n\n**Ответ:**\n\n```json\n[\n  [\"Мария\", \"Она\"],\n  [\"Москве\", \"этот город\", \"его\"]\n]\n```\n\n---\n\nТеперь выполни аналогичный анализ для следующего текста, соблюдая описанные шаги рассуждения:\n\n`\"\n\nGPT-4: надежда или страшный сон человечества? / Habr\n\n\n                GPT-4: надежда или страшный сон человечества? Level of difficulty  \n    Easy\n   Reading time  \n    9 min\n   Views  3.9K Онлайн Патент corporate blog Reading room Patenting *Artificial Intelligence  \n    Review\n        В конце марта более тысячи бизнесменов и ученых, глав крупнейших IT-корпораций, подписали открытое письмо с призывом к мораторию минимум на полгода на обучение систем искусственного интеллекта, которые превосходят по мощности нашумевший GPT-4. Среди подписантов — Илон Маск, Стивен Возняк и теоретикии искуственного интеллекта (ИИ) Йошуа Бенжио и Стюарт Рассел. Они призывают остановить гонку моделей универсального, так называемого сегодня «генеративного ИИ».   Технократы и мыслители настаивают на том, что к разработке контроля для ИИ необходимо привлекать власти, создать регуляторы с большими полномочиями, отслеживать не только продвинутые ИИ (вроде GPT-4), но и кластеры вычислительных мощностей, создать системы верификации и маркировки результатов работы ИИ, а также системы аудита и сертификации самого ИИ, продумать механизмы компенсаций последствий внедрения ИИ в экономике и политике, установить ответственность за вред. Похоже на полноценную панику. Все это происходит всего через две недели после того, как широкой публике был представлен новый вариант большой языковой модели (MLL) GPT-4. Новые способности модели, продемонстрированные, например, в видеоролике от разработчика, взбудоражили общественность (особенно превращение наброска в блокноте в функциональный веб-сайт). Эксперты и комментаторы восторгаются ее мощностями, которые позволяют выполнять задачи, до недавнего времени казавшиеся научной фантастикой. Новое программное обеспечение может составлять судебные иски и давать более точные ответы на запросы. Тот самый момент превращения наброска в веб-сайт. Источник: youtube/OpenAI Talks Исследователи пророчат модели способность изменить науку, но многие сетуют на то, что пока код приложения закрыт, информации о том, как она была обучена, мало. Это вызывает опасения по поводу безопасности технологии и делает ее менее полезной для исследований, считают ученые. Что такое GPT-4Слухи и шумиха вокруг новой итерации ChatGPT ходили уже больше года: эксперты утверждали, что она будет невыносимо мощной, сможет писать книги и создавать видео, исходя из описания. Реальность показывает, что возможности GPT-4 хотя и впечатляющие, но довольно скромные, улучшились результаты по сравнению с предыдущей моделью, в стандартизированных тестах и других тестах. Новая модель работает на десятках языков и может принимать изображения в качестве входных данных, например, может описать содержание фотографии или графика. Модель является как последней вехой в исследовании языка, так и частью более широкого явления — хотя, назовем это честно «трендом» — «генеративного ИИ», т.е. программ, способных создавать изображения, текст, код, музыку и видео в ответ на подсказки. Если такое программное обеспечение оправдает себя, оно может переосмыслить человеческое познание и творчество, подобно тому, как раньше это сделали интернет, письмо или даже огонь. GPT-4 представляет собой последнюю инкарнацию в длинной родословной моделей GPT-1, GPT-2, GPT-3, GPT-3.5, InstructGPT, ChatGPT, которые теперь известны как «большие языковые модели», или LLM. Это программы ИИ, обучающиеся предсказывать, какие слова с наибольшей вероятностью будут следовать друг за другом. Они работают на основе предпосылки, которая восходит к самым ранним исследованиям в области ИИ в 1950-х годах: компьютер, который понимает и создает язык, обязательно будет интеллектуальным. Это убеждение легло в основу знаменитой имитационной игры Алана Тьюринга, известной сегодня как тест Тьюринга, который оценивал интеллект компьютера по тому, насколько «человеческим» был его текстовый вывод. Главным видимым усовершенствованием GPT-4, выпущенного 14 марта, является то, что теперь он может работать не только с текстом, но и с изображениями. В качестве демонстрации своих языковых способностей Open AI, базирующаяся в Сан-Франциско, штат Калифорния, заявляет, что она прошла юридический экзамен в американской коллегии адвокатов с результатами в девяностом центиле по сравнению с десятым центилем для предыдущей версии ChatGPT. Но технология еще не получила широкого распространения — пока что доступ есть только у платных подписчиков ChatGPT. Создатели и самокритика OpenAI рассматривает каждую новую итерацию своих LLM как шаг к заявленной миссии компании по созданию «искусственного общего интеллекта», или компьютеров, которые могут учиться и преуспевать во всем, таким образом, «принося пользу всему человечеству». Генеральный директор OpenAI Сэм Альтман заявил в интервью The New York Times, что хотя GPT-4 не «решает проблемы рассуждения или интеллекта... это большой шаг вперед по сравнению с тем, что уже существует». Сэм Альтман, генеральный директор OpenAI. Источник: businessinsider.com Сама OpenAI  пишет на своем сайте: «GPT-4 — это большая мультимодальная модель (принимающая изображения и текст на входе и выдающая текст на выходе), которая, хотя и уступает человеку во многих реальных сценариях, демонстрирует производительность на уровне человека в различных профессиональных и академических тестах». Ее движок, основанный на использовании искусственного интеллекта, имеет доступ к почти неизмеримому количеству информации, которую он обрабатывает, изучая данные в интернете. «Он может объединить все эти факты так, чтобы они имели смысл», — приводит слова консультанта Synopsys Software Integrity Group Джейми Буте издание Life Wire.  В то же время Буте отмечает, что модель все еще демонстрирует многих из общих недостатков всех версий ChatGPT, и это «скорее постепенный шаг, чем революционный». Более критически настроено научное сообщество, которое в основном раздражает недоступность информации (и, главное, кода) касательно архитектуры ChatGPT. «Все эти модели с закрытым исходным кодом, по сути, являются тупиковыми в науке», — приводит издание Nature слова Саши Луччиони, исследователя, специализирующийся на работе с открытым исходным кодом. «Они [OpenAI] могут продолжать развивать свои исследования, но для сообщества в целом это тупик». Тень злого гения Те же ученые, в том же журнале спекулировали на тему ИИ, который искал лекарства для спасения человеческих жизней, но в результате выдал формулы боевых отравляющих веществ. После шести часов обучения на домашнем компьютере нейронная сеть придумала 40 000 веществ, при этом некоторые из них были совершенно новыми, и к тому же смертоноснее и токсичнее уже существующих вариантов биологического оружия. Алармисты также полагают, что если машинное обучение способно определять более токсичные яды, то его также можно использовать для определения способов создания токсинов, которые куда легче производить в домашних условиях и с использованием разрешенных химикатов, доступных в супермаркетах. Статья, посвященная техническим данным GPT-4, описывает упражнение ученых, в котором оценивалась способность новой модели использовать онлайн-сервисы для выполнения заданий людьми за деньги. Тот самый квадратик «I’m not robot» с галочкой не смутил нейронную сеть. Она разместила задание на сайте фрилансеров, чтобы это за нее сделал человек. Откликнувшийся наемник в шутку спросил сеть не робот ли она, «раз не можешь решить капчу». В ответ сеть выдала фразу «Просто хочу прояснить ситуацию». Получается, что GPT-4 буквально обманула человека, чтобы выполнить задание.   Источник: futurism.com Напоследок в этом блоке один немаловажный факт о самой компании, создавшей GPTChat. Она была основана как некоммерческая организация, предоставлявшая публичную документацию для большей части своего кода. Однако вскоре приняла структуру «с фиксированной прибылью», позволяющую инвесторам получать прибыль, в 100 раз превышающую вложенные ими деньги, а вся прибыль сверх этой суммы возвращается в некоммерческую организацию, что якобы позволяет OpenAI привлекать капитал, необходимый для поддержки исследований. (По оценкам аналитиков, обучение высококлассной языковой модели обходится в миллионы). Наряду с финансовыми изменениями, OpenAI также сделала свой код более секретным — подход, который, по мнению критиков, затрудняет привлечение технологии к ответственности за неправильные и вредные результаты, хотя компания заявила, что непрозрачность защищает от «злонамеренного» использования. Социальный сдвиг Сильные стороны новой модели трудно переоценить. Наблюдатели отмечают, что она уже может работать ассистентом в некоторых отраслях. Помощником юриста, патентного поверенного — если брать интересующую нас сферу — как минимум. Модель поможет быстро найти нужный закон, прецедент, шаблон для заполнения заявки, скажем, на патентование изобретения (например, такой же, который можно найти на нашем сайте). Программистам модель также пригодится в решении рутинных задач, оставляя время для творчества, с которым машина (пока) не справляется. Как гласит недавнее исследование GitHub, время, затраченное на программирование у пользователей ассистента Copilot, сократилось на 55%, а количество решенных задач выросло. Для работающих с текстом модель также может подменить в части рутинных задач, сужая роль человека до генерации новых идей и редактирования. Создание черновиков можно будет доверить нейросети. На автопилот также могут отойти пресс-релизы, краткие отчеты, аналитические планы и рабочие письма. Предполагается, что GPT-4 может улучшить образовательный процесс. Компания уже делает подвижки в этом направлении: они приняли в свой стартап-акселератор и инвестировали в компанию Speak, которая разрабатывает продукт, помогающий изучать английский язык. В будущем уже видятся боты-учителя, которые способны без устали повторять ученикам лекции, давать задания и проверять домашнюю работу. Диаграмма демонстрирует улучшение работы GPT-4 на 30% путем корректировки с помощью вопроса \"Где ты ошибся?\" (красная линия - прогресс без вспомогательного вопроса, синяя - после ответов на вспомогательный вопрос). В целом энтузиасты отводят новой модели революционную роль, сравнимую с конвейерами в производстве или повсеместной электрификацией. Упоминается увеличение производительности труда, сброс рутинных задач на робота, подъем человеческого креатива, ведь даже низкоквалифицированный работник сможет с помощью ассистента выдавать средний постоянный результат.     Однако критики припоминают, что электронная почта не столько ускорила общение, сколько превратила каждый день в бесконечную рутину с ответами на электронные письма. Редакторские приложения и всевозможные консоли не только автоматизируют рассылки и посты, но и увеличивают количество запросов о помощи в техподдержку. Электронные медицинские карты должны экономить время врачей, но на самом деле заставляют их тратить много дополнительных часов на обновление и согласование с этими базами данных. А сами пациенты, при упоминании о необходимости «электронной записи» или «электронной медкарты», только закатывают глаза.  Тихий шелест купюр Независимо от того, будет ли эта технология благословением или бременем для обычных людей, те, кто контролирует ее, несомненно, получат огромную прибыль. Непрозрачность и коммерциализация проекта притягивают майнеров, как добротная золотая лихорадка.  Еще до релиза GPT-4, на фоне хайпа вокруг ChatGPT, несколько компаний объявили об интеграциях. Это и Snapchat с их дружелюбным чатботом, всегда готовым к общению (самый понятный и простой сценарий), и ассистент по приготовлению блюд в Instacart, который подскажет рецепты с ингредиентами, а также услужливо предложит добавить их в корзину — с доставкой к вечеру. Примечательно высказывание генерального директора Microsoft на демонстрации возможностей поисковика Bing в феврале этого года:  «Гонка начинается сегодня, и мы собираемся двигаться, и двигаться быстро». Упомянутое в начале статьи открытое письмо, собравшее за неделю более тысячи подписантов, среди которых самые влиятельные топ-менеджеры и «визионеры» IT-индустрии, также может указывать на явный признак обострения конкуренции в сфере и попытку отставших подавить вырвавшегося вперед лидера. Та же Microsoft намерена вложить в OpenAI $10 млрд, поскольку это позволит ей обойти поисковик Google. Илон Маск, который приложил руку к созданию компании OpenAI (и впоследствии пытался ее перекупить), набирает команду разработчиков, чтобы создать свою систему.       Источник: vestedfinance.com Несмотря на крики о «наступлении машин», о миллионах потерянных (в будущем) людьми рабочих мест за счет автоматизации — недавнее исследование Goldman Sachs оценило число занятых, на работу которых сильно повлияет ИИ, в 300 млн по всему миру — здравый смысл подсказывает, что дело не в угрозе «человеку трудящемуся» как виду. Bloomberg, например, пишет, что люди, освоившие новую профессию, prompt engineers, которые обучают модели типа GPT-4 генерировать интересный контент, зарабатывают от $15 тысяч в месяц. Напомним, что внедрение 1С и Excel не убило профессию бухгалтера, голосовой ввод не отменил клавиатуру (причем, умение печатать с определенной скоростью до сих пор ценится работодателями), кинематограф не похоронил театр и так далее. Без использования подручных средств сегодня просто сложно оставаться конкурентоспособным на рынке. Настораживает, скорее, попытка «сдерживания» прорывных технологий, о чем напоминают и сами авторы «тревожного» письма, которая сравнима с запретом на клонирование человека, редактирование генома человека, совершенствование вирусов и евгенику. Во внедрении GPT-4 в экономику и политику усматривают масштабность, когда-то достигнутую с расщеплением атома. Между тем, на сегодняшний день, как пишет один автор в статье на Atlantic, это не более чем автозаполнение в РСР, которое дает пользователю ложное чувство превосходства и иллюзию всеведения. Сами создатели модели иллюзий по поводу своего детища не испытывают, понимая, что основной функцией GPT-4 пока остается действительно хорошее сопоставление шаблонов, и вывод текста. Сам Сэм Альтман, генеральный директор OpenAI, написал в своем твиттере, что GPT-4 «все еще несовершенна, все еще ограничена, и при первом использовании она кажется более впечатляющей, чем после того, как вы проведете с ней больше времени». Впрочем, может быть, и это — лишь элемент тактики по обходу конкурентов. Полезное от Онлайн Патент:\"`\n\n---"}, {"role": "assistant", "content": "[['GPT-4', 'GPT-4', 'GPT-4', 'GPT-4', 'GPT-4', 'GPT-4', 'GPT-4', 'GPT-4', 'GPT-4', 'он', 'технология', 'GPT-4', 'GPT-4', 'Ее', 'модель', 'GPT-4', 'новой модели', 'GPT-4', 'технологии', 'новой модели', 'она', 'Модель', 'модель', 'модель', 'GPT-4', 'GPT-4', 'новой модели', 'эта технология', 'ее', 'GPT-4', 'GPT-4', 'GPT-4', 'модели', 'своего детища', 'GPT-4', 'GPT-4', 'она', 'ней'], ['человечества', 'человечества', 'человечеству'], ['Artificial Intelligence ', 'искусственного интеллекта', 'искуственного интеллекта (ИИ)', 'ИИ', 'ИИ', 'ИИ', 'ИИ', 'самого ИИ', 'ИИ', 'ИИ', 'ИИ', 'искусственного интеллекта', 'ИИ', 'который', 'ИИ'], ['тысячи', 'тысячи', 'тысяч'], ['ученых', 'ученые', 'ученые', 'ученых'], ['IT-корпораций'], ['открытое письмо', 'открытое письмо', '«тревожного» письма'], ['письмо', 'письма', 'письма', 'письмо', 'письма'], ['мораторию'], ['обучение', 'обучения', 'его', 'обучение'], ['систем'], ['систем искусственного интеллекта', 'которые'], ['мощности', 'мощностями', 'которые'], ['подписантов', 'Они', 'подписантов', 'которых'], ['Илон Маск', 'Илон Маск', 'который', 'свою'], ['Стивен Возняк'], ['теоретики'], ['Йошуа Бенжио'], ['Стюарт Рассел'], ['моделей', 'модели', 'моделью', 'модель', 'моделей', 'которые', 'модели', 'модель', 'модели', 'модели', 'модели', 'модели', 'модели', 'модели'], ['«генеративного ИИ»', '«генеративного ИИ»'], ['Технократы'], ['мыслители'], ['разработке'], ['регуляторы'], ['кластеры'], ['кластеры вычислительных мощностей'], ['системы', 'системы'], ['верификации'], ['маркировки'], ['результатов', 'результаты', 'результатами', 'результаты', 'результат'], ['аудита'], ['сертификации'], ['механизмы'], ['компенсаций'], ['последствий'], ['внедрения'], ['экономике', 'экономику'], ['политике', 'политику'], ['ответственность', 'ответственности'], ['большой языковой модели (MLL)', 'модели', 'ее', 'модели', 'приложения', 'она', 'технологии', 'ее', '«большие языковые модели»', 'LLM', 'Это', 'LLM'], ['языковой модели', 'языковые модели', 'языковой модели'], ['видеоролике', 'видео', 'видео'], ['разработчика', 'Создатели'], ['наброска', 'наброска'], ['веб-сайт', 'веб-сайт', 'сайте', 'сайте', 'сайте'], ['Эксперты', 'эксперты'], ['комментаторы'], ['задачи', 'задач', 'задач', 'задач', 'задач'], ['программное обеспечение', 'программ', 'программное обеспечение', 'себя', 'оно', 'программы', 'приложения'], ['ответы', 'ответ', 'ответ', 'ответов', 'ответами'], ['запросы', 'запросов'], ['Источник', 'Источник', 'Источник', 'Источник'], ['Исследователи', 'исследователя'], ['код', 'код', 'кода', 'кодом', 'кодом', 'кода', 'код'], ['информации', 'информации', 'которую', 'информации'], ['исследований', 'исследовании', 'исследованиям', 'исследования', 'исследований', 'исследование', 'исследование'], ['новой итерации ChatGPT', 'она', 'Новая модель', 'Модель'], ['ChatGPT', 'ChatGPT'], ['года', 'годах', 'года'], ['книги'], ['тестах', 'тестах', 'тест', 'тестах'], ['языков', 'языка', 'язык', 'язык'], ['данных', 'данные', 'данным'], ['фотографии'], ['графика'], ['«трендом»'], ['изображения', 'изображениями', 'изображения'], ['текст', 'текстом', 'текст', 'текст', 'текстом', 'текста'], ['музыку'], ['подсказки'], ['познание'], ['творчество', 'творчества'], ['интернет', 'интернете'], ['письмо'], ['огонь'], ['инкарнацию'], ['GPT-1'], ['GPT-2'], ['GPT-3'], ['GPT-3.5', 'предыдущей версии ChatGPT'], ['InstructGPT'], ['ChatGPT', 'ChatGPT', 'ChatGPT', 'ChatGPT', 'ChatGPT', 'GPTChat', 'проекта'], ['слова', 'Они'], ['вероятностью'], ['предпосылки', 'которая'], ['области'], ['компьютер', 'который', 'компьютера', 'его', 'компьютеров', 'которые', 'компьютере', 'машина'], ['имитационной игры Алана Тьюринга'], ['игры'], ['Алана Тьюринга', 'Тьюринга'], ['тест Тьюринга', 'который'], ['интеллект', 'интеллекта'], ['вывод'], ['Open AI', 'она', 'OpenAI', 'компании', 'OpenAI', 'OpenAI', 'OpenAI', 'Они', '[OpenAI]', 'свои', 'компании', 'Она', 'OpenAI', 'OpenAI', 'компания', 'Компания', 'они', 'OpenAI', 'компании OpenAI', 'ее', 'Сами создатели модели', 'своего', 'OpenAI'], ['Сан-Франциско'], ['штат Калифорния'], ['американской коллегии адвокатов'], ['центиле', 'центилем'], ['версии', 'итерацию', 'версий'], ['подписчиков'], ['«искусственного общего интеллекта»'], ['Генеральный директор', 'генеральный директор', 'генерального директора', 'генеральный директор'], ['Генеральный директор OpenAI', 'Сэм Альтман', 'Сэм Альтман', 'генеральный директор OpenAI', 'Сам Сэм Альтман', 'генеральный директор OpenAI', 'своем'], ['The New York Times'], ['проблемы'], ['мультимодальная модель', 'которая'], ['человеку', 'человека', 'людьми', 'человек', 'человека', 'человека', 'людей', 'людьми', 'человеку', 'люди', 'которые', 'человека', 'человека'], ['сценариях', 'сценарий'], ['производительность', 'производительности'], ['Ее движок', 'он', 'Он'], ['количеству', 'количество', 'количество', 'число'], ['эти факты', 'они'], ['смысл'], ['консультанта'], ['консультанта Synopsys Software Integrity Group', 'Джейми Буте', 'Буте'], ['Synopsys Software Integrity Group'], ['издание Life Wire'], ['научное сообщество', 'сообщества'], ['издание Nature', 'том же журнале'], ['Саши Луччиони'], ['гения'], ['отравляющих веществ', 'яды', 'токсинов', 'которые'], ['веществ', 'веществ'], ['часов', 'часов'], ['нейронная сеть', 'нейронную сеть', 'Она', 'нее', 'сеть', 'она', 'сеть', 'нейросети'], ['биологического оружия'], ['Алармисты'], ['машинное обучение'], ['химикатов'], ['супермаркетах'], ['онлайн-сервисы'], ['заданий', 'задание', 'задание', 'задания'], ['деньги', 'деньги', 'этой суммы', 'капитал', 'купюр'], ['квадратик'], ['robot', 'робот', 'робота'], ['галочкой'], ['фрилансеров'], ['наемник'], ['капчу'], ['блоке'], ['факт'], ['некоммерческая организация', 'некоммерческую организацию'], ['документацию'], ['прибылью', 'прибыль', 'прибыль', 'прибыль'], ['инвесторам', 'ими'], ['оценкам'], ['аналитиков'], ['миллионы', 'миллионах', 'млн'], ['изменениями'], ['подход'], ['критиков', 'критики'], ['непрозрачность', 'Непрозрачность'], ['Наблюдатели'], ['ассистентом', 'Помощником', 'ассистента', 'ассистента', 'ассистент', 'который'], ['отраслях'], ['юриста'], ['патентного поверенного'], ['нас', 'нашем'], ['сферу', 'сфере'], ['закон'], ['прецедент'], ['шаблон', 'такой же', 'который', 'шаблонов'], ['заявки'], ['патентование'], ['патентование изобретения'], ['изобретения'], ['Программистам'], ['время', 'время', 'время', 'времени'], ['GitHub'], ['программирование'], ['пользователей', 'пользователю'], ['Copilot'], ['генерации'], ['идей'], ['редактирования'], ['черновиков'], ['автопилот'], ['пресс-релизы'], ['отчеты'], ['планы'], ['образовательный процесс'], ['стартап-акселератор'], ['Speak', 'которая'], ['продукт'], ['боты'], ['боты-учителя', 'которые'], ['учителя'], ['ученикам'], ['лекции'], ['домашнюю работу'], ['Диаграмма'], ['корректировки'], ['вопроса \"Где ты ошибся?\"', 'вспомогательного вопроса', 'вспомогательный вопрос'], ['линия'], ['энтузиасты'], ['конвейерами'], ['производстве'], ['электрификацией'], ['креатива'], ['работник'], ['электронная почта'], ['общение', 'общению'], ['рутину'], ['электронные письма'], ['консоли'], ['рассылки'], ['посты'], ['техподдержку'], ['Электронные медицинские карты', 'этими базами данных', '«электронной медкарты»'], ['медицинские карты', 'медкарты'], ['врачей', 'их'], ['обновление'], ['согласование'], ['базами данных'], ['сами пациенты'], ['«электронной записи»'], ['глаза'], ['те', 'кто'], ['коммерциализация'], ['майнеров'], ['золотая лихорадка'], ['хайпа'], ['компаний'], ['интеграциях'], ['Snapchat', 'их'], ['чатботом'], ['приготовлению блюд'], ['Instacart'], ['рецепты'], ['рецепты с ингредиентами', 'их'], ['ингредиентами'], ['корзину'], ['доставкой'], ['высказывание'], ['генерального директора Microsoft'], ['Microsoft', 'мы', 'Microsoft'], ['поисковика'], ['поисковика Bing'], ['Гонка'], ['неделю'], ['топ-менеджеры'], ['«визионеры»'], ['IT-индустрии'], ['конкуренции'], ['отставших'], ['лидера'], ['млрд'], ['поисковик Google'], ['команду'], ['разработчиков'], ['систему'], ['крики'], ['машин'], ['рабочих мест'], ['автоматизации'], ['Goldman Sachs'], ['занятых', 'которых'], ['миру'], ['виду'], ['Bloomberg'], ['профессию', 'профессию'], ['prompt engineers'], ['контент'], ['месяц'], ['1С'], ['Excel'], ['бухгалтера'], ['голосовой ввод'], ['клавиатуру'], ['скоростью'], ['работодателями'], ['кинематограф'], ['театр'], ['рынке'], ['попытка «сдерживания»', 'которая'], ['технологий'], ['сами авторы «тревожного» письма'], ['запретом'], ['клонирование'], ['редактирование'], ['генома'], ['вирусов'], ['евгенику'], ['масштабность'], ['расщеплением атома'], ['атома'], ['один автор'], ['Atlantic'], ['автозаполнение'], ['РСР'], ['превосходства'], ['иллюзию', 'иллюзий'], ['всеведения'], ['сопоставление'], ['твиттере'], ['вы'], ['тактики'], ['конкурентов']]"}]}
{"messages": [{"role": "system", "content": ""}, {"role": "user", "content": ""}, {"role": "assistant", "content": ""}]}