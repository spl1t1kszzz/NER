# text =   1 January 2010 at 07:59 Заметки об NLP (часть 2) Artificial Intelligence Natural Language Processing.
[
  ["Заметки об NLP (часть 2)", "Artificial Intelligence Natural Language Processing"]
]
# text =   Заметки об NLP (Natural Language Processing).
[
  ["NLP", "Natural Language Processing"]
]
# text =   Хотя в первой части я и говорил, что не собираюсь останавливаться на морфологии, видимо, совсем без неё не получится.
[
  ["я", "говорил", "не собираюсь"],
  ["морфологии", "неё"]
]
# text =   Всё-таки обработка предложений сильно завязана на предшествующий морфологический анализ.
[
  ["обработка предложений", "завязана"],
  ["предшествующий морфологический анализ"]
]
# text =   Наш с вами родной русский язык очень хорош (для нас) и труден (для иностранцев) богатой фонетикой и разнообразием грамматических средств.
[
  ["Наш с вами родной русский язык", "хорош", "труден"],
  ["нас"],
  ["иностранцев"]
]
# text =   Во-первых, в них не так много незнакомых нам фонем.
[
  ["них"],
  ["нам"]
]
# text =   Во-вторых, обилие грамматических явлений редко сталкивает нас с чем-либо непонятным.
[
  ["обилие грамматических явлений", "сталкивает"],
  ["нас"]
]
# text =   А для американца, например, само понятие рода или падежа совершенно неочевидно.
[
  ["американца"],
  ["само понятие рода или падежа", "неочевидно"]
]
# text =   Теперь о морфологии.
[
  ["морфологии"]
]
# text =   Автоматические морфологические анализаторы работают хорошо.
[
  ["Автоматические морфологические анализаторы", "работают"]
]
# text =   Если кому интересно посмотреть, как работает автоматический анализатор — можно поэкспериментировать на сайте С.А. Старостина.
[
  ["автоматический анализатор"],
  ["сайте С.А. Старостина"]
]
# text =   Смею предположить, что едва ли не все морфологические анализаторы русского так или иначе опираются на Грамматический словарь Зализняка.
[
  ["морфологические анализаторы русского", "опираются"],
  ["Грамматический словарь Зализняка"]
]
# text =   Сам я пользуюсь разработками Алексея Сокирко, «обёрнутыми» в удобный интерфейс на сайте Lemmatizer.
[
  ["я", "пользуюсь"],
  ["разработками Алексея Сокирко", "«обёрнутыми»"],
  ["удобный интерфейс на сайте Lemmatizer"]
]
# text =   Судите сами: упомянутый русский морфологический анализатор Алексея Сокирко оперирует базой данных в 18,5 мегабайт.
[
  ["русский морфологический анализатор Алексея Сокирко", "упомянутый", "оперирует"],
  ["базой данных в 18,5 мегабайт"]
]
# text =   На Грамоте предлагают относить их к «предикативам», но общепринятого подхода нет.
[
  ["Грамоте"],
  ["их", "«предикативам»"],
  ["общепринятого подхода"]
]
# text =   Например, ещё одна «фича» анализатора Сокирко: он называет глаголы в личной форме («бегаю») глаголами, а в начальной форме («бегать») — инфинитивами.
[
  ["анализатора Сокирко", "он"],
  ["глаголы в личной форме («бегаю»)", "глаголами"],
  ["в начальной форме («бегать»)", "инфинитивами"]
]
# text =   Tags: NLP, обработка текстов, компьютерная лингвистика.
[
  ["NLP"],
  ["обработка текстов"],
  ["компьютерная лингвистика"]
]
# text =   Туториал по фреймворку для программирования датасетов MTS AI corporate blog.
[
  ["Туториал"],
  ["фреймворку для программирования датасетов"],
  ["MTS AI corporate blog"]
]
# text =   Я Игорь Буянов, старший разработчик группы разметки данных MTS AI.
[
  ["Я", "Игорь Буянов", "старший разработчик группы разметки данных MTS AI"]
]
# text =   Недавно рассказывал о том, как делать иерархически датасет из Википедии.
[
  ["рассказывал"],
  ["как делать иерархически датасет из Википедии"]
]
# text =   В этом посте хочу рассказать вам о Сноркеле - фреймворке для программирования данных (data programming).
[
  ["этом посте"],
  ["хочу"],
  ["вам"],
  ["Сноркеле", "фреймворке для программирования данных (data programming)"]
]
# text =   В разметочные функции (labeling functions) закодированы все возможные правила, по которым можно поставить какую-либо метку каждому примеру из набора данных.
[
  ["разметочные функции", "labeling functions"],
  ["все возможные правила", "по которым"],
  ["какую-либо метку"],
  ["каждому примеру из набора данных"]
]
# text =   В качестве основы для таких функций используются:внешние базы данных, такие как WordNet или WikiBase.
[
  ["основы для таких функций"],
  ["внешние базы данных"],
  ["WordNet"],
  ["WikiBase"]
]
# text =   Генеративная модель, являющаяся сердцем Сноркеля, попытается учесть недостатки отдельных функций.
[
  ["Генеративная модель", "сердцем Сноркеля", "попытается"],
  ["недостатки отдельных функций"]
]
# text =   Для наглядности оставляю здесь иллюстрацию с последовательностью работы со Снокрелем для задачи information extraction из оригинальной статьи.
[
  ["иллюстрацию с последовательностью работы со Снокрелем для задачи information extraction", "оригинальной статьи"]
]
# text =   Авторы оригинальной статьи представляют ее как факторный граф, или графическую вероятностную модель.
[
  ["оригинальной статьи", "ее"],
  ["факторный граф", "графическую вероятностную модель"]
]
# text =  Тогда модель определяется так, чтобы обучить эту модель без доступа к истинным меткам, это нужно обучаться с помощью логарифмического негативного маргинализированного правдоподобия, зная матрицу.
[
  ["модель", "эту модель"],
  ["истинным меткам"],
  ["матрицу"]
]
# text =  Оптимизацию авторы проводили с помощью SGD с семплированием Гиббса.
[
  ["Оптимизацию"],
  ["авторы"],
  ["SGD с семплированием Гиббса"]
]
# text =   Загрузим заранее обученную модель fastText, чей выбор объясняется наличием огромного количества опечаток в текстах.
[
  ["заранее обученную модель fastText", "чей выбор"]
]
# text =   Таким образом мы получили опорный вектор для класса "диарея".
[
  ["мы"],
  ["опорный вектор для класса \"диарея\""]
]